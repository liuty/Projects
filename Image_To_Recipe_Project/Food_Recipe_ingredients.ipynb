{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1029720"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the dataset\n",
    "\n",
    "data = pd.read_json('det_ingrs.json')\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1029720"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1_data = pd.read_json('layer1.json')\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1029720\n",
      "                                               ingredients  \\\n",
      "0        [{'text': '6 ounces penne'}, {'text': '2 cups ...   \n",
      "1        [{'text': '1 c. elbow macaroni'}, {'text': '1 ...   \n",
      "2        [{'text': '8 tomatoes, quartered'}, {'text': '...   \n",
      "3        [{'text': '2 12 cups milk'}, {'text': '1 12 cu...   \n",
      "4        [{'text': '1 (3 ounce) package watermelon gela...   \n",
      "...                                                    ...   \n",
      "1029715  [{'text': '1 cup butter, room temperature'}, {...   \n",
      "1029716  [{'text': '150 grams Daikon radish'}, {'text':...   \n",
      "1029717  [{'text': '1 cup apple cider'}, {'text': '6 ta...   \n",
      "1029718  [{'text': '1 pound ground veal'}, {'text': '1/...   \n",
      "1029719  [{'text': '8 cups water'}, {'text': '2/3 cup p...   \n",
      "\n",
      "                                                       url partition  \\\n",
      "0        http://www.epicurious.com/recipes/food/views/-...     train   \n",
      "1        http://cookeatshare.com/recipes/dilly-macaroni...     train   \n",
      "2        http://www.foodnetwork.com/recipes/gazpacho1.html     train   \n",
      "3        http://www.food.com/recipe/crunchy-onion-potat...      test   \n",
      "4        http://www.food.com/recipe/cool-n-easy-creamy-...     train   \n",
      "...                                                    ...       ...   \n",
      "1029715  http://www.food.com/recipe/baumkuchen-the-king...       val   \n",
      "1029716  https://cookpad.com/us/recipes/153324-extremel...     train   \n",
      "1029717         http://cooking.nytimes.com/recipes/1015164     train   \n",
      "1029718  http://www.foodandwine.com/recipes/polpette-sp...     train   \n",
      "1029719  http://www.epicurious.com/recipes/food/views/m...     train   \n",
      "\n",
      "                                                     title          id  \\\n",
      "0                               Worlds Best Mac and Cheese  000018c8a5   \n",
      "1                              Dilly Macaroni Salad Recipe  000033e39b   \n",
      "2                                                 Gazpacho  000035f7ed   \n",
      "3                                Crunchy Onion Potato Bake  00003a70b1   \n",
      "4                       Cool 'n Easy Creamy Watermelon Pie  00004320bb   \n",
      "...                                                    ...         ...   \n",
      "1029715                   Baumkuchen -- the King of Cakes!  ffffbb45d2   \n",
      "1029716      Extremely Easy and Quick - Namul Daikon Salad  ffffcd4444   \n",
      "1029717         Pan-Roasted Pork Chops With Apple Fritters  ffffd33513   \n",
      "1029718                     Polpette in Spicy Tomato Sauce  ffffd533d7   \n",
      "1029719  Mexican-Style Sweetened Black Coffee (Cafe de ...  ffffdea29a   \n",
      "\n",
      "                                              instructions  \n",
      "0        [{'text': 'Preheat the oven to 350 F. Butter o...  \n",
      "1        [{'text': 'Cook macaroni according to package ...  \n",
      "2        [{'text': 'Add the tomatoes to a food processo...  \n",
      "3        [{'text': 'Preheat oven to 350 degrees Fahrenh...  \n",
      "4        [{'text': 'Dissolve Jello in boiling water.'},...  \n",
      "...                                                    ...  \n",
      "1029715  [{'text': 'Whip butter and sugar well until cr...  \n",
      "1029716  [{'text': 'Julienne the daikon and squeeze out...  \n",
      "1029717  [{'text': 'In a large bowl, mix the apple cide...  \n",
      "1029718  [{'text': 'Preheat the oven to 350.'}, {'text'...  \n",
      "1029719  [{'text': 'Bring water, brown sugar, molasses,...  \n",
      "\n",
      "[1029720 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(len(layer1_data))\n",
    "print(layer1_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove entries with NaN values for columns taken as features\n",
    "data = data[data['publication_year'].notna()]\n",
    "data = data[data['authors'].notna()]\n",
    "data = data[data['num_pages'].notna()]\n",
    "data = data[data['genre'].notna()]\n",
    "data = data[data['is_ebook'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     valid          id  \\\n",
      "0        [True, True, True, True, True, True, True, Tru...  000018c8a5   \n",
      "1        [True, True, True, True, True, True, True, Tru...  000033e39b   \n",
      "2        [True, True, True, True, True, True, True, Tru...  000035f7ed   \n",
      "3               [True, True, True, True, True, True, True]  00003a70b1   \n",
      "4                           [True, True, True, True, True]  00004320bb   \n",
      "...                                                    ...         ...   \n",
      "1029715  [True, True, True, True, True, True, True, Tru...  ffffbb45d2   \n",
      "1029716                     [True, True, True, True, True]  ffffcd4444   \n",
      "1029717  [True, True, True, True, True, True, True, Tru...  ffffd33513   \n",
      "1029718  [True, True, True, True, True, True, True, Tru...  ffffd533d7   \n",
      "1029719                     [True, True, True, True, True]  ffffdea29a   \n",
      "\n",
      "                                               ingredients  \n",
      "0        [{'text': 'penne'}, {'text': 'cheese sauce'}, ...  \n",
      "1        [{'text': 'elbow macaroni'}, {'text': 'America...  \n",
      "2        [{'text': 'tomatoes'}, {'text': 'kosher salt'}...  \n",
      "3        [{'text': 'milk'}, {'text': 'water'}, {'text':...  \n",
      "4        [{'text': 'watermelon gelatin'}, {'text': 'boi...  \n",
      "...                                                    ...  \n",
      "1029715  [{'text': 'butter'}, {'text': 'sugar'}, {'text...  \n",
      "1029716  [{'text': 'daikon radishes'}, {'text': 'sesame...  \n",
      "1029717  [{'text': 'apple cider'}, {'text': 'sugar'}, {...  \n",
      "1029718  [{'text': 'ground veal'}, {'text': 'sweet Ital...  \n",
      "1029719  [{'text': 'water'}, {'text': 'light brown suga...  \n",
      "\n",
      "[1029720 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients_list = pd.DataFrame(list(data['ingredients']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cheese sauce'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sanitizeString(input):\n",
    "    output = input\n",
    "    output = output.lower()\n",
    "    output = \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "540000\n",
      "550000\n",
      "560000\n",
      "570000\n",
      "580000\n",
      "590000\n",
      "600000\n",
      "610000\n",
      "620000\n",
      "630000\n",
      "640000\n",
      "650000\n",
      "660000\n",
      "670000\n",
      "680000\n",
      "690000\n",
      "700000\n",
      "710000\n",
      "720000\n",
      "730000\n",
      "740000\n",
      "750000\n",
      "760000\n",
      "770000\n",
      "780000\n",
      "790000\n",
      "800000\n",
      "810000\n",
      "820000\n",
      "830000\n",
      "840000\n",
      "850000\n",
      "860000\n",
      "870000\n",
      "880000\n",
      "890000\n",
      "900000\n",
      "910000\n",
      "920000\n",
      "930000\n",
      "940000\n",
      "950000\n",
      "960000\n",
      "970000\n",
      "980000\n",
      "990000\n",
      "1000000\n",
      "1010000\n",
      "1020000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ingredients_list_final = []\n",
    "ingredients_list_unique = set()\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if(i%10000 == 0):\n",
    "        print(i)\n",
    "    temp_list = []\n",
    "    for j in range(99):\n",
    "        if(ingredients_list[j][i] == None):\n",
    "            break\n",
    "        temp_list.append(ingredients_list[j][i]['text'])\n",
    "    ingredients_list_final.append(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170532\n"
     ]
    }
   ],
   "source": [
    "# Sanitize ingredients list\n",
    "\n",
    "\n",
    "ingredients_list_unique = set()\n",
    "for i in range(len(data)):\n",
    "    for j in range(99):\n",
    "        if(ingredients_list[j][i] == None):\n",
    "            break\n",
    "        ingredients_list_unique.add(ingredients_list[j][i]['text'])\n",
    "print(len(ingredients_list_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3 packets Equal sugar substitute\n",
      "1 lb Puff paste (see below) or possibly thawed frzn puff pastry (+ additional for garnish if you like)\n",
      "chocolate - covered wafers\n",
      "1/2 package sweet cream\n",
      "3 lrg Large eggs beaten lightly\n",
      "1 jar Diced Pimento (small Jar)\n",
      "4 thick-cut bacon slices, cut crosswise into 1/4-inch pieces\n",
      "1 cedar grilling plank\n",
      "1 Aburaage for Inarizushi\n",
      "1 pkg. instant pudding\n",
      "1 ball of focaccia dough (see recipe)\n",
      "1 recipe Pate Sucree, recipe follows\n",
      "1 can tomatoes 1 large can, whole tomato\n",
      "1 lb. (450 g) new potatoes, halved King Sooper's 5 lb For $1.88 thru 02/09\n",
      "mango and ginger conserve\n",
      "6 slc Thinly-sliced smoked bacon cut 1\" squares\n",
      "3 cans refrigerated biscuits (cut in quarters)\n",
      "shrimp flavor ramen noodle soup\n",
      "1 container fat free sour cream\n",
      "1 package Dubliner cheddar cheese, cut into slices\n",
      "cane syrup\n",
      "2 cups milk 1% or 2 %\n",
      "1 gal. Golden OREO Pieces-Medium, finely crushed\n",
      "sassafras leaves\n",
      "5 cups Raspberry, Frozen Or Fresh (I Used Frozen)\n",
      "4 Wild Garlic Blossoms, To Garnish\n",
      "1 tsp Preserved vegetable\n",
      "seasoned fish fry\n",
      "1 (15-ounce) can white (cannellini or navy) beans, drained\n",
      "14 whole Habanero Peppers Seeded And Steamed\n",
      "3 tbsp (level) of caramelised red onion chutney bought or homemade\n",
      "roasted ancho chiles\n",
      "4 KRAFT 2% Milk Singles, cut into quarters Target 2 For $5.00 thru 02/06\n",
      "1 C - Tempura crumbs\n",
      "1 piece Boneless, Skinless Chicken Breast, Cut Into Strips\n",
      "1 lb. boneless skinless chicken thighs Safeway 1 lb For $1.99 thru 02/09\n",
      "sweet dessert wine\n",
      "1 pouch Crystal Light Peach Iced Tea Low Calorie Drink Mix\n",
      "1 package burrito-size flour tortillas\n",
      "Accompaniment:Crisp Won Ton Strips\n",
      "1 large package vanilla pudding, not instant\n",
      "4 slc peasant bread grilled\n",
      "Small round pastry tip (#2 or similar)\n",
      "roasted tomatoes, for serving Safeway 1 lb For $1.29 thru 02/09\n",
      "2 cloves (Large) Garlic, Minced, Divided\n",
      "1 dash Baby leaves\n",
      "1 packages Bonito flakes\n",
      "2 Boneless, Skinless Breast Halves\n",
      "frappuccino caramel coffee drink\n",
      "24 ounces, weight Vanilla Candy Melts\n",
      "1 whole Pound Dark Chcolate\n",
      "orange jelly powder\n",
      "4 carrots, peeled, cut into sticks Target 2 lb For $3.00 thru 02/06\n",
      "2 can kidney beans, drained, 15 oz each\n",
      "1 can Swanson chicken broth\n",
      "4 TASSIMO GEVALIA Espresso T DISCs\n",
      "2 each Tomato, roma, Diced small/ medium\n",
      "1 small piece dried nori seaweed sheet\n",
      "1 bag Fresh Spinach (10 - 12 Oz. Bag)\n",
      "1/2 pounds, 78 ounces, weight Strawberries, Chopped\n",
      "2 cans chow mein vegetables\n",
      "1 whole chicken (approx. 3 lbs (1.4 kg).)\n",
      "1 bottle red wine (white works, too)\n",
      "4 bottles Prosecco\n",
      "8 1/2-inch-thick diagonal baguette slices\n",
      "1 box clove (whole)\n",
      "3 packages waverly Club crackers, divided\n",
      "2 can your favorite biscuits\n",
      "Blow torch\n",
      "1 packages Mexican blend cheese\n",
      "1 jar Salsa (whichever Brand You Like)\n",
      "2 cups honey-flavored multi-grain cereal flakes with oat clusters, any variety, divided\n",
      "1 (14-ounce) can fat-free, less-sodium chicken broth, divided\n",
      "6 packets Equal sugar substitute\n",
      "1 box rice pilaf\n",
      "fine-quality extra-virgin olive oil for drizzling\n",
      "1 can diced tomatoes do not drain\n",
      "green coconuts\n",
      "decorative candies\n",
      "1/2 tsp Mustard, (I used Coleman's Warm English powdered mustard and mixed it with a little water) Seasoning to taste\n",
      "1 box of small soup pasta (i.e. ancni pepe, tubietti, starletta, orzo) cooked, drained and cooled. OPTIONAL\n",
      "1/2 c. water Butter flavored sprinkles or possibly spray see * Note 2\n",
      "yeast energizer\n",
      "280 gr di farina bianca\n",
      "pesto sauce\n",
      "1 box baby bella mushroom\n",
      "14 oz 2 cans of Italian seasoned Crushed Tomators\n",
      "5-3/4 cups prepared fruit (buy about 3-1/2 lb. fully ripe plums) Safeway 1 lb For $3.99 thru 02/09\n",
      "4 store-bought puppadum\n",
      "White Chocolate-Cream Cheese Frosting:\n",
      "1/2 packages Pepperidge Farm Stuffing\n",
      "Vegetable Spray\n",
      "Shredded\n",
      "1 pound, 7 ounces, weight Extra Lean Ground Beef\n",
      "1/4 cups Rounded, Freshly Grated Parmesan Cheese\n",
      "1 oz. reduced-fat colby & Monterey Jack cheese, cut into 6 thin slices Target 2 For $5.00 thru 02/06\n",
      "1 package Active Dry Yeast (2 1/4 Teaspoons In A Package)\n",
      "400 grams sorbetto al mandarino (orange icecream)\n",
      "1 lrg Cold Whip\n",
      "1/4 cup sliced green onions*\n",
      "ground flax seeds\n",
      "2 cups raw Spanish peanuts with skins\n",
      "2 tsp Kinako (or toasted barley flour if allergic to soy)\n",
      "5 medium beefsteak tomatoes (about 2 pounds), cut into 1-inch chunks (about 5 cups)\n",
      "1/4 teaspoon Essence, plus 1/4 teaspoon, recipe follows\n",
      "8 cups arugula (roquette) leaves, prefer baby arugula\n",
      "1 bag 7 bean mix, rinsed and soaked over night or 1 bag 15 bean mix\n",
      "4 bottles Mexican beer, ice-cold\n",
      "1 boneless beef sirloin steak (1-1/2 lb.) Whole Foods 1 lb For $7.99 thru 02/09\n",
      "1 can mushrooms stems and pieces, undrained\n",
      "bluefish fillets\n",
      "pecan (optional)\n",
      "4-quart slow cooker\n",
      "2 cans Van Kamps Pork N Beans (31 Ounce Cans)\n",
      "12 whole 8\" Tortillas Of Your Choice\n",
      "Hachiya\n",
      "1 (If you prefer it sweet, add about 1 teaspoon of beet sugar or honey)\n",
      "2 bunches Swiss chard (1 1/2 pounds)stems sliced 1/2 inch thick, leaves cut into 1-inch ribbons\n",
      "1 can crushed pineapple, heavy syrup\n",
      "1 packages pork chunks\n",
      "2 tablespoons (or more) fish sauce (such as nam pla or nuoc nam)*\n",
      "8 1 1/4 inch thick lamb loin chops abt 2 pounds\n",
      "8 skate wings, (3 to 4 ounces each) cleaned\n",
      "1 Other toppings\n",
      "1 cup mixed assorted fresh berries (blueberries, raspberries, sliced strawberries) Whole Foods 3 For $10.00 thru 02/09\n",
      "Two 1 1/2-pound boneless goose breast halves with skin, skin scored in a crosshatch pattern (see Note)\n",
      "2 lb Leavened Dough\n",
      "dark baking chocolate\n",
      "1 pkg. soup mix, example: Knorrs\n",
      "4 each yellow peppers, julienne cut Target 1 pkg For $2.99 thru 02/06\n",
      "1 pound young tender turnip tops\n",
      "1/3 cup BREAKSTONE'S or KNUDSEN Sour Cream*\n",
      "1 x Makes 2-1/2 Qts\n",
      "1 3/4 lbs. skinned, boned chicken cut in sm. chunks\n",
      "1/2 large cube Soup stock cube\n",
      "1 Tongs\n",
      "3 large Cans of bushes country style baked beans\n",
      "27 packets Splenda sugar substitute\n",
      "1 can lunch meat, cubed into 1/2 inch cubes\n",
      "mushroom gravy mix\n",
      "1 jar sliced onions (optional)\n",
      "dried red pepper flakes\n",
      "1/2 pounds, 78 ounces, weight Packet Of Gingernut Biscuits\n",
      "1/4 cup chopped Kimchee, liquid reserved\n",
      "1 package 12 Ounce Size, Wonton Wrappers\n",
      "1 cup vegetable oil (for example -- lite olive)\n",
      "godiva petite mousse biscuits\n",
      "3/4 cups Raisins, Chopped (optional)**\n",
      "1/2 tsp. dried\n",
      "1 whole chicken, cut into 6 or 8 pieces\n",
      "1 1/2 pounds 90-percent lean, freshly ground turkey (not all white meat), chilled\n",
      "1/2 teaspoon dark cocoa powder, or more to taste (optional)\n",
      "2 can Tomato sauce 15 oz each\n",
      "Lighter wines will often be more appreciated with brunch entertaining. Seek out light wines with low- moderate alcohol levels and fresh acidity in the finish.\n",
      "tabasco peppers\n",
      "1 thick yellow onion slice, coarsely chopped\n",
      "4 (2.1 ounce) bars chocolate-covered crispy peanut butter candy (such as Butterfinger), crushed, or more to taste\n",
      "2 packages lowfat mozzarella cheese\n",
      "1 can ro-tel\n",
      "1/4 lb Mushrooms, , finely minced\n",
      "2 half-ripe pineapples\n",
      "1 tin flat anchovies, drained and chopped\n",
      "1 packages Zesty hot sausage (Bob Evans is what I use)\n",
      "Dijon mustard\n",
      "1 packages parmesan cheese, only needs 1/3 cup\n",
      "15 whole wings\n",
      "Frosting (optional):\n",
      "3 packages Strawberries\n",
      "14 12-inch-long, 1/4-inch-diameter wooden dowels\n",
      "geranium leaves\n",
      "Extra-virgin olive oil for drizzling, plus 1/3 cup\n",
      "2 1/2 pound chickens, cut into serving pcs\n",
      "1 carton of freshly sliced strawberry\n",
      "3 tbsp Japanese Worcestershire style sauce\n",
      "1- 1/2 pound Boneless, Skinless Chicken Breasts, Cut Into Strips\n",
      "10 ounces, weight Frozen, Chopped Spinach, Thawed\n",
      "3 ounces, weight Package Watermelon Jell-O Mix\n",
      "low fat noodles\n",
      "3 tablespoons tamarind (from a pliable block)\n",
      "1 container Whipped Topping (the Kind In A Can)\n",
      "2 box chicken broth (32 oz. each)\n",
      "Some oil for the baking tray\n",
      "1 can white whole kernel corn, (10-ounce.)\n",
      "3 bags chopped clams\n",
      "1 tablespoon vegetable flakes\n",
      "slim cucumbers\n",
      "1 powdered sugar + egg white Icing\n",
      "Cookie Dough:\n",
      "(Shortbread Crust):\n",
      "1/2 lb. cleaned medium shrimp Safeway 1 lb For $8.99 thru 02/09\n",
      "1 recipe of Sauce Supreme\n",
      "1 ~3 teaspoons Dashi stock granules\n",
      "1 pair shad roe\n",
      "1 can diced tomatoes (398 ml)\n",
      "reduced - sodium marinara sauce\n",
      "250 grams Kabocha\n",
      "3 large onions (2 pounds), thinly sliced (see Note)\n",
      "four 1 1/4inch-thick frenched veal rib chops* (each about 12 ounces)\n",
      "1 can frozen cranberry concentrate\n",
      "granola - type cereal\n",
      "2 box kraft pasta salad classic italian\n",
      "1 envelope Knorr vegtable soup\n",
      "1 can canellini beans\n",
      "1 each 4 oz Plastic Cup\n",
      "2 dried chili's, cut\n",
      "1/4 cup tobik-ko (flying fish roe)\n",
      "1 package Dry Yeast (about 2 1/4 Teaspoons)\n",
      "pizza cheese\n",
      "1 (Optional - chocolate sprinkles)\n",
      "2 large tomatoes, seeded, chopped King Sooper's 1 lb For $0.99 thru 02/09\n",
      "Thai chiles\n",
      "3-3/4 qt. onions, cut into 1/4-inch-thick strips King Sooper's 1 lb For $0.99 thru 02/09\n",
      "1 can Diced Tomatoes, 14 Oz\n",
      "1 packet Wiener sausages\n",
      "1 pkg. instant vanilla pudding (sm.)\n",
      "1 1/2 cup or handful dried lily bulbs\n",
      "1 can Campbell's condensed tomato soup\n",
      "4 clove Garlic (whole) I plan to pick out for other dishes, but want the flavor!\n",
      "Maple Dijon Vinaigrette:\n",
      "whole wheat sandwich wraps\n",
      "1 lb spaghetti or possibly perciatelli, - (tubular spaghetti\n",
      "1 can pie filling any flavor\n",
      "1/4 cups Queso Cojita\n",
      "6 paratha or chapatti\n",
      "1 package Mozzarella Cheese (16 Oz)\n",
      "4 KRAFT Light Singles, cut in half Target 2 For $5.00 thru 02/06\n",
      "tom yam paste\n",
      "2 cans ranch-style beans\n",
      "2 tbsp Tsukemono (Japanese pickles) (this time I used Kyoto turnips)\n",
      "12 x raw tiger prawns peeled with dark intestinal vein removed\n",
      "1 package chinese-style firm tofu\n",
      "1 x Place the shrimp in a large nonreactive bowl and season liberally\n",
      "1 can green beans french cut\n",
      "1 1/2 pounds mushrooms, washed (see Notes), dried, and quartered\n",
      "2 tablespoons extra-virgin olive oil plus additional for serving\n",
      "5 whole Broccolini Florets, Cut In Half Lengthwise\n",
      "maui onions\n",
      "8 hot baked potatoes, split Safeway 2 pkg For $5.00 thru 02/09\n",
      "1 packages morrison's bis kits\n",
      "Pepsi\n",
      "Cubes, softened\n",
      "1 tub whipped cream topping\n",
      "1 fritter ingredients ------------------------\n",
      "1 can Mexican-style kernel corn\n",
      "4 chicken thigh fillets, cut into even sized pieces Whole Foods 1 lb For $3.99 thru 02/09\n",
      "1 pkt Dry lentils, (12 ounce)\n",
      "1 1/2 pounds skinless, boneless chicken breasts, sliced on the bias 3/4 inch thick\n",
      "1 each white onion-sliced julienne\n",
      "1 x red chilli finely minced\n",
      "2 (4 ounce) packets sweetened tropical punch flavored drink mix powder\n",
      "1 packages mission soft tortilla\n",
      "yellow squash\n",
      "1 jar Maraschino Cherries, Drained And Chopped (10 Ounces)\n",
      "Basic pizza dough, rolled out for 2 (12-inch) pizzas, recipe follows\n",
      "yellow hominy\n",
      "pink sugar\n",
      "1 pkg. UNCLE BEN'S&reg; Wild Rice Stuffing\n",
      "1 (16 ounce.) pkg. instant chocolate drink pwdr\n",
      "1- 1/2 cup Milk, Or More As Needed\n",
      "1 package williams chili seasoning mix\n",
      "4 cups favorite marinara sauce (see Kittencal's Marinara Pasta Sauce (Vegetarian))\n",
      "13 cup milk, skim, (non fat) powder non-fat\n",
      "4 dsh warm sauce\n",
      "1 large bunch red beets (2 inches diameter or more)\n",
      "1 dry hot chilli to side as need\n",
      "1 pkg. (156 g) Kraft Dinner Macaroni & Cheese Teenage Mutant Ninja TurtlesTM Shapes\n",
      "miniature peanut butter cups\n",
      "tangerine peel\n",
      "1 cup Shredded, Cooked Chicken\n",
      "jeera powder\n",
      "4 tablespoons A.1. Original Sauce\n",
      "kosher pickle\n",
      "peeled, seeded and chopped\n",
      "2 citrus leaves\n",
      "Layer #1:\n",
      "2 Tbsp. A.1. Bold & Spicy Sauce\n",
      "3/4 pound carrots (4 to 5 medium), peeled and cut into 3-inch lengths (halve thick ends lengthwise, then cut into 3-inch lengths)\n",
      "1 pkg. burrito flavor mix\n",
      "2 cup World Table brand Roasted tomato chipoltle roja dip\n",
      "1 jar of ragu parmesan garlic cheese sauce\n",
      "5 Raw Pacific saury (sanma)\n",
      "2 tablespoons Asian chili paste (sambal), or more to taste (optional)\n",
      "Persian cucumbers\n",
      "Pancakes:\n",
      "2 tablespoons pignoli (pine nuts)\n",
      "1 package Refrigerated Cheese Tortellini (9oz) Prepared According To Package Directions\n",
      "1 whole chicken (Saudi Chicken Kofta (Gluten-Free) recipe)\n",
      "1 bunch fresh basil (about 6 ounces), soaked, rinsed thoroughly, and stripped from stems (reserve about a half dozen of the sprigs from the top of the stem for garnish)\n",
      "1 c. Uncooked medium buckwheat kernels (kasha)\n",
      "1 bottle tasty red wine\n",
      "1 (14-ounce or 400 gram) tin of cannellini beans, or use 6 ounces (17 grams) dried ones, soaked and cooked until tender\n",
      "rum flavoring\n",
      "1 caradamom\n",
      "1/2 cup chipotle sauce\n",
      "1/3 bag Bean sprouts (optional)\n",
      "2 Chilled Serving Bowls\n",
      "2 cup mashef potatoez\n",
      "2 Italian-style crostini toasts\n",
      "1 large can green enchilada sauce\n",
      "fat - free apricot yogurt\n",
      "1 1/2 cup mini M&Ms\n",
      "1 pkg. (7-1/4 oz.) KRAFT THICK 'N CREAMY Macaroni & Cheese Dinner\n",
      "1/2 pkg. Crystal Light lemonade mix\n",
      "2 packages light cream cheese\n",
      "Dash Louisiana Red Warm Sauce\n",
      "lavash bread\n",
      "1 Silpat silicone sheet\n",
      "1/2 cup warm beer, or more as needed\n",
      "1 (lots( Shiso leaves\n",
      "1 packages grated cheese (mexican blend)\n",
      "side pork\n",
      "6 Crostoni Bagnati (page 48)\n",
      "1 can Crush tomatoes\n",
      "1/4 cup calcium oxide\n",
      "3 to 4 boxes of cake mix. Any flavors\n",
      "1 can Pumpkin, Large Can\n",
      "1/2 teaspoon file pwdr\n",
      "1-1/2 lb. (675 g) small red potatoes (about 8), halved King Sooper's 5 lb For $1.88 thru 02/09\n",
      "1/2 to 1 small pack Enoki mushrooms\n",
      "1 pinch ground nutmeg, or more to taste (optional)\n",
      "shelled pecans 2 tablespoons\n",
      "1 1/2 lb. beets, trimmed, peeled and cut into 1?2-inch slices\n",
      "1 package Plain Sweet Biscuits\n",
      "1 can diced tomatoes in puree\n",
      "1/2 cup onions, grated King Sooper's 1 lb For $0.99 thru 02/09\n",
      "2 boxes Yellow Cake Mix, 18 Ounce Boxes\n",
      "2 cups KRAFT FREE Raspberry Vinaigrette Fat Free Dressing\n",
      "34 cup peanut butter cup, chopped into chunks (or use Karen's =^..^= \"Reese's Square's\" Reeses Squares - 5 Ingredients & No Bake (Reese's))\n",
      "1 can Pineapples\n",
      "3 Tbs. tandoori spice blend\n",
      "1 can water chestnuts (8 oz.)\n",
      "1 whole Large Green Peppers Sliced Into 1 Inch Strips\n",
      "1 package Rigatoni, A Package Is 16 Ounces\n",
      "1/3 packages baby bella mushrooms\n",
      "1 clove garlic, crushed, sliced thinly *\n",
      "3 cups cubed red and green apples,, (with skin on) Safeway 1 lb For $0.99 thru 02/09\n",
      "berry yogurt\n",
      "1/2 black olive per saltena\n",
      "1 medium crinkle cut carrot used a mandolin\n",
      "raisin bread\n",
      "1 pkg. Cheddar cheese, grated\n",
      "1 pound beef, flank steak (london broil) trimmed\n",
      "1 x -to\n",
      "1/2 cup (125 ml) Judges 5:25 (butter, softened)\n",
      "1 lb. medium shrimp, cleaned King Sooper's 1 lb For $7.99 thru 02/09\n",
      "1 each sweet yellow bell peppers seeded\n",
      "2 whole boneless, very lean pork tenderloins, about 1 1/2 pounds\n",
      "4 cans peas\n",
      "1/2 Egg ( You can also make it without using egg --> Refer to Step 5)\n",
      "2 teaspoons Spicee Gourmet's St. Louis Rib Rub\n",
      "double bouillon cubes\n",
      "1 packages baby bella mushrooms\n",
      "4 juniper berries* (optional)\n",
      "6 soft flour tortillas 7 to 8 inches\n",
      "1 can Tomatoes(8oz)\n",
      "1 box cornbread stuffing mix, cooked\n",
      "cherry drink mix\n",
      "2 teaspoons bottled smoke\n",
      "1 Tony Cachery's, to taste\n",
      "1-1/2 qt. avocados, peeled, cut in 1/2-inch dice Whole Foods 5 ea For $5.00 thru 02/09\n",
      "2 cans Crescent Rolls\n",
      "1 1/2 can diced green chilies, small can\n",
      "1 pound tomatoes, or 1 (14.5-ounce) can\n",
      "1/8 teaspoon null ground red peppernull\n",
      "2 large, sweet, juicy carrots, julienned\n",
      "1 can White Hominy\n",
      "8 twigs\n",
      "4 1/2 boxes of Oreos (16 x 4.5 72 Oreos)\n",
      "75 ml halt fat double cream icing sugar to dust\n",
      "1 cup Masa Harina (corn tortilla mix)*\n",
      "2 (1 1/2 to 2-pound) Petrole soles, scaled and eviscerated, head off\n",
      "1 ounce bitter orange digestif, such as Campari\n",
      "1 whole green pepper cut in strips\n",
      "Chilled brewed strong MAXWELL HOUSE Coffee, any variety Walgreens $6.99 thru 02/06\n",
      "1 container fresh bean sprouts\n",
      "8 ounce. Velveeta cheese cut in chunks\n",
      "3/4 cup fregola (Sardinian pasta; see Ingredient Resources, page 193)\n",
      "Aged balsamico condimento\n",
      "2 cup Daiya cheddar cheese shreads (lactose free, vegan, non-dairy)\n",
      "1 1/2 libra de yautia blanca en pedazos\n",
      "1/2 cup ketchup + some for top\n",
      "1 pkt frzn cut green beans - (9 ounce)\n",
      "1 small roasted chicken (about 1 3/4 pounds; recipe, page 351) or cooked rotisserie chicken, skinned and shredded (about 4 cups), carcass discarded\n",
      "Label and color-code items for easy identification\n",
      "milk chocolate\n",
      "Special Equipment: Several ceramic or silicone ghost-shaped molds\n",
      "Hot Sweet-and-Sour Peanut Sauce), as an accompaniment\n",
      "1 lb. flank, round,\n",
      "1/2 cup worchershire\n",
      "waxy potatoes about 2 pounds (900g to 1kg)\n",
      "3/4 cup sugar + 2 tbsp\n",
      "Dry beef slices\n",
      "3 peaches*\n",
      "2 squares Italian Herb Saute Express Saute Starter\n",
      "1/3 can Spam\n",
      "1 cup Kewpie (Japanese) mayonnaise, to serve (optional)\n",
      "1 tablespoon extra-virgin olive oil plus additional for drizzling\n",
      "4 Tablespoons Thai Iced Tea Mix\n",
      "3 thin tomato slices King Sooper's 1 lb For $0.99 thru 02/09\n",
      "100 grams mixed oriental veg\n",
      "4 fried tomato skins\n",
      "1 can chopped clams, liquid removed\n",
      "zest + juice of 1 lime\n",
      "1 tbsp Sun Dried Tomatoes [+ 1 tbs reserve]\n",
      "Spicy Dip, recipe follows\n",
      "1 package tofu (firm or extra firm)\n",
      "2 cup White Raspberries if you can't find them leave them out & add more of the others\n",
      "3/4 lb. red potatoes (about 2), cut into 1-inch chunks King Sooper's 5 lb For $1.88 thru 02/09\n",
      "Pies:\n",
      "1 pkg. lasagna noodles, uncooked\n",
      "1 Omurice filling (onion, carrot, bell peppers, chicken... anything you usually use)\n",
      "Appenzeller cheese\n",
      "1 can Sweet Peas\n",
      "1 package cake mix chocolate fudge, with pudding\n",
      "4 can skinless and boneless salmon seven ounces each\n",
      "Elastack molding material\n",
      "1 bottle Fruity White Wine, 750 Ml Size\n",
      "4 KRAFT Light n' Tasty Singles\n",
      "Avocado Aioli:\n",
      "2 1/2 cups Kraft Creamy Cucumber Dill Dressing\n",
      "Miracle Whip light\n",
      "1/2 teaspoons Sriracha, Or To Taste, Plus More For Garnish (optional)\n",
      "1 pkg. Stove Top dressing, cooked according to directions on pkg.\n",
      "Tabasco...lots of Tabasco. I usually use about 6 tbsp (90 ml) worth.\n",
      "1 small size flower tip\n",
      "1 small bag frozen peas\n",
      "**Available at Latin American markets and many supermarkets.\n",
      "1 pound ground beef (optional), browned, drained\n",
      "1 sm. can white chunk chicken\n",
      "1 cup walnut halves or pieces (3 ounces)\n",
      "1/2 cup favorite BBQ sauce (1 part), plus more for basting (recommended: Corky's)\n",
      "2 Tbsp. A.1. Sweet Hickory Sauce\n",
      "1 bottle garden ranch dressing\n",
      "110 grams Hanpen\n",
      "1 10\" Springform pan\n",
      "1 pkt yellow cake mix\n",
      "prime rib roast\n",
      "2 pkt Rapid-rise yeast (yeast which requires only one rising)\n",
      "goya bitter orange marinade\n",
      "4 skinless, boneless chicken breast halves - grilled\n",
      "1 (6-ounce) ramekin\n",
      "white tuna packed in oil\n",
      "12 Tbsp. cut up unsalted butter room temperature\n",
      "1 whole salmon (about 7 pounds and 2 1/2 to 3 inches thick), cleaned, fins and gills trimmed, rinsed well (have the fishmonger do this)\n",
      "1 1/4 cup milk. (It calls for skim, but I use 2% or lactose free \"milk\")\n",
      "1 lg. can chili bean\n",
      "fresh horseradish\n",
      "1/4 cup corn or grapeseed oil for frying, or more as needed\n",
      "3/4 cup bonito shavings\n",
      "1 small eggplantpeeled, sliced 1 inch thick and cut into 1/2-inch dice\n",
      "1 can Tomatoes, cut up (16 ounce.)\n",
      "Ricotta Mixture:\n",
      "1 packages Ginseng Herb\n",
      "2 tablespoons blackening spice\n",
      "1 can Pillsbury Seamless Crescent Roll Dough (8 Ounce Can)\n",
      "1/2 fresh mango, cut into thin slices Whole Foods 2 ea For $4.00 thru 02/09\n",
      "1 packages Squid\n",
      "250 grams Kuromame\n",
      "high - fiber rolls\n",
      "1 packages Hidden Valley Ranch\n",
      "1 each green, yellow and red pepper, chopped\n",
      "1 bag shredded cabbage with carrots\n",
      "18 thin red pepper strips\n",
      "9 green onions (white and light green parts only), cut into 36 (2-inch) pieces\n",
      "1 Vegetarian Battered Veg\n",
      "Additional garnish options:\n",
      "1 tea bag (I tried with both green tea and earl grey--both were excellent)\n",
      "2 springonions\n",
      "1 can of comstock cherry pie filling\n",
      "4 nice-size beef cheeks, about 10 ounces (280 g) each, trimmed\n",
      "1 syrup!\n",
      "2 cups crushed blueberries (about 2-1/2 cups blueberries) Whole Foods 3 For $10.00 thru 02/09\n",
      "1 jar Red Jalapenos Slices (You'll Only Need A Few, To Taste)\n",
      "2 Tbsp. Walnut pcs,minced\n",
      "1 pkg. devils food cake mix\n",
      "1-1/2 lb. baking potatoes (about 2 medium) Safeway 2 pkg For $5.00 thru 02/09\n",
      "1 tube refrigerated pizza dough (recommended: Pillsbury brand)\n",
      "2 packages Kielbasa sausage (regular or turkey) sliced\n",
      "3/4 pound skinless, boneless chicken breast half - cut into cubes\n",
      "1 rabbit, about 3 1/2 pounds, cut in 6 or 7 pieces\n",
      "1 pack Whipped soy cream\n",
      "black gram dal\n",
      "1-1/2 qt. Oriental Sesame Slaw\n",
      "1 box of round toothpick\n",
      "1 sm. container whipping cream\n",
      "1 jar caramel sauce\n",
      "1 package instant pudding mix and pie filling, fudge\n",
      "1 pkg. Jello brand pistachio flavor instant pudding & pie filling\n",
      "2 1/4 pounds red-skinned sweet potatoes (yams), peeled, cut into 1 1/2-inch pieces (about 7 cups)\n",
      "1 bag boil-in-the-bag rice\n",
      "20 drops Food Coloring Each: Red, Blue, Green And Yellow (for Divided Batters)\n",
      "1/4 small onion, sliced (about 3 slices separated into rings) King Sooper's 1 lb For $0.99 thru 02/09\n",
      "2 thick tomato slices\n",
      "1 lb/20 small brussels sprouts cut in half, leave the stem on so they don't fall apart\n",
      "1/4 teaspoon Aji Amarillo\n",
      "1/4 cup Pecans ~ chopped\n",
      "1 container Whipped Topping 8oz. Thawed\n",
      "3 small star anis\n",
      "2 squares (3 inch) flatbread, cut horizontally in half\n",
      "1 ounce Amaro\n",
      "dukkah\n",
      "4 pkg. KRAFT Simple Signatures Classic White Cheddar & Twisted Elbow Macaroni\n",
      "3 whole Clementine Oranges (or 2 Regular Oranges) Peeled, And Chopped Into Small Bite Sized Pieces\n",
      "1 can Chopped Ripe Olives (4 1/2 Oz)\n",
      "2 whole (2.1 Oz. Size) Butterfinger Candy Bars, Chopped\n",
      "2-1/2 qt. fresh strawberries, sliced Safeway 1 lb For $3.99 thru 02/09\n",
      "1 1/2 pounds skinless, boneless chicken breasts, cut into 3/4-inch pieces\n",
      "....any combination you like according to taste and imagination!\n",
      "cooked cream of buckwheat\n",
      "1 package Taco Seasoning\n",
      "1 jar hot fudge\n",
      "1/2 cup achiote seeds or 1/2 block Mexican axiote (If using Mexican axiote, dissolve in 1/4 cup hot water)\n",
      "8 ounces, fluid Irish Creme (I Used Bailey's)\n",
      "1 box pizza dough\n",
      "1 can asparagus tips and pieces\n",
      "1 cup tightly packed katsuobushi (bonito fish flakes)\n",
      "1 box devil cake mix\n",
      "1 package frozen spinach, thawed and very well drained\n",
      "1/2 cup chicken stock, preferably homemade (page 160), or as needed\n",
      "1 gallon Pitcher\n",
      "12 whole Papa Criollas (Andean Potatoes), Or Substitute Fingerling Potatoes Instead\n",
      "woodruff\n",
      "1 box of chicken broth (small box) or can\n",
      "4 ounce slab (unsliced) bacon, rind removed, cut into 1/3-inch dice\n",
      "1 bottle spray oil if frying eggs not poaching\n",
      "4 each navel oranges, peeled, each cut into 6 slices Safeway 1 lb For $0.79 thru 02/09\n",
      "2 cans Mexican Style Diced Tomatoes\n",
      "1 packages Organic grape tomatoes\n",
      "1 Block of Cabot's Sharp (or Extra Sharp) Cheddar\n",
      "Rubber or silicone spatula\n",
      "1 can mandarin orange, drained well\n",
      "1 medium butternut squash (about 1 1/2 pounds), peeled, seeded and cut into small dice (about 1/2 inch), about 4 cups\n",
      "2 can (small) or possibly\n",
      "2 slices OSCAR MAYER Bacon, cooked, crumbled Rite Aid 2 For $7.00 thru 02/06\n",
      "1 lg. can peeled Italian tomatoes\n",
      "1 pkt lager yeast\n",
      "4 ounces slab (unsliced) bacon, rind removed and cut into 1/3-inch dice\n",
      "2 pig's ears\n",
      "2 box Oreo pudding mix\n",
      "1 whole Crust For One Pizza\n",
      "TART\n",
      "1 pkg. white chocolate chips\n",
      "1/2 pkt Hidden Valley Ranch dip mix\n",
      "1 recipe Pistachio Crunch (page 186)\n",
      "1 non stick pan\n",
      "1 bottle store bought ranch dressing, for dipping\n",
      "2 pieces Katsu (any kind)\n",
      "light soy sauce\n",
      "1 package spangler circus peanuts\n",
      "1 can Black Beans, (15 Oz) Drained, Not Rinsed\n",
      "1 x Cake mix *\n",
      "1 can water chestnuts, chopped\n",
      "1 large jar storebought BBQ sauce (Note: Put your favorite choice in here: hot and spicy \"Scott's\" vinegar base, which is my favorite, or sweet, or hickory...)\n",
      "1 package pie crust mix\n",
      "2 c. Pimm's No. 1\n",
      "2 tablespoons grill seasoning blend ( recommended: Montreal Steak Seasoning by McCormick)\n",
      "4 cups cinnamon-flavored rice cereal squares\n",
      "1 pound skinless, boneless chicken breast halves, cut into 1-inch cubes\n",
      "1 Shirasu\n",
      "cranberry - flavored tea bags\n",
      "1 jar John West Boysenberries, liquid removed & syrup reserved\n",
      "1 pkg. spaghetti, cooked and drained\n",
      "1/2 cup walnuts (2 oz), coarsely chopped and \">toasted\n",
      "1 can of Pillsbury pizza dough\n",
      "4 whole Thai Bird Chilies, Stems Removed (add More/less) For Spice\n",
      "For the cheese crackers:\n",
      "1/4 tsp (1 ml) to 1/2 tsp (2 ml) Molly McButter roasted garlic flavor granules (or garlic powder if preferred)\n",
      "6 can chicken broth\n",
      "2 tablespoons fine ginger shreds (see Note)\n",
      "8 thick bacon slices, cooked until crisp\n",
      "**available at specialty produce markets\n",
      "3 can Chopped clams\n",
      "1/4 cup french bean diced\n",
      "2 cans White Northern Beans (15.5 Oz Cans), Drained\n",
      "1/2 cup fresh baby mozzarella balls\n",
      "6 circles of pastry to top your bites.\n",
      "6 Tbsp. Kraft Roasted Red Pepper with Parmesan Dressing, divided\n",
      "2 whole 2-inch In Diameter Goat Cheese Rounds, Cut In Half Length-wise\n",
      "burger of choice\n",
      "1 box White Cake Mix (18 Ounce Box, Without The Pudding In The Mix)\n",
      "less depending on your taste)\n",
      "1 pkt Yeast, active dry or possibly compressed\n",
      "1 to 2 Egg(s)\n",
      "1 quart. cucumber chunks\n",
      "1 x Sweet DOUGH MASTER RECIPE\n",
      "3 c. Very cool, unsalted butter, cut into chunks\n",
      "4 filets mignons or sirloin (New York) strip steaks, each 6 to 8 ounces\n",
      "1 box Jiffy corn mix\n",
      "100 grams Homemade natural leaven (starter)\n",
      "1/2 pounds, 2-58 ounces, weight Oats\n",
      "8 bags black tea\n",
      "McDonald's Clone\n",
      "7 pasilla chiles*\n",
      "25 to 30 store-bought empanada disks, defrosted\n",
      "1 Large Box Strawberry Jello\n",
      "2 cans cream of chicken soup (you can use cream of mushroom - taste is different)\n",
      "pickle juice\n",
      "2 can pink lemonade frozen juice concentrate\n",
      "Sun - Maid golden seedless raisins\n",
      "2 1/2 tsp Extra virgin olive oil *Note\n",
      "2 jars of favorite alfredo sauce\n",
      "One 6 oz (170g) can tuna fish in water, drained and flaked\n",
      "2 cans tomatoes, stewed, canned mexican styled\n",
      "4 small eggplants (1-1/2 lb./675 g), ends trimmed, cut lengthwise in half Safeway 1 ea For $1.28 thru 02/09\n",
      "2 Sunchokes\n",
      "100 ml kikomon soup base sauce\n",
      "1/2 cups Durkee's French Fried Onions (Optional) For Top\n",
      "For the brussels sprouts:\n",
      "1 European cucumberpeeled, halved lengthwise, seeded and thinly sliced crosswise\n",
      "2+ c. chicken broth\n",
      "1 package chocolate chips (semi-sweet) 12 oz\n",
      "1/2 pounds, 78 ounces, weight Barley, Cooked In Chicken Stock According To Package Instructions\n",
      "7 1/2 cups Kraft Sauce Works Dijon Bistro Sauce\n",
      "1/4 cantaloupepeeled, seeded and cut into cubes, plus 1 cantaloupe wedge\n",
      "1/2 whole Large, Cabbage, Washed, Cored And Loose Leaves Removed\n",
      "3 peaches, each cut into 12 slices\n",
      "6 Satoimo (taro)\n",
      "8 ounce Green Chiles, Minced, *\n",
      "8 ounce Salted sardines rinsed, beheaded, and roughly minced\n",
      "1 green pepper. minced\n",
      "425 can Corn kernels\n",
      "2 whole Large Boneless, Skinless Chicken Breasts\n",
      "1 can peach halves in juice, 400g or so\n",
      "1 medium thinly sliced carrot Target 2 lb For $3.00 thru 02/06\n",
      "1 container Strawberries, Diced, 8 Ounce Container\n",
      "1 package Frozen Spinach, Defrosted And Drained, 10 Oz\n",
      "A heavy-bottomed, 8-quart, 12-inch saucepan\n",
      "1 3/4 cups fat-free, less-sodium beef broth, divided\n",
      "3 lb. small red potatoes (about 12), cooked, cooled and quartered King Sooper's 5 lb For $1.88 thru 02/09\n",
      "1 box lime jello (3oz)\n",
      "1 bag of sweetened coconut\n",
      "2 oz (60 mil vodka\n",
      "*These special squashes are available at many specialty produce markets.\n",
      "1-tsp vannilla\n",
      "1 2-pound monkfish tail, skin off\n",
      "3 cups hard white whole wheat flour, or more as needed\n",
      "2 Tablespoons Soy Paste\n",
      "4 med russet potatoes - (abt 1 3/4 lbs) unpeeled\n",
      "1 packages of ranch dip mix\n",
      "2 can Campbell's golden mushroom soup\n",
      "1 box Red Velvet Cake mix\n",
      "1/2 cups Mini Mozzarella Balls\n",
      "2 medium leeks, white parts and a little of the light green part, halved and sliced thinly. (Note -- Make sure to wash thoroughly, after cutting in half, pat dry.)\n",
      "1 pkg. fun-size Milky Way Bars\n",
      "4 limes, juiced or 3/4 cup\n",
      "1/4 cup finely chopped parsley, or (more to taste)\n",
      "1 package mccormick au jus sauce\n",
      "1 packages Pioneer Brown Gravy\n",
      "(makes about 4 quarts)\n",
      "melon seeds\n",
      "2 envelopes unflavored gelatin powder\n",
      "4 ounces sashimi grade Monterey albacore, chopped into a near paste\n",
      "1 broiler-fryer chicken (2-1/2 to 3 lb.), cut up Safeway 1 lb For $1.99 thru 02/09\n",
      "1 can tuna, mashed\n",
      "Progresso Traditional Carb Monitor soup chicken cheese enchilada soup\n",
      "1 9 oz (252 grm). package refrigerated tortelloni\n",
      "3 bunches leafy greens, (such as Swiss Chard, Escarole and Kale), washed, stemmed and cut into pieces\n",
      "12 cooked prawn cutlets Safeway 1 lb For $8.99 thru 02/09\n",
      "1 large onion, sliced King Sooper's 1 lb For $0.99 thru 02/09\n",
      "2 bottles balsamic vinegar, reduced to 20 percent to syrup\n",
      "1 box linguine noodles\n",
      "1 envelope Liptons savory herb & garlic mix\n",
      "2 cans Water (use The Soup Can)\n",
      "2 cans Refrigerated Canned Crescent Rolls, 8 Ounce Cans\n",
      "10-inch cast-iron skillet (Yes, it really has to be a cast-iron skillet. If you dont have one, get onenothing else will do the job.)\n",
      "1 package Plum Tomatoes\n",
      "2 box Jiffy Corn Muffin Mix\n",
      "4 Granny Smith apples (2 pounds)peeled, cored and cut into 1/2-inch dice\n",
      "1/4 cups + 5 Tablespoons Dulce De Leche\n",
      "1 can Refrigerated Crescent Roll Dough (8 Ounce Can)\n",
      "1 jar artichoke hearts, drained\n",
      "1 can of slice peach, drained\n",
      "brambleberries\n",
      "2 cans dark kidney beans\n",
      "instant chocolate fudge pudding\n",
      "1-1/4 cups sliced strawberries Safeway 1 lb For $3.99 thru 02/09\n",
      "powdered cocoa mix\n",
      "40 ml Heavy cream + milk\n",
      "1 slice reduced-sodium bacon, cooked, broken into 4 pieces Rite Aid 2 For $7.00 thru 02/06\n",
      "1 packages bacon; baked crispy and diced\n",
      "4 cups sauerkraut (preferably fresh), rinsed, drained, squeezed dry (from one 32-ounce jar)\n",
      "1 packages shredded sharp cheddar cheese\n",
      "2/3 cup roasted, salted peanuts, coarsely chopped\n",
      "5 Koya dofu\n",
      "==Stock==\n",
      "2 teaspoons Hidden Valley Farmhouse Originals Garlic & Herb Dressing & Seasoning Mix\n",
      "1 Ikura\n",
      "2 lb. red potatoes (about 6), cut into 1/2-inch chunks King Sooper's 5 lb For $1.88 thru 02/09\n",
      "1 box Knorr's Swiss Oxtail Soup mix\n",
      "10 -inch deep-frying pan\n",
      "2 pieces New Korean BBQ! Yuzu-Scented Samgyeopsal\n",
      "1/2 cup dried soy chunks (textured vegetable protein)\n",
      "1/2 pound escarolelarge stems discarded, leaves coarsely chopped (3 cups)\n",
      "1 fresh mango, chopped (about 1 cup) Whole Foods 2 ea For $4.00 thru 02/09\n",
      "Oatmeal:\n",
      "kishka\n",
      "gluten - free soy sauce\n",
      "1 envelope Dream Whip, prepared\n",
      "2 packages vermicelli,soaked in warm water 15 mins before use\n",
      "2 envelope lipton onion soup mix\n",
      "1/2 box fine spaghetti, uncooked (4 ounce.)\n",
      "crumpets\n",
      "13 cups Natural, Creamy Peanut Butter\n",
      "14 ounces skinless, boneless chicken breast meat - cut into bite-size pieces\n",
      "1 teaspoon fresh thyme leaves (or fresh basil) plus 4 to 6 sprigs, for garnish\n",
      "1 c. Den Miso (see below) Pickled ginger and plums optional garnish\n",
      "1 can whole corn, drained\n",
      "540 ml chick-peas\n",
      "Skittles candies\n",
      "1/2 cup crumbled Cotija or farmers cheese (2 ounces), optional\n",
      "1 pack frzn broccoli (10 ounce.)\n",
      "Banana Nut Frosting:\n",
      "4 boneless, skinless chicken breast halves (about 1 1/4 pounds), cut into large cubes\n",
      "1 Bichamel sauce recipe\n",
      "1 Very hardy sliced bread of your choice. (I use sourdough)\n",
      "2 slice Butterball Thin & Crispy, Turkey Bacon\n",
      "cornflake crumbs, extra\n",
      "1 bag frzn broccoli (still frzn)\n",
      "1 (4 ounce) package Carroll Shelby chili kit (use 1/3 or so of the package)\n",
      "1 box Certo Light Pectin Crystals\n",
      "2 pkg. Royal Pistachio pudding\n",
      "1 tablespoon grill seasoning blend (recommended:Montreal Steak Seasoning by McCormick Grill Mates)\n",
      "1 packages Pie crust\n",
      "strawberry juice concentrate\n",
      "cooked angel hair pasta\n",
      "small to medium tomatoes 16\n",
      "8 inch pie shell\n",
      "8 to 10 whole Tuscan-style peperoncini in vinegar, drained, seeded, and sliced in strips ( 1/2 cup), or more to taste\n",
      "1/2 packages mushrooms, chopped\n",
      "1/2 Tbsp soy sauce *\n",
      "1/2 lb white american cheese cut in half\n",
      "1 tapatio\n",
      "low - fat firm silken tofu\n",
      "kheer\n",
      "2 packages milk chocolate prefer german\n",
      "Grated Parmiagano-Reggiano, as needed\n",
      "zest + juice of 1 lemon\n",
      "EVOO (extra-virgin olive oil) for liberal drizzling, plus 2 tablespoons\n",
      "20 to 25 grams Shio-kombu\n",
      "1 can Lowfat milk, evaporated, 5 1/2 ounce.\n",
      "2/3 cup 1/2-inch butternut squash cubes\n",
      "18 (7 by 3 1/2-inch) sheets dry no-boil lasagne (about 1 pound)\n",
      "1 approx. 1 tablespoon Doubanjiang\n",
      "1 1/2 teaspoons chopped fresh dill**\n",
      "2 packages Tortilla shells\n",
      "1 (9 ounce) package milk chocolate kisses (recommended -- Hershey's Kisses)\n",
      "2 tbsp (25 mL) confectioners (icing) sugar, sifted\n",
      "2 tsp Chopped canned chipotle chilies*\n",
      "3 green garlic plants (bulbs and stalks), trimmed of green parts and chopped fine\n",
      "13 tablespoons, 1- 1/2 teaspoons, 1- 1/4 pinches Heavy Cream\n",
      "1/3 cup heavy cream, or more if necessary\n",
      "1 bunch Shungiku (Chrysanthemum greens)\n",
      "1 bunch Scallions Or Spring Onions, , Finely Chopped\n",
      "1 1/2 c. dry small white beans rinsed (or possibly 2 [15-ounce] cans white beans, see comments)\n",
      "1 can Coconut Cream (425ml)\n",
      "4 bars of crunchie\n",
      "tender quick\n",
      "six 7-inch squares instant (no-boil) lasagne*\n",
      "4 small apples, peeled, cored King Sooper's 1 lb For $0.87 thru 02/09\n",
      "1 Pink sparkles (for frosting and nose).\n",
      "baby artichokes\n",
      "Tomato-Ginger Compote, for serving\n",
      "1 tbsp + 1 tablespoon Katakuriko + water\n",
      "20 grape clusters, for garnish\n",
      "Sandwich spread, to taste\n",
      "12 cups Vegetable Or Chicken (for Non-vegan) Stock, Plus A Bit More If Needed\n",
      "kale leaf\n",
      "1/2 Recipe No Excuses Pie Dough (Full Recipe Follows--save The Other Half For Another Use) Or Use Your Favorite Crust Recipe (you'll Need Enough To Line A 10-inch Tart Pan)\n",
      "4 x Sun-dry tomatoes -- (not In oil)\n",
      "3/4 lb. lean ground beef King Sooper's 1 lb For $3.99 thru 02/09\n",
      "1 packages bean sprout, washed and remove husk\n",
      "1 can frozen cream of shrimp soup, thawed\n",
      "2 tbsp. sunflower\n",
      "1 container of small multi-colored sprinkles\n",
      "1 can Tomato Sauce large\n",
      "3 can chickpeas\n",
      "1 packages of mushroom soup to thicken stew ( optional)\n",
      "3 OSCAR MAYER Jalapeno Dogs, sliced\n",
      "1 your favorite Pound cake pan\n",
      "3/4 pound cooked Alaska king crab leg in shell (1 leg), thawed if frozen and split lengthwise\n",
      "white christmas flavored syrup\n",
      "1 to 2 packs Coffee creamer (or milk)\n",
      "Stir-Fry:\n",
      "liver spread\n",
      "chana dal powder\n",
      "herb with garlic soup mix\n",
      "12 ounces, weight Mozzarella Cheese Pearls\n",
      "6 cups ripe, fresh, red tomatoes, seeded\n",
      "1 bag of tortilla chips of your choice\n",
      "1 can Thank You brand pie filling (can be blueberry, cherry or possibly strawberry)\n",
      "1- 1/2 pound Boneless, Skinless Chicken Breast, In One Inch Pieces\n",
      "1/2 cups All-purpose, Unbleached Flour\n",
      "1 can hot dog\n",
      "9 ounce Cold Whip\n",
      "2 teaspoons fennel seeds, or more to taste - lightly crushed\n",
      "1 pkt Dry onion soup mix,**\n",
      "irish soda bread\n",
      "8 ounces, weight Pasta Of Your Choice (We Like Something Bite-Sized, And I Usually Use A Little More Than 8 Oz. Maybe 10-12 Oz.)\n",
      "5 big black berrys\n",
      "2/3 cup flocon avoine\n",
      "rouladen beef\n",
      "croaker fish\n",
      "1 can Tomato paste 6 oz\n",
      "2 skin-on, bone-in chicken breasts, cut airline-style (See Cook's Note)\n",
      "PictSweet seasoning blend\n",
      "1 pound fregola (see Note, page 93)\n",
      "6 ounces, weight Package, Strawberry Jello\n",
      "1-2 cans diced green chiles\n",
      "whole berry cranberry sauce\n",
      "Med. amount of cheese of choice\n",
      "2 c. Mediterranean Tomato-Caper Sauce, (page 709)\n",
      "1 cup beef or chicken broth, low sodium + extra to thin grits as they cook\n",
      "4 Boneless, Center Cut Pork Chops\n",
      "Blueberry Tequila Filling, recipe follows\n",
      "5 whole Oreos + More For Topping\n",
      "2 cups fresh blueberry coulis\n",
      "5 strips Thick-cut Bacon Slices, Cut Crosswise Into Thin Strips\n",
      "1 can pineapple chunk, reserving a few tablespoons of it own juice\n",
      "1 package Betty Crocker's hamburger helper (or any mix of meat loaf seasonings and bread crumbs)\n",
      "One 2-inch orange zest strip\n",
      "100 grams Kaki-no-tane (small rice crackers glazed in spicy soy sauce, with a handful of peanuts)\n",
      "1/4 c.molasses\n",
      "1- 1/2 cup Crumbled Panela (or Queso Fresco)\n",
      "1 1/2 can chopped tomatoes\n",
      "Two 5-to 6-pound Long Island (Peking) ducks, thawed if frozen\n",
      "Accompaniments: dim sum dipper and cranberry-teriyaki glaze<epi:recipelink>.</epi:recipelink>\n",
      "2 cans (4-3/8 oz. each) John West Smoked Tuna Slices, drained\n",
      "1 tbsp Mentsuyu (concentrate)\n",
      "13 cups Soft, Fresh Goat Cheese (sold In Tubs), At Room Temperature\n",
      "For chocolate web and spider\n",
      "1 jar of Blueberry Preserves\n",
      "1- 1/4 pound Skinless, Boneless Chicken Breasts\n",
      "1 pkg. miniature muffin paper c.\n",
      "1 1/2 teaspoons table salt*\n",
      "DUXELLES\n",
      "1 can vanilla cream cheese frosting\n",
      "8 slices crusty peasant style bread cut 1/2-inch thick\n",
      "1 sm. can Hershey's syrup\n",
      "1 L)\n",
      "1 pack of roasted potato seasoning.\n",
      "1 tbsp Agave syrup +/-\n",
      "1 stick Room Temperature, Unsalted Butter\n",
      "1.8 kg free range chicken, neck removed Safeway 1 lb For $1.99 thru 02/09\n",
      "2 sliced fresh strawberries Safeway 1 lb For $3.99 thru 02/09\n",
      "1 bag Pepperidge Farm regular stuffing\n",
      "1 can water-packed light tuna, drained\n",
      "4 cups small-dice cooked chicken (from about 1 1/2 pounds of boneless, skinless chicken breasts or 1 [4- to 5-pound] whole chicken)\n",
      "1/2 packages sour cream and onion potatoe chips\n",
      "1 packages graham crackers\n",
      "5 whole jalapeno pepper seeded and chopped (optional)\n",
      "2 (8 count) pkg. warm dogs\n",
      "baking cocoa\n",
      "dry fettuccine\n",
      "1 lg. pkg. red Jello (or possibly 2 sm. pkg.)\n",
      "2 tbsp unfiltered apple cider vinegar with the mother\n",
      "2 teaspoons essence Emeril's Southwest\n",
      "1 packages mixed Greens\n",
      "1- 1/2 pound Chopped, Cooked Chicken Breast\n",
      "1 pound boneless, skinless chicken breast, sliced and salted\n",
      "Campbell's condensed chicken broth\n",
      "1 box vanilla instant pudding\n",
      "1 shredded lettece\n",
      "Small hand strainer\n",
      "1 cup Kikkoman Teriyaki Marinade & Sauce\n",
      "2 Umeboshi ()\n",
      "1 Pack chuck chopped family back\n",
      "dried bamboo\n",
      "1/2 bags Pepperidge Farms Herb Seasoned Stuffing (16 Oz)\n",
      "1 qt. MIRACLE WHIP Light Dressing Family Dollar $3.50 thru 02/07\n",
      "2 cans liquid removed mushrooms (optional)\n",
      "1/2 tsp. sour salt (optional) *\n",
      "1 sm can tomato sauce\n",
      "1/2 can coconut\n",
      "1 pack Shirataki noodles\n",
      "Recipe from Every Day Is a Party, by Emeril Lagasse, with Marcelle Bienvenu and Felicia Willett, published by William Morrow, 1999\n",
      "1 small Vidalia onion, sliced Safeway 1 lb For $1.49 thru 02/09\n",
      "1 Tbsp. Maxwell House Cafe, French Vanilla Cafe Instant Coffee Beverage Mix\n",
      "1/2 pkg. frozen chopped spinach, thawed, drained, and squeezed dry\n",
      "1 can very cherry fruit cocktail\n",
      "1/2 cup finely slivered inner radicchio leaves\n",
      "1 large can red enchilada sauce\n",
      "1 packages firm tofu, cut into 1/2-inch cubes\n",
      "sesame linguine\n",
      "1 packages already cut squid\n",
      "8 boneless skinless chicken thighs (1-1/2 lb.), cut crosswise in half King Sooper's 1 lb For $1.19 thru 02/09\n",
      "16 ounce Can whole tomatoes**\n",
      "4 piece Chilean sea bass - (6 ounce ea) Salt to taste Freshly-grnd black pepper to taste\n",
      "1 whole Roasted Chicken From Super Maket Deli, Remove Skin And Bones And Shred Meat Into Bite Sized Pieces\n",
      "1/4 tsp Korean red pepper pwdr\n",
      "14 cup prepared pesto sauce (use Pesto (Food Processor))\n",
      "1 pound Frozen, Shelled Edamame, Thawed\n",
      "Assorted mustards, such as whole-grained, Dijon, or French's (optional)\n",
      "7 ounces (14 tablespoons) plus 2 ounces (4 tablespoons) unsalted butter, cut into pieces\n",
      "1 packages ready made rice e.g uncle bens\n",
      "pickling spices\n",
      "3 cups Kraft Rancher's Choice Dressing\n",
      "1 can diced tomatoes - (14 1/2 ounce) undrained\n",
      "1 recipe Crunchy Asian Slaw, recipe follows\n",
      "426 ml Single cream, (15 floz) Salt to taste\n",
      "16 ounces, weight Canned Whole Tomatoes In Natural Juices\n",
      "1 can peaches slices\n",
      "2 16 fl oz bottle , favorite flavor\n",
      "Pillsbury pecan swirl quick bread and coffeecake mix\n",
      "1 (1.2-ounce) package dry Caesar salad dressing mix*\n",
      "4 Washed-rind cheeses\n",
      "2 cans whole tomatoes, minced\n",
      "paper baking cups\n",
      "1 packet no-boil wholewheat lasagne sheets\n",
      "1 whole Sheet Of Frozen Puff Pastry That Has Thawed In The Refrigerator\n",
      "4 boneless, thick-cut, pork loin chops, rinsed and patted dry\n",
      "2 slices OSCAR MAYER Bacon, crisply cooked or OSCAR MAYER Ready to Serve Bacon Rite Aid 2 For $7.00 thru 02/06\n",
      "1 jar chunk-style applesauce\n",
      "1 can lime juice sweetened\n",
      "1 whole MorningStar Farms Buffalo Chik Patties (veggie Buffalo Chicken)\n",
      "1 pound monkfish, diced\n",
      "1 pkt wide egg noodles - (16 ounce)\n",
      "1 packages Frozen Whipped Topping - 8 Ounces\n",
      "1- 1/4 cup Mini Marshhallows\n",
      "1 (9-inch) store-bought pound cake loaf\n",
      "2 can red kidney beans\n",
      "1 square Hanpen\n",
      "lemon - lime flavored soda\n",
      "1 (5.5-ounce) can spicy vegetable juice*\n",
      "1 pkg. Good Season's Italian dressing\n",
      "2 pounds, 3-13 ounces, weight New Potatoes, Washed And Scrubbed\n",
      "Reserved, Cleaned Shellfish\n",
      "6 red M&Ms .\n",
      "395 can condensed milk\n",
      "1 packet instant vanilla pudding\n",
      "1 tsp sweet soy sauce *\n",
      "1 packages Weber white wine & herb marinade\n",
      "1 tablespoon sriracha\n",
      "1 pkt Refrigerated buttermilk Biscuits\n",
      "2 packages maria cookies\n",
      "1 tbsp Tobesco Sauce\n",
      "1 can beer\n",
      "urad dal four\n",
      "1 donut cutter\n",
      "4 ounce. each cooked fresh or possibly thawed and liquid removed frzn crabmeat and cooked, shelled and deveined shrimp\n",
      "mud fish\n",
      "Lightly oiled cookie sheet\n",
      "1 package dry apricot (in pressed sheet form)\n",
      "1 jar chicken gravy\n",
      "1/2 cups Chopped, Roasted Almonds\n",
      "2 cups sliced shiitake mushroom caps (about 1 [3.5-ounce] package)\n",
      "Pinto Beans and Chipotle Peppers in Zesty Santa Fe Sauce\n",
      "4 tablespoons Q's Bulgogi, recipe follows\n",
      "2 tablespoons coffee liqueur, e.g. Kahlua Walgreens $6.99 thru 02/06\n",
      "1 Bottle BBQ Sauce\n",
      "1 recipe basic pasta dough rolled out to thinnest setting, recipe follows\n",
      "1 A thick plastic bag\n",
      "Apricot-Honey\n",
      "2 x nd layer:\n",
      "1 cold, cooked whole chicken breast, cut into chunks\n",
      "48 slices 9-grain bread slices\n",
      "Cured black (Sicilian) olives, pitted brine\n",
      "1 Tbsp. KRAFT Honey Mustard with Chipotle Dressing\n",
      "1 Tbsp. CLASSICO Traditional Basil Pesto Sauce and Spread\n",
      "1/4 cup green (preferably Picholine) olives, pitted\n",
      "1 pound boneless, skinless chicken breasts, thighs, or a mixture of both, cut crosswise into 1/4-inch-thick pieces\n",
      "1/2 teaspoon coarsely ground black, plus extra for sprinkling\n",
      "wine vinegar\n",
      "other possible ingredients (see comments)\n",
      "pickled hot red chilies\n",
      "1 package Fresh Chicken Wings, Package Size According To Desired Amount Of Servings\n",
      "1 large zip loc bag\n",
      "Italian cut green beans\n",
      "1 c. canned beef consomme undiluted\n",
      "144 slices OSCAR MAYER Bacon, cooked, drained Rite Aid 2 For $7.00 thru 02/06\n",
      "10 FOODS THAT LAST FOREVER\n",
      "mocha - flavored non - dairy creamer\n",
      "3 packages butter\n",
      "3 medium Taters\n",
      "2 bag lowfat milk chocolate chips (12-oz bags)\n",
      "1 (12-ounce) center-cut tuna loin\n",
      "1 1/2 cups half and-half\n",
      "Lime-peel spiral, for garnish\n",
      "1 orange, peeled, cut into segments, reserving any juice Safeway 1 lb For $1.28 thru 02/09\n",
      "1/4 c. brandy*\n",
      "1 pizza base sauce, I used some mexican salsa I had left over.\n",
      "1-1/2 cups NUTTER BUTTER Creme Variegate, warmed slightly\n",
      "One 1 in (2.5cm) piece fresh ginger, peeled and chopped\n",
      "Funnel Cake Batter:\n",
      "1 (Yuzu Flavored Tofu Mayonnaise\n",
      "8 (5-ounce) turbot fillets,* skinned\n",
      "3 tablespoons unsweetened instant ice tea mix\n",
      "4 whole Boneless Chicken Breased, Cubed (or 1 Pound Shrimp)\n",
      "1 kit kats\n",
      "4 long narrow 18-inch baking pans\n",
      "4 cups ratatouille (see recipe)\n",
      "2 packages sugar-free instant vanilla pudding mix\n",
      "1 box 4.7 Ounce Box, Au Gratin Potatoes (Any Brand *Note: Do Not Use Seasoning Packet In Box For This Recipe)\n",
      "1/2 cup (125 ml) plus 1 tbsp (15 ml) granulated sugar, used in two separate measurements\n",
      "2 lbs skinless boneless turkey breast, cut into 1/2-inch (1 cm) cubes (see tips)\n",
      "Cream Cheese Filling:\n",
      "3 Citrus Shortcake squares, each cut into 8 cubes\n",
      "*Available at Asian markets and in the Asian section of some supermarkets.\n",
      "1 medium tomatohalved, seeded and cut into 1/3-inch dice\n",
      "ziploc bags\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for item in ingredients_list_unique:\n",
    "    i += 1\n",
    "    print(item)\n",
    "    if (i == 1000):\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients_list = pd.DataFrame(list(data['ingredients']))\n",
    "print(ingredients_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3500\n",
      "['salt', 'pepper', 'butter', 'garlic', 'sugar', 'flour', 'onion', 'olive oil', 'water', 'ground', 'olive', 'powder', 'sliced', 'eggs', 'black pepper', 'milk', 'cheese', 'cream', 'lemon', 'chicken', 'sauce', 'tomatoes', 'brown', 'white', 'egg', 'onions', 'vinegar', 'vegetable', 'brown sugar', 'lemon juice', 'ground black pepper', 'parsley', 'cinnamon', 'garlic cloves', 'extract', 'vegetable oil', 'vanilla', 'baking powder', 'vanilla extract', 'unsalted butter', 'ginger', 'chocolate', 'leaves', 'soda', 'parmesan', 'tomato', 'celery', 'potatoes', 'kosher salt', 'mustard', 'cheddar', 'juice', 'baking soda', 'kosher', 'sour cream', 'cilantro', 'soy sauce', 'cream cheese', 'parmesan cheese', 'cheddar cheese', 'oregano', 'red pepper', 'carrots', 'clove', 'chicken broth', 'mushrooms', 'honey', 'packed', 'orange', 'bread', 'thyme', 'oil', 'basil', 'seasoning', 'extra', 'margarine', 'mayonnaise', 'bell pepper', 'cayenne', 'garlic powder', 'nutmeg', 'plus', 'cloves', 'garlic minced', 'white wine', 'bacon', 'peppers', 'ground cinnamon', 'heavy cream', 'boneless', 'garnish', 'paprika', 'black', 'beans', 'toasted', 'virgin olive oil', 'extra virgin olive oil', 'yellow', 'green onions', 'chicken breasts']\n"
     ]
    }
   ],
   "source": [
    "#Master ingredient list\n",
    "\n",
    "unique_ingredient_list = np.load('ingredients.npy')\n",
    "\n",
    "unique_ingredient_list = unique_ingredient_list.tolist()\n",
    "print(len(unique_ingredient_list))\n",
    "print(unique_ingredient_list[0:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3500\n",
      "['salt', 'pepper', 'butter', 'garlic', 'sugar', 'flour', 'onion', 'olive oil', 'water', 'ground', 'olive', 'powder', 'sliced', 'eggs', 'black pepper', 'milk', 'cheese', 'cream', 'lemon', 'chicken', 'sauce', 'tomatoes', 'brown', 'white', 'egg', 'onions', 'vinegar', 'vegetable', 'brown sugar', 'lemon juice', 'ground black pepper', 'parsley', 'cinnamon', 'garlic cloves', 'extract', 'vegetable oil', 'vanilla', 'baking powder', 'vanilla extract', 'unsalted butter', 'ginger', 'chocolate', 'leaves', 'soda', 'parmesan', 'tomato', 'celery', 'potatoes', 'kosher salt', 'mustard', 'cheddar', 'juice', 'baking soda', 'kosher', 'sour cream', 'cilantro', 'soy sauce', 'cream cheese', 'parmesan cheese', 'cheddar cheese', 'oregano', 'red pepper', 'carrots', 'clove', 'chicken broth', 'mushrooms', 'honey', 'packed', 'orange', 'bread', 'thyme', 'oil', 'basil', 'seasoning', 'extra', 'margarine', 'mayonnaise', 'bell pepper', 'cayenne', 'garlic powder', 'nutmeg', 'plus', 'cloves', 'garlic minced', 'white wine', 'bacon', 'peppers', 'ground cinnamon', 'heavy cream', 'boneless', 'garnish', 'paprika', 'black', 'beans', 'toasted', 'virgin olive oil', 'extra virgin olive oil', 'yellow', 'green onions', 'chicken breasts']\n"
     ]
    }
   ],
   "source": [
    "#Sort by length of data\n",
    "# unique_ingredient_list = sorted(unique_ingredient_list, key=len, reverse=True)\n",
    "print(len(unique_ingredient_list))\n",
    "print(unique_ingredient_list[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2838\n",
      "2838\n",
      "['salt', 'pepper', 'butter', 'garlic', 'sugar', 'flour', 'onion', 'olive oil', 'water', 'ground', 'olive', 'powder', 'sliced', 'eggs', 'black pepper', 'milk', 'cheese', 'cream', 'lemon', 'chicken', 'sauce', 'tomatoe', 'brown', 'white', 'egg', 'vinegar', 'vegetable', 'brown sugar', 'lemon juice', 'parsley', 'cinnamon', 'garlic clove', 'extract', 'vegetable oil', 'vanilla', 'baking powder', 'vanilla extract', 'unsalted butter', 'ginger', 'chocolate', 'leave', 'soda', 'parmesan', 'tomato', 'celery', 'potatoe', 'kosher salt', 'mustard', 'cheddar', 'juice', 'baking soda', 'kosher', 'sour cream', 'cilantro', 'soy sauce', 'cream cheese', 'parmesan cheese', 'cheddar cheese', 'oregano', 'red pepper', 'carrot', 'clove', 'chicken broth', 'mushroom', 'honey', 'packed', 'orange', 'bread', 'thyme', 'oil', 'basil', 'seasoning', 'extra', 'margarine', 'mayonnaise', 'bell pepper', 'cayenne', 'garlic powder', 'nutmeg', 'plus', 'garlic minced', 'white wine', 'bacon', 'ground cinnamon', 'heavy cream', 'bonele', 'garnish', 'paprika', 'black', 'bean', 'toasted', 'yellow', 'green onion', 'chicken breast', 'green', 'sesame', 'wine', 'lime juice', 'corn', 'granulated sugar', 'ground pepper', 'coconut', 'spinach', 'ground beef', 'cayenne pepper', 'cornstarch', 'white sugar', 'wine vinegar', 'syrup', 'red onion', 'chili powder', 'rice', 'worcestershire sauce', 'ground cumin', 'mozzarella', 'orange juice', 'red wine', 'walnut', 'chocolate chip', 'unsweetened', 'pasta', 'pepper flake', 'minced garlic', 'temperature', 'almond', 'sea salt', 'yogurt', 'canola', 'chicken stock', 'skinless chicken', 'jalapeno', 'pecan', 'shrimp', 'wheat', 'dijon mustard', 'pineapple', 'lettuce', 'coriander', 'canola oil', 'dressing', 'rosemary', 'mozzarella cheese', 'raisin', 'powdered sugar', 'instant', 'buttermilk', 'sausage', 'dried oregano', 'sesame oil', 'roasted', 'scallion', 'cumin', 'semisweet chocolate', 'zucchini', 'whipping cream', 'apple', 'confectioners sugar', 'dry', 'crumb', 'stock', 'peanut butter', 'egg yolk', 'pork', 'red', 'cabbage', 'noodle', 'egg white', 'chili', 'tomato sauce', 'tortilla', 'beef', 'tomato paste', 'chive', 'yeast', 'shortening', 'peas', 'fillet', 'lemon zest', 'broccoli', 'cider vinegar', 'cooking spray', 'seed', 'shallot', 'balsamic vinegar', 'breadcrumb', 'more', 'turkey', 'potato', 'prepared', 'strawberr', 'bay leave', 'dried thyme', 'ground ginger', 'bread crumb', 'bay leaf', 'cocoa powder', 'lime', 'ketchup', 'bay', 'black bean', 'pumpkin', 'ice', 'onion powder', 'nuts', 'oats', 'dill', 'allspice', 'sesame seed', 'green pepper', 'sweetened', 'basil leave', 'ham', 'ground nutmeg', 'maple syrup', 'broth', 'banana', 'white pepper', 'cucumber', 'jack cheese', 'yellow onion', 'chil', 'turmeric', 'cake', 'cranberr', 'liquid', 'cold water', 'boiling water', 'monterey jack', 'peanut', 'avocado', 'wheat flour', 'cornmeal', 'hot sauce', 'blueberr', 'dried basil', 'a', 'mint', 'curry powder', 'sherry', 'bouillon', 'coconut milk', 'melted butter', 'mushroom soup', 'green bean', 'spaghetti', 'ice cream', 'topping', 'corn syrup', 'breast halve', 'parsley leave', 'feta cheese', 'peppercorn', 'boneless chicken', 'halfandhalf', 'apple cider', 'half', 'pastry', 'dry yeast', 'ricotta', 'salsa', 'white vinegar', 'roast', 'white rice', 'pure vanilla', 'cake mix', 'warm water', 'chip', 'rice vinegar', 'ground clove', 'fennel', 'molasse', 'orange zest', 'condensed milk', 'cocoa', 'asparagu', 'cherry tomatoe', 'black olive', 'gelatin', 'plain yogurt', 'tarragon', 'cooked chicken', 'dry mustard', 'salmon', 'whipped topping', 'cracker', 'almond extract', 'ground coriander', 'steak', 'cool whip', 'kidney', 'ricotta cheese', 'yolk', 'green chil', 'pudding', 'raspberr', 'bottle', 'garlic salt', 'beef broth', 'swiss cheese', 'low sodium', 'sage', 'hot water', 'flour tortilla', 'flavor', 'peanut oil', 'other', 'rum', 'coarse salt', 'kidney bean', 'plum tomatoe', 'caper', 'chile', 'sweet potatoe', 'cherr', 'chipotle', 'vegetable broth', 'zest', 'whipped cream', 'sliced mushroom', 'macaroni', 'crust', 'cardamom', 'liqueur', 'fish sauce', 'pine nut', 'cilantro leave', 'meat', 'cake flour', 'mint leave', 'strawberry', 'sodium', 'baby spinach', 'italian seasoning', 'unbleached', 'grain', 'rolled oat', 'jalapeno pepper', 'white chocolate', 'thyme leave', 'cauliflower', 'chicken soup', 'chicken thigh', 'crosswise', 'beer', 'evaporated milk', 'pepper sauce', 'coffee', 'ground turkey', 'raspberry', 'white onion', 'cereal', 'vegetable stock', 'cherry', 'tofu', 'mix', 'stick butter', 'chickpea', 'fat free', 'marshmallow', 'hot pepper', 'brandy', 'dried cranberr', 'brown rice', 'salad dressing', 'cottage cheese', 'lamb', 'sliced almond', 'frozen pea', 'filling', 'taco seasoning', 'flavored', 'roll', 'romaine lettuce', 'minced onion', 'peache', 'grated orange', 'hamburger', 'dough', 'arugula', 'white bread', 'sweet onion', 'rice wine', 'horseradish', 'grated parmesan', 'skim milk', 'vodka', 'black peppercorn', 'barbecue sauce', 'chili sauce', 'dried parsley', 'cook', 'sunflower', 'icing', 'chicken bouillon', 'corn tortilla', 'pineapple juice', 'frozen spinach', 'coloring', 'greek yogurt', 'goat cheese', 'grated nutmeg', 'salad', 'apple juice', 'red chili', 'blue cheese', 'bread flour', 'artichoke heart', 'dark chocolate', 'cranberry', 'breast', 'ground pork', 'spinach leave', 'food coloring', 'of tartar', 'applesauce', 'slivered almond', 'sprout', 'ground allspice', 'blanched', 'saffron', 'flaked coconut', 'shell', 'bittersweet chocolate', 'ice cube', 'celery rib', 'fruit', 'butternut', 'concentrate', 'minced ginger', 'pork chop', 'eggplant', 'tenderloin', 'beef stock', 'soup mix', 'minced parsley', 'pie crust', 'medium tomatoe', 'dry sherry', 'butternut squash', 'corn kernel', 'egg noodle', 'plain flour', 'lentil', 'masala', 'seasoning mix', 'preserve', 'garam masala', 'ground turmeric', 'tortilla chip', 'cooked rice', 'onion soup', 'sirloin', 'tuna', 'caster', 'mirin', 'marjoram', 'mango', 'sake', 'vegetable shortening', 'curry', 'apricot', 'caster sugar', 'prosciutto', 'italian sausage', 'leaf parsley', 'romano', 'dill weed', 'chunky', 'leek', 'all-purpose flour', 'icing sugar', 'soup', 'splenda', 'cracker crumb', 'frozen corn', 'any', 'vinaigrette', 'broccoli floret', 'spice', 'sprig', 'pie filling', 'stuffing', 'date', 'baguette', 'milk chocolate', 'serrano', 'cooking oil', 'yukon gold', 'seasoning salt', 'boneless pork', 'kalamata', 'mustard seed', 'chopped', 'spring', 'blend', 'pesto', 'chili pepper', 'pear', 'lettuce leave', 'paste', 'purple', 'dried rosemary', 'essence', 'puff pastry', 'french bread', 'raw', 'velveeta', 'roma tomatoe', 'caramel', 'condensed cream', 'vanilla bean', 'in water', 'american', 'romano cheese', 'light cream', 'chestnut', 'lemon peel', 'firm tofu', 'organic', 'anise', 'crabmeat', 'squash', 'instant coffee', 'sodium chicken', 'pumpkin puree', 'caraway', 'salmon fillet', 'gingerroot', 'orange peel', 'medium shrimp', 'candy', 'natural', 'medium potatoe', 'soft', 'oyster', 'grape', 'yellow mustard', 'russet potatoe', 'maraschino', 'pistachio', 'creme fraiche', 'smoked paprika', 'lemon rind', 'ranch dressing', 'ground cardamom', 'coconut oil', 'quinoa', 'dried red', 'cinnamon stick', 'cannellini', 'green olive', 'quick', 'bourbon', 'espresso', 'sundried tomatoe', 'gold potatoe', 'grape tomatoe', 'garbanzo', 'jelly', 'pecan halve', 'pork loin', 'elbow macaroni', 'hamburger bun', 'medium zucchini', 'watercre', 'pie spice', 'sugar substitute', 'oatmeal', 'oregano leave', 'herb', 'button mushroom', 'canned tomatoe', 'cashew', 'stewed tomatoe', 'yellow squash', 'ice water', 'sherry wine', 'baby carrot', 'bean sprout', 'pinto bean', 'reduced fat', 'sriracha', 'fish', 'hazelnut', 'scallop', 'asian', 'ginger root', 'basmati', 'pasta sauce', 'garbanzo bean', 'barley', 'unsweetened chocolate', 'sunflower seed', 'baking potatoe', 'pork sausage', 'beef bouillon', 'peach', 'prepared mustard', 'kale', 'unsweetened coconut', 'blackberr', 'shiitake mushroom', 'jello', 'egg substitute', 'golden raisin', 'penne pasta', 'hoisin sauce', 'white flour', 'dark rum', 'american cheese', 'couscou', 'water chestnut', 'unflavored gelatin', 'lime zest', 'coriander seed', 'tomato juice', 'radishe', 'low fat', 'berr', 'smith apple', 'vanilla pudding', 'beet', 'pork tenderloin', 'salted butter', 'butterscotch', 'celery seed', 'vidalia', 'flaked', 'rib', 'sweet potato', 'oyster sauce', 'champagne', 'starch', 'frosting', 'parsley flake', 'rice flour', 'basmati rice', 'turkey breast', 'dried dill', 'pepperoni', 'poppy seed', 'salted', 'distilled', 'cannellini bean', 'new potatoe', 'stuffing mix', 'graham cracker', 'tahini', 'pineapple chunk', 'salad green', 'mixed vegetable', 'large egg', 'linguine', 'provolone cheese', 'poblano', 'marinara sauce', 'whiskey', 'fennel seed', 'sauerkraut', 'seasoned salt', 'tequila', 'gruyere cheese', 'sourdough', 'green cabbage', 'curry paste', 'lasagna noodle', 'juice concentrate', 'cooked ham', 'kalamata olive', 'chocolate syrup', 'cookie', 'wheat bread', 'marmalade', 'spring onion', 'purple onion', 'pie shell', 'pork shoulder', 'round', 'shiitake', 'celery salt', 'chicken wing', 'agave nectar', 'adobo', 'prepared horseradish', 'cumin seed', 'ears', 'bisquick', 'lime wedge', 'cajun seasoning', 'iceberg lettuce', 'clam', 'peppermint', 'currant', 'tomato puree', 'green pea', 'italian bread', 'mussel', 'red chile', 'corn oil', 'orange rind', 'tomato soup', 'watermelon', 'white bean', 'star anise', 'poultry seasoning', 'maraschino cherr', 'lemon pepper', 'tapioca', 'cold milk', 'pancetta', 'yellow cornmeal', 'rhubarb', 'lemongra', 'refried bean', 'crab meat', 'chuck', 'flavoring', 'pomegranate', 'corn starch', 'orange marmalade', 'penne', 'mascarpone', 'skin', 'red cabbage', 'sprinkle', 'stew meat', 'pickle', 'frozen broccoli', 'mashed potatoe', 'cranberry juice', 'anchovy fillet', 'sage leave', 'hot chili', 'parsnip', 'enchilada sauce', 'chile powder', 'fresh ginger', 'liquid smoke', 'arborio rice', 'sweet paprika', 'fennel bulb', 'dried apricot', 'unsweetened applesauce', 'chunky salsa', 'pizza sauce', 'onion flake', 'rosemary leave', 'smoked ham', 'italian dressing', 'bok choy', 'wild rice', 'snow pea', 'saffron thread', 'cranberry sauce', 'green chile', 'cheese blend', 'roasted peanut', 'caraway seed', 'sherry vinegar', 'jalapeno chil', 'vermouth', 'soymilk', 'habanero', 'port', 'chopped cilantro', 'buns', 'agave', 'cooked turkey', 'vanilla yogurt', 'superfine sugar', 'head cauliflower', 'pumpkin seed', 'duck', 'ancho', 'lowfat yogurt', 'mandarin orange', 'grapeseed oil', 'vanilla essence', 'wheat germ', 'diced tomatoe', 'brown mustard', 'cooked bacon', 'biscuit', 'panko breadcrumb', 'mustard powder', 'apricot preserve', 'grain rice', 'tomatillo', 'salad oil', 'corn flour', 'crisco', 'lard', 'smooth', 'sundried tomato', 'with skin', 'softened butter', 'asiago', 'simple syrup', 'club soda', 'gin', 'root', 'coating', 'baking mix', 'roasted garlic', 'granulated garlic', 'amaretto', 'oreo', 'nonfat yogurt', 'vinaigrette dressing', 'pastry flour', 'fine salt', 'gluten', 'thyme sprig', 'fat', 'celery soup', 'popcorn', 'rotini', 'grapefruit', 'ground chuck', 'ground chicken', 'lobster', 'nonfat milk', 'crumble', 'floret', 'dinner roll', 'cremini', 'flank steak', 'catsup', 'flax seed', 'adobo sauce', 'dried onion', 'table salt', 'okra', 'fish fillet', 'almond milk', 'season', 'chipotle chile', 'plum', 'corn flake', 'halibut', 'fresh parsley', 'salami', 'soy milk', 'gorgonzola', 'blanched almond', 'blue', 'crouton', 'pickle relish', 'wholewheat', 'brisket', 'chile pepper', 'mini marshmallow', 'milk powder', 'glaze', 'pancake', 'brussels sprout', 'sunflower oil', 'leaf lettuce', 'cardamom pod', 'dried cherr', 'savory', 'grand marnier', 'fontina', 'smoked salmon', 'asparagus spear', 'prune', 'bacon bit', 'cheese spread', 'chuck roast', 'accompaniment', 'fresh lemon', 'dry milk', 'brownie', 'relish', 'ginger ale', 'berry', 'chorizo sausage', 'northern bean', 'raspberry jam', 'teriyaki sauce', 'bay seasoning', 'lemonade', 'frank', 'bran', 'rice noodle', 'turnip', 'cremini mushroom', 'creole seasoning', 'chili paste', 'chopped onion', 'rice cereal', 'white mushroom', 'fl', 'quick oat', 'triple sec', 'dashi', 'polenta', 'ghee', 'chorizo', 'coleslaw', 'with juice', 'bouillon granule', 'artichoke', 'corned beef', 'balsamic vinaigrette', 'ripe olive', 'marinade', 'pesto sauce', 'chard', 'baby arugula', 'asiago cheese', 'phyllo', 'veal', 'napa cabbage', 'of lamb', 'crystallized ginger', 'ground lamb', 'yellow pepper', 'color', 'butterscotch chip', 'lemon extract', 'bitter', 'english cucumber', 'tarragon leave', 'cognac', 'cantaloupe', 'swiss chard', 'almond flour', 'chilli', 'pizza crust', 'katakuriko', 'cauliflower floret', 'gravy', 'radish', 'chili flake', 'guacamole', 'sweet pickle', 'radicchio', 'figs', 'kielbasa', 'crab', 'dried sage', 'mascarpone cheese', 'pita bread', 'instant espresso', 'mace', 'beef tenderloin', 'sirloin steak', 'fresh basil', 'muffin', 'angel hair', 'vegetarian', 'peeled tomatoe', 'peppermint extract', 'protein', 'onion salt', 'wonton wrapper', 'dripping', 'brie cheese', 'sourdough bread', 'marsala wine', 'steak seasoning', 'for dusting', 'crescent roll', 'chicken drumstick', 'orzo pasta', 'miso', 'chicken leg', 'mangoe', 'smoked turkey', 'dark soy', 'strawberry jam', 'unbleached flour', 'cracked pepper', 'baking chocolate', 'rotini pasta', 'fontina cheese', 'cooked shrimp', 'hominy', 'pimento', 'sweetener', 'curry leave', 'frozen strawberr', 'bulgur', 'fudge', 'blueberry', 'sea scallop', 'xanthan', 'xanthan gum', 'tamarind', 'serrano chile', 'neufchatel cheese', 'dried fruit', 'goat', 'prawn', 'lemon wedge', 'pearl onion', 'daikon', 'granola', 'fresh', 'bacon slice', 'instant yeast', 'corn chip', 'vermicelli', 'tomato ketchup', 'yoghurt', 'sweet corn', 'clarified butter', 'rotisserie chicken', 'potato chip', 'light mayonnaise', 'anchovy', 'schnapp', 'rigatoni', 'semolina', 'bouquet', 'de provence', 'macadamia nut', 'ciabatta', 'cumin powder', 'mushroom cap', 'parmigiano', 'garlic paste', 'coffee granule', 'lima bean', 'beef brisket', 'spanish onion', 'smoked sausage', 'beef stew', 'mayer bacon', 'hash brown', 'seafood', 'clam juice', 'tortellini', 'english muffin', 'salted peanut', 'sausage link', 'raising flour', 'chutney', 'apricot jam', 'basil leaf', 'jasmine', 'coconut flake', 'turbinado', 'ritz cracker', 'mint sprig', 'mexican cheese', 'tart apple', 'hot dog', 'puree', 'rotel tomatoe', 'dill pickle', 'venison', 'food cake', 'pepper jack', 'chiffonade', 'steak sauce', 'cinnamon ground', 'whole', 'flax', 'picante sauce', 'chocolate candy', 'pearl barley', 'ground flax', 'silken tofu', 'fenugreek', 'double cream', 'coriander powder', 'vidalia onion', 'papaya', 'capsicum', 'baker', 'wasabi', 'cilantro sprig', 'boiled egg', 'instant pudding', 'roast beef', 'anchov', 'crusty bread', 'pink', 'trout', 'port wine', 'espresso powder', 'tea bag', 'champagne vinegar', 'sliced carrot', 'cointreau', 'pork roast', 'turkey sausage', 'deli ham', 'powdered milk', 'turmeric powder', 'potato starch', 'frozen raspberr', 'grenadine', 'meatball', 'pimiento', 'vanilla wafer', 'seaweed', 'splenda granular', 'chili bean', 'toast', 'smoked bacon', 'grated pecorino', 'carbonated', 'salt pork', 'protein powder', 'tilapia fillet', 'jicama', 'cilantro fresh', 'orange liqueur', 'endive', 'edamame', 'pepper cheese', 'pancake mix', 'fish stock', 'red snapper', 'rye flour', 'vegan', 'jasmine rice', 'cashew nut', 'kahlua', 'white corn', 'golden deliciou', 'scotch', 'rising flour', 'frozen cranberr', 'pudding mix', 'honey mustard', 'soy', 'artificial', 'parsley sprig', 'fresh lime', 'poblano chile', 'walnut halve', 'sweetened coconut', 'tea', 'chervil', 'cheese slice', 'whole milk', 'honeydew', 'brewed coffee', 'acorn squash', 'gluten free', 'pork butt', 'swi', 'grit', 'sea ba', 'jam', 'liver', 'ramen noodle', 'taco sauce', 'raw sugar', 'jumbo shrimp', 'porcini mushroom', 'sliced onion', 'pomegranate juice', 'ground almond', 'pasta shell', 'pretzel', 'alfredo sauce', 'gouda cheese', 'cumin ground', 'cheese soup', 'lime leave', 'partially', 'allspice berr', 'lemon gra', 'wild', 'blackberry', 'squid', 'pork rib', 'ground mustard', 'yellow corn', 'spelt', 'marshmallow creme', 'panko', 'toasted pecan', 'bonito', 'clean', 'five spice', 'kaffir lime', 'mixed nut', 'brioche', 'herb seasoning', 'red bean', 'no', 'navy bean', 'caramel sauce', 'safflower', 'pepper ground', 'portabella mushroom', 'ground sirloin', 'lemonade concentrate', 'pomegranate seed', 'turbinado sugar', 'chocolate morsel', 'creamer', 'frozen blueberr', 'collard green', 'tart', 'fat mayonnaise', 'almond paste', 'beef sirloin', 'mixed green', 'light rum', 'meal', 'red lentil', 'toasted walnut', 'ingredient', 'white rum', 'instant rice', 'baked bean', 'dressing mix', 'jack', 'nutritional yeast', 'golden syrup', 'beef roast', 'split pea', 'oil spray', 'celery leave', 'bouquet garni', 'sorbet', 'queso fresco', 'stout', 'oat bran', 'phyllo dough', 'artificial sweetener', 'lavender', 'green tea', 'summer squash', 'rosemary sprig', 'hummu', 'salsa verde', 'rye bread', 'pie dough', 'andouille sausage', 'bamboo shoot', 'pistachio nut', 'sausage casing', 'pectin', 'ground sage', 'mixed spice', 'spelt flour', 'chia seed', 'coconut cream', 'nectarine', 'pork belly', 'shell pasta', 'lemon slice', 'ground thyme', 'chili oil', 'flounder', 'fusilli', 'russet', 'stewing beef', 'batter', 'mayonaise', 'buckwheat', 'kiwi', 'casing', 'boiling potatoe', 'wholewheat flour', 'key lime', 'bread slice', 'bbq sauce', 'chipotle pepper', 'pecorino cheese', 'canned chicken', 'boston lettuce', 'dry vermouth', 'ground veal', 'pepperoncini', 'dried tomatoe', 'cane sugar', 'tangerine', 'vegetable juice', 'cider', 'muffin mix', 'orecchiette', 'veal stock', 'shank', 'tumeric', 'savoy cabbage', 'palm sugar', 'ginger paste', 'popped popcorn', 'bonito flake', 'orange extract', 'lamb chop', 'dark beer', 'loave', 'vanilla flavor', 'dried chili', 'mexican oregano', 'truffle oil', 'rubbed sage', 'sultana', 'halibut fillet', 'and bean', 'safflower oil', 'ground round', 'frond', 'hungarian paprika', 'turkey bacon', 'grapefruit juice', 'fresh cilantro', 'muenster', 'baby green', 'minute rice', 'cheese sauce', 'fettuccine pasta', 'much', 'chili seasoning', 'red capsicum', 'harissa', 'loin chop', 'ground meat', 'masa harina', 'imported', 'mixed mushroom', 'gingersnap', 'cod', 'celery root', 'back rib', 'green tomatoe', 'green apple', 'pickled jalapeno', 'orzo', 'green cardamom', 'shaoxing', 'bean paste', 'pumpernickel', 'honeydew melon', 'butter flavor', 'smoked gouda', 'sparerib', 'stevia', 'lean beef', 'processed cheese', 'catfish fillet', 'sandwich bread', 'toasted almond', 'gravy mix', 'round steak', 'rum extract', 'butter chip', 'cornbread mix', 'serrano pepper', 'french baguette', 'sweet rice', 'sweet pepper', 'seedless cucumber', 'orange blossom', 'top sirloin', 'rice milk', 'sweet cherr', 'bicarbonate', 'amaretto liqueur', 'buttermilk biscuit', 'escarole', 'plain breadcrumb', 'grape juice', 'porcini', 'poblano pepper', 'roasting chicken', 'egg beater', 'frisee', 'beaten egg', 'soba noodle', 'chicken meat', 'cotija', 'raspberry preserve', 'tarragon vinegar', 'nutella', 'dried currant', 'chinese cabbage', 'taco shell', 'canadian bacon', 'sparkling wine', 'fresh mint', 'angostura bitter', 'of soda', 'sweet chocolate', 'wafer', 'chocolate shaving', 'butter lettuce', 'mashed banana', 'italian cheese', 'madeira', 'pea pod', 'nam pla', 'base', 'pitted prune', 'bread dough', 'seedless watermelon', 'chipotle powder', 'ziti', 'pepita', 'navel orange', 'mung bean', 'agave syrup', 'shiso leave', 'roquefort', 'beef round', 'island dressing', 'wild mushroom', 'bibb lettuce', 'thai basil', 'seasoned breadcrumb', 'wafer cook', 'deveined shrimp', 'corn muffin', 'walnut oil', 'bacon dripping', 'cooking apple', 'soybean', 'sliced ham', 'kaiser roll', 'almond meal', 'eggnog', 'marshmallow cream', 'mixed fruit', 'seltzer', 'wholegrain mustard', 'candy sprinkle', 'desiccated coconut', 'cooked pasta', 'tamarind paste', 'buffalo', 'pickle juice', 'bass fillet', 'link', 'cherry juice', 'plantain', 'sprite', 'oat flour', 'anchovy paste', 'dark molasse', 'sour milk', 'mein noodle', 'chile paste', 'bass', 'blood orange', 'cheese crumble', 'cilantro stem', 'bone in', 'dried herb', 'biscuit mix', 'tuna steak', 'corn meal', 'loin', 'english mustard', 'vanilla sugar', 'chanterelle', 'hand', 'flower', 'cinnamon sugar', 'mango chutney', 'mesclun', 'seed meal', 'currant jelly', 'spaghetti squash', 'butter bean', 'flavour', 'grated carrot', 'cheese tortellini', 'custard', 'idaho potatoe', 'japanese eggplant', 'gnocchi', 'lemon curd', 'of coconut', 'pound cake', 'vegetable bouillon', 'semolina flour', 'lime peel', 'haddock', 'blood', 'nori seaweed', 'onion slice', 'umeboshi', 'sweet basil', 'fenugreek seed', 'oleo', 'colby cheese', 'caviar', 'muenster cheese', 'de gallo', 'pulp', 'shaoxing wine', 'chocolate chunk', 'chopped tomatoe', 'tapioca flour', 'filet', 'almond butter', 'coconut extract', 'sandwich cook', 'mustard green', 'seasoning blend', 'real bacon', 'cream topping', 'tater tot', 'de cacao', 'haricots vert', 'cheese frosting', 'mentsuyu', 'cheese dressing', 'galangal', 'wheat bran', 'catalina dressing', 'hoagie roll', 'sponge', 'indian', 'burgundy', 'pasteurized', 'tenderloin steak', 'toffee bit', 'oyster mushroom', 'anise seed', 'chardonnay', 'cooking wine', 'snapper fillet', 'iceberg', 'red potato', 'crushed tomatoe', 'quail', 'candy cane', 'wheat gluten', 'passion fruit', 'crema', 'delicious apple', 'bacon fat', 'rose water', 'fava bean', 'matcha', 'coffee creamer', 'country bread', 'soya', 'roquefort cheese', 'lamb shoulder', 'seed oil', 'uncle ben', 'fried onion', 'apple butter', 'sauce mix', 'sardine', 'swordfish', 'spiced rum', 'reggiano cheese', 'matzo meal', 'udon', 'chicken base', 'corn husk', 'coffee liqueur', 'grated horseradish', 'ruby port', 'spanish paprika', 'rutabaga', 'winter squash', 'cod fillet', 'sweet butter', 'lamb shank', 'ground walnut', 'plastic', 'machine yeast', 'pickling salt', 'masa', 'cointreau liqueur', 'silver', 'citru', 'havarti', 'dried lentil', 'beef consomme', 'coconut flour', 'crumbled gorgonzola', 'bosc pear', 'lime slice', 'chickpea flour', 'fresh thyme', 'rump roast', 'rice blend', 'lobster meat', 'chicken flavor', 'corn bread', 'seafood seasoning', 'cooking sherry', 'chicken part', 'demerara sugar', 'sherbet', 'salmon steak', 'apple jelly', 'crawfish', 'littleneck clam', 'tapioca starch', 'soda water', 'maple extract', 'juniper berr', 'ponzu', 'twist', 'canned corn', 'mackerel', 'candied ginger', 'melon', 'neutral oil', 'irish whiskey', 'white miso', 'dried chive', 'de sel', 'pastry dough', 'sesame paste', 'poultry', 'bechamel', 'chambord', 'whole wheat', 'refrigerated biscuit', 'coffee powder', 'cornflour', 'toasted coconut', 'fruit cocktail', 'creole mustard', 'leg quarter', 'onion top', 'belgian endive', 'lager', 'dream whip', 'tilapia', 'shelled pistachio', 'licorice', 'kirsch', 'nonhydrogenated margarine', 'fingerling potatoe', 'fingerling', 'chicken cutlet', 'riesling', 'caramel topping', 'apricot nectar', 'lime rind', 'light butter', 'ravioli', 'sandwich bun', 'bay scallop', 'sucanat', 'green chili', 'black tea', 'crisp rice', 'grape jelly', 'red chilli', 'palm', 'grilled chicken', 'white kidney', 'chocolate frosting', 'candy bar', 'peach schnapp', 'shortbread', 'orange segment', 'chicken liver', 'ale', 'garlic sauce', 'earth balance', 'sourdough starter', 'peach slice', 'roasted tomatoe', 'double crust', 'lemon pudding', 'sliced apple', 'adobo seasoning', 'calvado', 'liquid honey', 'bulb fennel', 'nori', 'sliced tomatoe', 'limoncello', 'pork sparerib', 'leaving', 'calamari', 'dal', 'brewed espresso', 'ground fennel', 'brown gravy', 'dorito', 'cookie crumb', 'turkey broth', 'grana padano', 'bacon grease', 'raspberry vinegar', 'bouillon powder', 'chopped celery', 'amaretti', 'thai chile', 'white tuna', 'meringue', 'rib roast', 'turkish', 'sirloin tip', 'unsalted cashew', 'of palm', 'onion ring', 'pinenut', 'chill', 'bourbon whiskey', 'yams', 'sushi rice', 'pot roast', 'wholemeal', 'game', 'heirloom tomatoe', 'mozzarella ball', 'chocolate milk', 'asafetida', 'unsalted pistachio', 'pepper seasoning', 'candied cherr', 'pepperoni slice', 'ricotta salata', 'pimenton', 'white peppercorn', 'phyllo pastry', 'browning', 'ham hock', 'cooked chickpea', 'dried yeast', 'seedless raisin', 'soft margarine', 'wasabi powder', 'wax', 'tangerine juice', 'orange slice', 'boneless lamb', 'cooked quinoa', 'spanish chorizo', 'carnation', 'sweet relish', 'grated coconut', 'sandwiche', 'fresh rosemary', 'food colouring', 'pepper powder', 'potato soup', 'gingersnap cook', 'dried fig', 'gala apple', 'cabernet', 'pinot grigio', 'seasoned crouton', 'paneer', 'rabbit', 'gherkin', 'beef gravy', 'fruit pectin', 'frozen meatball', 'chicken sausage', 'pitted cherr', 'vegg', 'coconut rum', 'grill seasoning', 'citron', 'cassi', 'frying oil', 'roasted cashew', 'dried beef', 'strawberry gelatin', 'thin spaghetti', 'vanilla frosting', 'habanero pepper', 'sweet vermouth', 'merlot', 'vegetable soup', 'baby corn', 'strawberry preserve', 'pineapple slice', 'medium egg', 'kiwi fruit', 'karo syrup', 'pastry sheet', 'doubanjiang', 'pumpernickel bread', 'crumb crust', 'msg', 'broccoli rabe', 'rye', 'fruit juice', 'raw honey', 'wasabi paste', 'honey sugar', 'distilled vinegar', 'cardamom seed', 'italian tomatoe', 'slab bacon', 'liquor', 'madeira wine', 'flaxseed', 'bulb', 'potato flake', 'burger', 'pita chip', 'wheat cereal', 'frozen banana', 'sliced shallot', 'urad dal', 'havarti cheese', 'white truffle', 'long-grain rice', 'country ham', 'dried coconut', 'stilton', 'turkey stock', 'cod fish', 'frankfurter', 'french dressing', 'chile sauce', 'apricot halve', 'grape leave', 'waffle', 'clover', 'vegan margarine', 'sandwich roll', 'brine', 'tomato salsa', 'clear honey', 'rice mix', 'chocolate sprinkle', 'soy flour', 'plain chocolate', 'duck breast', 'sanding sugar', 'dry pasta', 'sour cherr', 'marjoram leave', 'arrowroot', 'free milk', 'dried lavender', 'chicken gravy', 'goose', 'gouda', 'hothouse cucumber', 'peach preserve', 'japanese rice', 'crust pie', 'morsel', 'lobster tail', 'spice mix', 'light molasse', 'liquid amino', 'penne rigate', 'chopped spinach', 'beansprout', 'fig', 'grain mustard', 'shoulder roast', 'smoked pork', 'strip steak', 'pernod', 'linguini', 'fresh oregano', 'burgundy wine', 'smoked mozzarella', 'combined', 'flounder fillet', 'breast fillet', 'eye steak', 'dark raisin', 'guajillo', 'cherry pepper', 'ground sausage', 'floweret', 'sorghum', 'shimeji mushroom', 'chinese chive', 'malt vinegar', 'roll wrapper', 'frozen vegetable', 'ditalini', 'streaky bacon', 'pink grapefruit', 'buckwheat flour', 'kabocha squash', 'shucked oyster', 'cabernet sauvignon', 'coffee bean', 'genoa salami', 'kumquat', 'mahi mahi', 'in adobo', 'hass avocado', 'boursin', 'soy yogurt', 'frozen okra', 'cocoa butter', 'pasta water', 'raw almond', 'active yeast', 'skinless thigh', 'skirt steak', 'italian herb', 'ground coffee', 'canned tuna', 'roux', 'shortbread cook', 'banana liqueur', 'mixed berr', 'shiso', 'raisin bread', 'powdered gelatin', 'frozen mango', 'cracked wheat', 'concentrated', 'pasilla', 'raspberry liqueur', 'cubed potatoe', 'arrow root', 'lemon flavor', 'chai', 'house seasoning', 'cinnamon powder', 'lettuce heart', 'large shrimp', 'cookie dough', 'cooked potatoe', 'melon liqueur', 'boiled ham', 'wheat berr', 'striped ba', 'boston butt', 'shoyu', 'asparagus tip', 'tea leave', 'red grape', 'dried blueberr', 'beef rump', 'yeast packet', 'nonstick spray', 'chocolate bar', 'shelled edamame', 'wondra', 'fresh coriander', 'french roll', 'meatloaf', 'whey protein', 'brazil', 'de leche', 'blossom honey', 'dark meat', 'nut', 'ginger powder', 'polish sausage', 'chocolate sauce', 'pork fat', 'sauvignon blanc', 'cut bacon', 'chicken strip', 'quickcooking grit', 'beef shank', 'manchego cheese', 'ciabatta roll', 'fiber one', 'sour sauce', 'suet', 'slaw', 'popcorn kernel', 'ground oregano', 'flavored syrup', 'serrano chil', 'jack daniel', 'beef liver', 'pink peppercorn', 'galliano', 'cheese ravioli', 'lamb loin', 'biscuit dough', 'mutton', 'petite pea', 'hershey chocolate', 'bean sauce', 'prosecco', 'meyer lemon', 'green peppercorn', 'mexicorn', 'love', 'crab boil', 'game hen', 'red apple', 'seasoned stuffing', 'wing sauce', 'sugar syrup', 'campari', 'rice paper', 'rub', 'frangelico', 'beefsteak tomatoe', 'bartlett pear', 'liquid pectin', 'bocconcini', 'london broil', 'light syrup', 'beef steak', 'cubed ham', 'green grape', 'cream liqueur', 'beetroot', 'dairy', 'raw cashew', 'burdock', 'kimchi', 'turkey gravy', 'roasted chestnut', 'cocktail sauce', 'passata', 'cola', 'treacle', 'aioli', 'mission fig', 'frito', 'pita', 'manicotti', 'flower water', 'dried shrimp', 'pappardelle', 'wiener', 'chopped garlic', 'shrimp paste', 'roasted pistachio', 'root beer', 'curd', 'butter oil', 'de arbol', 'baking apple', 'strawberry flavor', 'flavored vodka', 'millet', 'grana', 'chocolate liqueur', 'mint extract', 'catfish', 'dehydrated', 'confit', 'giardiniera', 'cooked beef', 'pack pumpkin', 'peach nectar', 'bulgur wheat', 'apricot brandy', 'chocolate curl', 'malted milk', 'cut oat', 'carrot stick', 'brown lentil', 'vanilla vodka', 'wax bean', 'crepe', '& half', 'diced onion', 'sponge cake', 'candied fruit', 'turkey meat', 'monkfish', 'greek seasoning', 'crisco shortening', 'green lentil', 'vanilla flavoring', 'chilli powder', 'scallion green', 'hot sausage', 'gyoza', 'sweet pea', 'mincemeat', 'fennel frond', 'kale leave', 'piquillo pepper', 'banana pepper', 'dry rub', 'chicken fat', 'brazil nut', 'chianti', 'burger bun', 'ketjap', 'lavender flower', 'crimini mushroom', 'absinthe', 'chicory', 'sausage meat', 'cavatappi', 'sweet cream', 'brownie mix', 'sliced cucumber', 'miso paste', 'peeled shrimp', 'coarse sugar', 'sole', 'filet mignon', 'hazelnut oil', 'cooked macaroni', 'clementine', 'ground pecan', 'gray salt', 'tempeh', 'sorghum flour', 'teriyaki marinade', 'coconut sugar', 'chopped parsley', 'white cabbage', 'easy', 'tart cherr', 'mousse', 'pomegranate molasse', 'pea shoot', 'marcona almond', 'horseradish sauce', 'lemon thyme', 'soft tofu', 'spice blend', 'puffed rice', 'other herb', 'pear tomatoe', 'lager beer', 'cooking liquid', 'slaw mix', 'tartar sauce', 'white hominy', 'pinot noir', 'filtered water', 'unsalted margarine', 'cornichon', 'peasant bread', 'cornish hen', 'saltine', 'all natural', 'bucatini', 'baby potatoe', 'cold cut', 'fresh dill', 'marzipan', 'steamed rice', 'unrefined', 'fenugreek leave', 'shortcrust pastry', 'bing cherr', 'tentacle', 'avocado oil', 'and flaked', 'dried tomato', 'alfalfa sprout', 'flatbread', 'fiber', 'spam', 'smoke', 'grenadine syrup', 'stilton cheese', 'cupcake', 'lamb stock', 'salt added', 'for color', 'achiote', 'albacore', 'glutinous rice', 'bagel', 'tahini paste', 'rising yeast', 'sweet soy', 'monosodium glutamate', 'monosodium', 'rice vermicelli', 'ginger beer', 'cheese dip', 'dandelion', 'orange bitter', 'red beet', 'glace cherr', 'granulated onion', 'sumac', 'seltzer water', 'togarashi', 'ground paprika', 'boysenberr', 'pastrami', 'meat tenderizer', 'oxtail', 'tea powder', 'habanero chile', 'breast tender', 'jerk seasoning', 'preserved lemon', 'southern comfort', 'milkfat', 'pizza dough', 'asafetida powder', 'flavor shortening', 'dried rice', 'camembert', 'almond liqueur', 'seasoned flour', 'tap water', 'and carrot', 'waxy potatoe', 'tuna fish', 'asafoetida', 'ganache', 'black mushroom', 'truffle', 'chai tea', 'minced meat', 'citric acid', 'citric', 'sugar blend', 'minced beef', 'cooked pumpkin', 'ragu', 'limeade concentrate', 'frozen limeade', 'cream butter', 'broccoli stem', 'turnip green', 'straw mushroom', 'tagliatelle', 'aubergine', 'rock salt', 'potato flour', 'reposado', 'hot mustard', 'banana leave', 'trans fat', 'minced clam', 'chocolate cookie', 'other nut', 'plum sauce', 'naan', 'garlic chive', 'bottom round', 'red miso', 'sweet sherry', 'style corn', 'mexican beer', 'dumpling', 'lardon', 'clover honey', 'sliced chicken', 'veal cutlet', 'chicken tenderloin', 'prime rib', 'barilla', 'hemp seed', 'frozen peache', 'veal shank', 'grouper', 'dried garlic', 'espresso bean', 'hickory smoke', 'alcohol', 'spaghettini', 'pink salmon', 'asadero', 'lean bacon', 'harissa paste', 'wafer crumb', 'crusty roll', 'red grapefruit', 'white cheese', 'albacore tuna', 'sliced leek', 'baked ham', 'bacardi', 'old bread', 'accent seasoning', 'chinese noodle', 'focaccia', 'risotto', 'mizuna', 'sticky rice', 'sliced olive', 'file powder', 'cane syrup', 'multigrain bread', 'breast strip', 'yeast flake', 'worchestershire sauce', 'dried egg', 'ranch seasoning', 'white cornmeal', 'mexican chorizo', 'mild salsa', 'frozen yogurt', 'dried bean', 'cubed beef', 'pork cutlet', 'ginger juice', 'sofrito', 'bird chile', 'rye whiskey', 'cream sherry', 'nigella seed', 'cherry preserve', 'broiler', 'medium heat', 'mediterranean', 'queso blanco', 'asafoetida powder', 'original essence', 'tawny port', 'egg product', 'peperoncino', 'cocoa mix', 'cone', 'bread mix', 'sazon goya', 'blueberry jam', 'bulgar wheat', 'dried chile', 'shredded carrot', 'roasted almond', 'fondant', 'dried chickpea', 'peach jam', 'kitchen bouquet', 'pickling spice', 'ramen', 'cabbage leave', 'dill seed', 'spring water', 'string bean', 'juice cocktail', 'calor', 'dried date', 'chick pea', 'crumb fresh', 'five-spice powder', 'fresh chive', 'caramel syrup', 'pastry shell', 'flax meal', 'european cucumber', 'salted cashew', 'greek feta', 'stock cube', 'cocoa solid', 'baby beet', 'tapenade', 'shrimp stock', 'lemon verbena', 'up chicken', 'curly kale', 'swordfish steak', 'croissant', 'sun-dried tomatoe', 'hard salami', 'fatback', 'banana extract', 'unsulfured molasse', 'food color', 'amchur', 'chaat masala', 'egg bread', 'red radishe', 'rump steak', 'peach halve', 'tarragon sprig', 'table cream', 'tamarind pulp', 'not available', 'compressed yeast', 'shredded cheese', 'roe', 'pan dripping', 'guar gum', 'tamarind juice', 'bleached', 'pizza seasoning', 'wheat cracker', 'black rice', 'garlic puree', 'borlotti', 'shredded lettuce', 'celery flake', 'baby artichoke', 'ranch dip', 'fuji apple', 'haddock fillet', 'bread roll', 'stew', 'pearl tapioca', 'oils', 'tangerine zest', 'zinfandel', 'dried mushroom', 'butter pickle', 'anjou pear', 'mineral water', 'cockle', 'yellow rice', 'duck fat', 'japanese breadcrumb', 'ham steak', 'cereal flake', 'rapeseed oil', 'nuoc mam', 'crumbled cornbread', 'fresh tomatoe', 'chicken skin', 'moong dal', 'summer savory', 'minced pork', 'hen', 'pie pastry', 'light beer', 'pineapple ring', 'french fr', 'tostada shell', 'carbonated water', 'turkey kielbasa', 'alfalfa', 'spring green', 'cake yeast', 'celery heart', 'chipotle salsa', 'whisky', 'sambuca', 'pear nectar', 'spearmint', 'grey tea', 'capellini', 'rice syrup', 'wild salmon', 'part-skim mozzarella', 'mature cheddar', 'light margarine', 'butterfinger', 'fish bone', 'clove ground', 'frozen cherr', 'wonton skin', 'golden beet', 'broad bean', 'crab stick', 'portobello cap', 'breadstick', 'vegan butter', 'tart shell', 'fettucine', 'armagnac', 'carca', 'cracker crust', 'muscat', 'spanish olive', 'coke', 'rock shrimp', 'tomatillo salsa', 'mustard dressing', 'dashi powder', 'marrow', 'portabello mushroom', 'dried pineapple', 'dehydrated onion', 'cubed bread', 'gingersnap crumb', 'amaranth', 'sicilian', 'dried apple', 'blue crab', 'chabli', 'campanelle', 'peach puree', 'dried fettuccine', 'mcintosh apple', 'lamb neck', 'tomato vinaigrette', 'herb dressing', 'short rib', 'red raspberr', 'organic chicken', 'short pasta', 'pitted olive', 'el hanout', 'smoked trout', 'lavash', 'aniseed', 'unsulphured molasse', 'red salmon', 'margarita mix', 'string cheese', 'mild sausage', 'beef fat', 'guar', 'bologna', 'sweetcorn', 'lavender blossom', 'white tequila', 'parmesan romano', 'cocktail peanut', 'soybean paste', 'yellow miso', 'manzanilla', 'round loaf', 'nut butter', 'ajwain', 'dried celery', 'black vinegar', 'chopped walnut', 'ciabatta loaf', 'mexican chocolate', 'konbu', 'flour mix', 'rich', 'chocolate powder', 'sundae syrup', 'rock sugar', 'peanut sauce', 'toast point', 'power', 'ahi', 'runny honey', 'seafood stock', 'boneless sirloin', 'silver tequila', 'chicken piece', 'bean dip', 'dessert wine', 'quail egg', 'pineapple preserve', 'laughing cow', 'anaheim chile', 'garlic granule', 'radicchio leave', 'sliced pineapple', 'unsalted almond', 'arbol chile', 'aleppo pepper', 'veal shoulder', 'lasagna sheet', 'cherry gelatin', 'gelatine', 'blackberry jam', 'malt', 'beef base', 'cactu', 'dipping chocolate', 'style bean', 'forest ham', 'sliced turkey', 'guava', 'sunflower kernel', 'aged cheddar', 'tostito', 'rib chop', 'blanched hazelnut', 'candied pineapple', 'epazote', 'ground flaxseed', 'kirby cucumber', 'barley malt', 'cheese cube', 'sweetened cranberr', 'annatto', 'middle eastern', 'veal chop', 'candied peel', 'gyoza skin', 'plum jam', 'ground hazelnut', 'mustard oil', 'bran flake', 'white asparagu', 'dandelion green', 'sugar pumpkin', 'distilled water', 'mortadella', 'canola mayonnaise', 'soft cheese', 'ouzo', 'kefir', 'rice bran', 'sliced salami', 'cipollini', 'rice pilaf', 'strawberry yogurt', 'spicy sausage', 'salted pistachio', 'mochiko', 'fudge topping', 'dijon', 'white vermouth', 'low-fat milk', 'corn cereal', 'yellowtail', 'whitefish', 'parma ham', 'canned salmon', 'lady apple', 'natural yogurt', 'muscovado sugar', 'gluten flour', 'organic sugar', 'root vegetable', 'raspberry puree', 'chopped pecan', 'bean flour', 'buttercream frosting', 'nama shoyu', 'guanciale', 'buffalo mozzarella', 'cachaca', 'jarlsberg', 'carob', 'fresh mozzarella', 'yuzu juice', 'krispies cereal', 'fresh tarragon', 'gooseberr', 'puy lentil', 'guajillo chile', 'dry fettuccine', 'citrus juice', 'bonnet chile', 'bolillo', 'shoepeg corn', 'liverwurst', 'butter salt', 'iron', 'light tuna', 'peach yogurt', 'bow-tie pasta', 'asian pear', 'frozen potatoe', 'steamer', 'fresh sage', 'lacinato kale', 'flat anchovy', 'gumbo', 'unsweetened soymilk', 'shaved chocolate', 'curry leaf', 'pork steak', 'meringue powder', 'bresaola', 'pig', 'pureed pumpkin', 'cranberry bean', 'crust mix', 'methi', 'tonic water', 'chinese sausage', 'italian vinaigrette', 'marzano tomatoe', 'barbecue rub', 'buckwheat groat', 'french mustard', 'swede', 'carnaroli rice', 'rotelle', 'calamata olive', 'sliced beet', 'quinoa flour', 'jaggery', 'cacao powder', 'sazon seasoning', 'arrowroot powder', 'sugar pea', 'accent', 'lumpia wrapper', 'beet sugar', 'char', 'green garlic', 'paprika powder', 'risotto rice', 'tostada', 'cherry brandy', 'vegetarian chicken', 'maggi', 'kamaboko', 'minced chicken', 'hazelnut liqueur', 'char siu', 'shredded cabbage', 'carob powder', 'szechwan peppercorn', 'anisette', 'squirt', 'lychee', 'lady finger', 'round roast', 'grated cotija', 'mahi fillet', 'muesli', 'soy protein', 'himalayan salt', 'burrata', 'raspberry sauce', 'pasilla chile', 'garlic bulb', 'graham flour', 'instant oat', 'shrimp shell', 'farro', 'ripened tomatoe', 'fresh orange', 'pearled barley', 'curly endive', 'aquavit', 'king prawn', 'sugar cube', 'stuffed olive', 'salt flake', 'mochi', 'coconut water', 'duck sauce', 'nacho chip', 'seasoned chicken', 'yuzu', 'somen', 'cassava', 's', 'long bean', 'orange roughy', 'kitchen', 'mushroom broth', 'coleslaw dressing', 'gram flour', 'stick noodle', 'pickling liquid', 'broccolini', 'sour mix', 'fish steak', 'chocolate drink', 'shoulder butt', 'chilegarlic sauce', 'taleggio', 'vegan mayonnaise', 'slider bun', 'dried corn', 'turkey leg', 'the raw', 'tasso', 'whiting', 'fresh mushroom', 'sangiovese', 'luncheon meat', 'persimmon', 'vanilla pod', 'israeli couscou', 'bran oil', 'amber', 'anise extract', 'free flour', 'kohlrabi', 'ton wrapper', 'dessert topping', 'banana pudding', 'cinnamon roll', 'bean seed', 'tortilla shell', 'st', 'ground wheat', 'm', 'hard cider', 'tamarind concentrate', 'lasagne', 'maple sugar', 'veal loin', 'mango puree', 'marmite', 'filbert', 'black ba', 'chestnut mushroom', 'apple brandy', 'puffed wheat', 'dried pear', 'ditalini pasta', 'filo', 'frozen blackberr', 'smoke flavoring', 'long green', 'free yogurt', 'roasted pepper', 'brown cardamom', 'laurel', 'iodized salt', 'almond filling', 'ground nut', 'beef shoulder', 'claw', 'mango powder', 'turkey carca', 'sauce tomato', 'porterhouse steak', 'wondra flour', 'rose petal', 'chayote', 'porridge oat', 'brioche bread', 'condiment', 'raspberry juice', 'salt water', 'lillet', 'dean sausage', 'beet green', 'shellfish', 'shiraz', 'yogurt cheese', 'with garlic', 'cheese dinner', 'corn grit', 'cookie mix', 'salad leave', 'pappardelle pasta', 'tortilla wrap', 'regular sugar', 'canela', 'chrysanthemum', 'lemon balm', 'baking spray', 'sub roll', 'pork stock', 'cutlet', 'tempura batter', 'potato gnocchi', 'thick-cut bacon', 'pink bean', 'garlic bread', 'nondairy creamer', 'italian roll', 'pear juice', 'crostini', 'custard powder', 'bread round', 'toasted baguette', 'budweiser', 'roasted hazelnut', 'stock powder', 'dessicated coconut', 'fructose', 'organic apple', 'ginger syrup', 'cooked meatball', 'beef sausage', 'mulberr', 'linguica', 'poblano chil', 'watercress leave', 'emerils essence', 'flakes cereal']\n"
     ]
    }
   ],
   "source": [
    "#Since the most of the ingredients above have the last two words as the most important, we reduce all ingredients to their last two words for simplicity\n",
    "unique_ingredient_list_copy = unique_ingredient_list\n",
    "for i in range(len(unique_ingredient_list_copy)):\n",
    "    \n",
    "    newword = unique_ingredient_list_copy[i].split()[-2:]\n",
    "    if len(newword) == 2:\n",
    "        newword = (newword[0] + ' ' + newword[1])\n",
    "    else:\n",
    "        newword = newword[0]\n",
    "        \n",
    "#Remove last letter if it is an 's', this helps to standardize plural ingredients\n",
    "    if (len(newword) > 6 and newword[-3:] == 'ies'):\n",
    "        newword = newword.rstrip(newword[-3:])\n",
    "    elif (len(newword) > 5 and newword[-2:] == 'es'):\n",
    "        newword = newword.rstrip(newword[-1:])\n",
    "\n",
    "    elif (len(newword) > 4 and newword[-1] == 's'):\n",
    "        newword = newword.rstrip(newword[-1])\n",
    "    \n",
    "    unique_ingredient_list_copy[i] = newword\n",
    "    \n",
    "unique_ingredient_list = []\n",
    "[unique_ingredient_list.append(x) for x in unique_ingredient_list_copy if x not in unique_ingredient_list]\n",
    "print (len(unique_ingredient_list))\n",
    "#Remove last ~40 things on list (since they are nonsense)\n",
    "\n",
    "# unique_ingredient_list = sorted(unique_ingredient_list, key=len, reverse=True)\n",
    "# unique_ingredient_list = unique_ingredient_list[0:len(unique_ingredient_list) - 35]\n",
    "print (len(unique_ingredient_list))\n",
    "print (unique_ingredient_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(unique_ingredient_list_copy)):\n",
    "    \n",
    "    newword = unique_ingredient_list_copy[i].split()[-2:]\n",
    "    if len(newword) == 2:\n",
    "        newword = (newword[0] + ' ' + newword[1])\n",
    "    else:\n",
    "        newword = newword[0]\n",
    "\n",
    "#manually remove items that don't make sense\n",
    "unique_ingredient_list.remove(\"water\")\n",
    "unique_ingredient_list.remove(\"ground\")\n",
    "unique_ingredient_list.remove(\"powder\")\n",
    "unique_ingredient_list.remove(\"sliced\")\n",
    "unique_ingredient_list.remove(\"sauce\")\n",
    "unique_ingredient_list.remove(\"brown\")\n",
    "unique_ingredient_list.remove(\"white\")\n",
    "unique_ingredient_list.remove(\"tomatoe\")\n",
    "unique_ingredient_list.remove(\"packed\")\n",
    "unique_ingredient_list.remove(\"plus\")\n",
    "unique_ingredient_list.remove(\"black\")\n",
    "unique_ingredient_list.remove(\"green\")\n",
    "unique_ingredient_list.remove(\"oil\")\n",
    "unique_ingredient_list.remove(\"red\")\n",
    "unique_ingredient_list.remove(\"hot water\")\n",
    "unique_ingredient_list.remove(\"low sodium\")\n",
    "unique_ingredient_list.remove(\"a\")\n",
    "unique_ingredient_list.remove(\"in water\")\n",
    "unique_ingredient_list.remove(\"blend\")\n",
    "unique_ingredient_list.remove(\"any\")\n",
    "unique_ingredient_list.remove(\"flavor\")\n",
    "unique_ingredient_list.remove(\"other\")\n",
    "unique_ingredient_list.remove(\"cook\")\n",
    "unique_ingredient_list.remove(\"soda\")\n",
    "unique_ingredient_list.remove(\"dry\")\n",
    "unique_ingredient_list.remove(\"bay\")\n",
    "unique_ingredient_list.remove(\"liquid\")\n",
    "unique_ingredient_list.remove(\"soft\")\n",
    "unique_ingredient_list.remove(\"fl\")\n",
    "unique_ingredient_list.remove(\"smooth\")\n",
    "unique_ingredient_list.remove(\"easy\")\n",
    "unique_ingredient_list.remove(\"fresh\")\n",
    "unique_ingredient_list.remove(\"more\")\n",
    "\n",
    "\n",
    "\n",
    "#since the list is sorted by most common ingredients, take the most common 1000 for simplicity\n",
    "unique_ingredient_list = unique_ingredient_list[0:1000]\n",
    "print (len(unique_ingredient_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "833\n"
     ]
    }
   ],
   "source": [
    "subs = ['potato','water','tomato','cheese','chicken','salt','garlic','sugar','pork','beef','liqueur','rice','cinnamon','ground','dried','frozen','sliced','roasted','cooked','mixed']\n",
    "#simplify any of the found above terms to just that term, singular. Adjectives like 'ground', 'dried' are also removed\n",
    "#because it is assumed the base word also appears\n",
    "\n",
    "for word in subs:\n",
    "    \n",
    "    res = [i for i in unique_ingredient_list if word in i]\n",
    "    for j in range(len(res)):\n",
    "        if res[j] != word:\n",
    "            unique_ingredient_list.remove(res[j])\n",
    "\n",
    "print(len(unique_ingredient_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cheese']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "subs = 'cheese'\n",
    "res = [i for i in unique_ingredient_list if subs in i]\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n"
     ]
    }
   ],
   "source": [
    "#Do a string search for every ingredient in this list in the long ingredient string list\n",
    "\n",
    "ingredients_list_final = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if(i%100000 == 0):\n",
    "        print(i)\n",
    "    temp_list = []\n",
    "    for j in range(99):\n",
    "        found_ingredient = False\n",
    "        if(ingredients_list[j][i] == None):\n",
    "            break\n",
    "        for m in range(len(unique_ingredient_list)):\n",
    "            if ingredients_list[j][i]['text'].find(unique_ingredient_list[m]) != -1:\n",
    "                temp_list.append(unique_ingredient_list[m])\n",
    "                found_ingredient = True\n",
    "                break\n",
    "                \n",
    "                \n",
    "    ingredients_list_final.append(temp_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['macaroni', 'cream', 'flour', 'salt', 'garlic', 'cheese', 'bread', 'margarine'], ['sugar', 'salt', 'pepper', 'cumin', 'mustard', 'fillet', 'olive oil'], ['oats', 'flour', 'carrot', 'salt', 'sugar', 'vegetable', 'molasse'], ['roll', 'chicken', 'cheese', 'parsley', 'sherry'], ['lemon', 'lemon', 'egg', 'sugar'], ['bread', 'butter', 'olive oil', 'scallop', 'garlic', 'white wine', 'lemon', 'parsley', 'butter', 'pepper', 'salt'], ['mix', 'flour', 'sugar', 'eggs', 'butter', 'chocolate'], ['canola', 'onion', 'garlic', 'ginger', 'lemon', 'curry', 'pumpkin', 'vegetable', 'salt', 'milk', 'juice', 'cilantro', 'chili'], ['potato', 'egg', 'cheese', 'cheese', 'salt', 'pepper', 'nutmeg', 'flour', 'flour', 'salt', 'leave', 'leave', 'cheese'], ['salt', 'lemon', 'sugar', 'egg', 'vanilla', 'flour', 'baking powder', 'salt', 'blueberr', 'butter']]\n"
     ]
    }
   ],
   "source": [
    "print(ingredients_list_final[5000:5010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save array\n",
    "ingredients_list_final_numpy = np.asarray(ingredients_list_final)\n",
    "np.save('ingredients_list_final.npy', ingredients_list_final_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list(['penne', 'cheese', 'cheese', 'cheese', 'chip', 'salt', 'flour', 'milk', 'cheese', 'cheese', 'salt', 'chip', 'garlic'])\n",
      " list(['macaroni', 'cheese', 'celery', 'pepper', 'pimento', 'mayonnaise', 'vinegar', 'salt', 'dill'])\n",
      " list(['tomato', 'salt', 'onion', 'pepper', 'pepper', 'pepper', 'cucumber', 'olive oil', 'basil'])\n",
      " ...\n",
      " list(['apple', 'sugar', 'salt', 'leave', 'chil', 'berr', 'seed', 'mustard', 'coriander', 'pork', 'pepper', 'salt', 'shallot', 'leave', 'brandy', 'cream', 'pork', 'mustard', 'horseradish', 'apple', 'cinnamon', 'apple', 'egg', 'flour', 'flour'])\n",
      " list(['round', 'sausage', 'bread', 'milk', 'garlic', 'parsley', 'egg', 'tomato', 'cheese', 'salt', 'tomato', 'pepper'])\n",
      " list(['sugar', 'molasse', 'cinnamon', 'coffee'])]\n"
     ]
    }
   ],
   "source": [
    "print(ingredients_list_final_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>pagesqr</th>\n",
       "      <th>age</th>\n",
       "      <th>agesqr</th>\n",
       "      <th>genre</th>\n",
       "      <th>is_ebook</th>\n",
       "      <th>average_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'author_id': '3041852', 'role': ''}]</td>\n",
       "      <td>162.0</td>\n",
       "      <td>12.727922</td>\n",
       "      <td>14</td>\n",
       "      <td>3.741657</td>\n",
       "      <td>history</td>\n",
       "      <td>0</td>\n",
       "      <td>4.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'author_id': '37778', 'role': ''}]</td>\n",
       "      <td>400.0</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>3.316625</td>\n",
       "      <td>history</td>\n",
       "      <td>0</td>\n",
       "      <td>3.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'author_id': '137561', 'role': ''}]</td>\n",
       "      <td>288.0</td>\n",
       "      <td>16.970563</td>\n",
       "      <td>14</td>\n",
       "      <td>3.741657</td>\n",
       "      <td>history</td>\n",
       "      <td>0</td>\n",
       "      <td>3.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'author_id': '51229', 'role': ''}, {'author_...</td>\n",
       "      <td>332.0</td>\n",
       "      <td>18.220867</td>\n",
       "      <td>8</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>history</td>\n",
       "      <td>1</td>\n",
       "      <td>4.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[{'author_id': '337108', 'role': ''}, {'author...</td>\n",
       "      <td>659.0</td>\n",
       "      <td>25.670995</td>\n",
       "      <td>14</td>\n",
       "      <td>3.741657</td>\n",
       "      <td>history</td>\n",
       "      <td>0</td>\n",
       "      <td>3.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8000</th>\n",
       "      <td>[{'author_id': '8108129', 'role': ''}]</td>\n",
       "      <td>270.0</td>\n",
       "      <td>16.431677</td>\n",
       "      <td>6</td>\n",
       "      <td>2.449490</td>\n",
       "      <td>young adult</td>\n",
       "      <td>1</td>\n",
       "      <td>3.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8002</th>\n",
       "      <td>[{'author_id': '614210', 'role': ''}]</td>\n",
       "      <td>199.0</td>\n",
       "      <td>14.106736</td>\n",
       "      <td>6</td>\n",
       "      <td>2.449490</td>\n",
       "      <td>young adult</td>\n",
       "      <td>0</td>\n",
       "      <td>3.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8003</th>\n",
       "      <td>[{'author_id': '11664', 'role': ''}]</td>\n",
       "      <td>208.0</td>\n",
       "      <td>14.422205</td>\n",
       "      <td>7</td>\n",
       "      <td>2.645751</td>\n",
       "      <td>young adult</td>\n",
       "      <td>1</td>\n",
       "      <td>4.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8005</th>\n",
       "      <td>[{'author_id': '2987125', 'role': ''}, {'autho...</td>\n",
       "      <td>288.0</td>\n",
       "      <td>16.970563</td>\n",
       "      <td>7</td>\n",
       "      <td>2.645751</td>\n",
       "      <td>young adult</td>\n",
       "      <td>0</td>\n",
       "      <td>4.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8006</th>\n",
       "      <td>[{'author_id': '2885318', 'role': ''}]</td>\n",
       "      <td>454.0</td>\n",
       "      <td>21.307276</td>\n",
       "      <td>5</td>\n",
       "      <td>2.236068</td>\n",
       "      <td>young adult</td>\n",
       "      <td>0</td>\n",
       "      <td>3.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5347 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                authors  num_pages    pagesqr  \\\n",
       "0                [{'author_id': '3041852', 'role': ''}]      162.0  12.727922   \n",
       "1                  [{'author_id': '37778', 'role': ''}]      400.0  20.000000   \n",
       "2                 [{'author_id': '137561', 'role': ''}]      288.0  16.970563   \n",
       "4     [{'author_id': '51229', 'role': ''}, {'author_...      332.0  18.220867   \n",
       "6     [{'author_id': '337108', 'role': ''}, {'author...      659.0  25.670995   \n",
       "...                                                 ...        ...        ...   \n",
       "8000             [{'author_id': '8108129', 'role': ''}]      270.0  16.431677   \n",
       "8002              [{'author_id': '614210', 'role': ''}]      199.0  14.106736   \n",
       "8003               [{'author_id': '11664', 'role': ''}]      208.0  14.422205   \n",
       "8005  [{'author_id': '2987125', 'role': ''}, {'autho...      288.0  16.970563   \n",
       "8006             [{'author_id': '2885318', 'role': ''}]      454.0  21.307276   \n",
       "\n",
       "      age    agesqr        genre  is_ebook  average_rating  \n",
       "0      14  3.741657      history         0            4.13  \n",
       "1      11  3.316625      history         0            3.93  \n",
       "2      14  3.741657      history         0            3.98  \n",
       "4       8  2.828427      history         1            4.28  \n",
       "6      14  3.741657      history         0            3.54  \n",
       "...   ...       ...          ...       ...             ...  \n",
       "8000    6  2.449490  young adult         1            3.84  \n",
       "8002    6  2.449490  young adult         0            3.79  \n",
       "8003    7  2.645751  young adult         1            4.10  \n",
       "8005    7  2.645751  young adult         0            4.15  \n",
       "8006    5  2.236068  young adult         0            3.42  \n",
       "\n",
       "[5347 rows x 8 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#select columns title, authors, num_pages, age, genre, and average_rating as data\n",
    "data = data[['authors', 'num_pages','pagesqr', 'age', 'agesqr','genre', 'is_ebook', 'average_rating']]\n",
    "data['is_ebook'] = data['is_ebook'].astype(int)\n",
    "#drop null values\n",
    "data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5347 entries, 0 to 8006\n",
      "Data columns (total 8 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   authors         5347 non-null   object \n",
      " 1   num_pages       5347 non-null   float64\n",
      " 2   pagesqr         5347 non-null   float64\n",
      " 3   age             5347 non-null   int64  \n",
      " 4   agesqr          5347 non-null   float64\n",
      " 5   genre           5347 non-null   object \n",
      " 6   is_ebook        5347 non-null   int32  \n",
      " 7   average_rating  5347 non-null   float64\n",
      "dtypes: float64(4), int32(1), int64(1), object(2)\n",
      "memory usage: 355.1+ KB\n"
     ]
    }
   ],
   "source": [
    "#look at the data\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f992616a9b0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# find no. of pages outliers\n",
    "# sns.boxplot(x=data['num_pages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers from no. of pages \n",
    "data = data.drop(data.index[data['num_pages'] >= 1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5347"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering\n",
    "#Transforming textual features into numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode authors column\n",
    "data['authors'] = le.fit_transform(data['authors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode genre column\n",
    "data['genre'] = le.fit_transform(data['genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5347 entries, 0 to 8006\n",
      "Data columns (total 8 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   authors         5347 non-null   int32  \n",
      " 1   num_pages       5347 non-null   float64\n",
      " 2   pagesqr         5347 non-null   float64\n",
      " 3   age             5347 non-null   int64  \n",
      " 4   agesqr          5347 non-null   float64\n",
      " 5   genre           5347 non-null   int32  \n",
      " 6   is_ebook        5347 non-null   int32  \n",
      " 7   average_rating  5347 non-null   float64\n",
      "dtypes: float64(4), int32(3), int64(1)\n",
      "memory usage: 313.3 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot encoding for authors and genre\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "max_value_authors = data['authors'].max()\n",
    "max_value_genre = data['genre'].max()\n",
    "authors_encoder = LabelBinarizer()\n",
    "genre_encoder = LabelBinarizer()\n",
    "authors_encoder.fit(data['authors'])\n",
    "genre_encoder.fit(data['genre'])\n",
    "transformed_authors = authors_encoder.transform(data['authors'])\n",
    "transformed_genres = genre_encoder.transform(data['genre'])\n",
    "onehot_authors_df = pd.DataFrame(transformed_authors)\n",
    "onehot_genres_df = pd.DataFrame(transformed_genres)\n",
    "data = pd.concat([data,onehot_authors_df,onehot_genres_df],axis=1).drop(['authors','genre'],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the data into attributes and labels\n",
    "X = data.drop(['average_rating'], axis = 1)\n",
    "y = data['average_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split 80% of the data to the training set and 20% of the data to test set \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.10, random_state = 10)\n",
    "X_train = torch.tensor(X_train.values).float()\n",
    "X_test = torch.tensor(X_test.values).float()\n",
    "y_train = torch.tensor(y_train.values).float()\n",
    "y_train = torch.unsqueeze(y_train,1)\n",
    "y_test = torch.tensor(y_test.values).float()\n",
    "y_test = torch.unsqueeze(y_test,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0133, 0.1155, 0.0022,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0067, 0.0816, 0.0043,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0590, 0.2429, 0.0285,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0133, 0.1155, 0.0054,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.1120, 0.3347, 0.0102,  ..., 0.0000, 1.0000, 0.0000],\n",
      "        [0.0533, 0.2309, 0.0151,  ..., 0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# remove nan values for page number,age,authors\n",
    "X_train_mask = X_train[:,0].eq(X_train[:,0]) # mask for nan values\n",
    "X_train_mask = X_train_mask & X_train[:,1].eq(X_train[:,1]) # mask for nan values\n",
    "X_train_mask = X_train_mask & X_train[:,2].eq(X_train[:,2]) # mask for nan values\n",
    "X_train_mask = X_train_mask & X_train[:,3].eq(X_train[:,3]) # mask for nan values\n",
    "X_train_mask = X_train_mask & X_train[:,4].eq(X_train[:,4]) # mask for nan values\n",
    "X_train_mask = X_train_mask & X_train[:,4].eq(X_train[:,15]) # mask for nan values\n",
    "\n",
    "X_test_mask = X_test[:,0].eq(X_test[:,0]) # mask for nan values\n",
    "X_test_mask = X_test_mask & X_test[:,1].eq(X_test[:,1]) # mask for nan values\n",
    "X_test_mask = X_test_mask & X_test[:,2].eq(X_test[:,2]) # mask for nan values\n",
    "X_test_mask = X_test_mask & X_test[:,3].eq(X_test[:,3]) # mask for nan values\n",
    "X_test_mask = X_test_mask & X_test[:,4].eq(X_test[:,4]) # mask for nan values\n",
    "X_test_mask = X_test_mask & X_test[:,15].eq(X_test[:,15]) # mask for nan values\n",
    "\n",
    "X_train = X_train[X_train_mask,:]\n",
    "y_train = y_train[X_train_mask,:]\n",
    "X_test = X_test[X_test_mask,:]\n",
    "y_test = y_test[X_test_mask,:]\n",
    "\n",
    "#normalize pages, age\n",
    "pages_max = torch.max(X_train[:,0])\n",
    "age_max = torch.max(X_train[:,1])\n",
    "pagesqr_max = torch.max(X_train[:,2])\n",
    "agesqr_max = torch.max(X_train[:,3])\n",
    "\n",
    "X_train[:,0] = X_train[:,0] / pages_max\n",
    "X_train[:,1] = X_train[:,1] / age_max\n",
    "X_train[:,2] = X_train[:,2] / pagesqr_max\n",
    "X_train[:,3] = X_train[:,3] / agesqr_max\n",
    "\n",
    "X_test[:,0] = X_test[:,0] / pages_max\n",
    "X_test[:,1] = X_test[:,1] / age_max\n",
    "X_test[:,2] = X_test[:,2] / pagesqr_max\n",
    "X_test[:,3] = X_test[:,3] / agesqr_max\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "X_train_nan = torch.isnan(X_train)\n",
    "print(X_train_nan)\n",
    "print(torch.sum(X_train_nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[3.7100000381469727]</td>\n",
       "      <td>[8841.1396484375]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[4.429999828338623]</td>\n",
       "      <td>[-51359.66015625]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[3.680000066757202]</td>\n",
       "      <td>[258.14410400390625]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[3.7300000190734863]</td>\n",
       "      <td>[257.9705505371094]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[3.559999942779541]</td>\n",
       "      <td>[-4363.0458984375]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[3.7799999713897705]</td>\n",
       "      <td>[258.25469970703125]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[4.110000133514404]</td>\n",
       "      <td>[3.910736083984375]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[3.990000009536743]</td>\n",
       "      <td>[-7972.32958984375]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[3.630000114440918]</td>\n",
       "      <td>[258.18670654296875]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[3.880000114440918]</td>\n",
       "      <td>[258.22998046875]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Actual             Predicted\n",
       "0  [3.7100000381469727]     [8841.1396484375]\n",
       "1   [4.429999828338623]     [-51359.66015625]\n",
       "2   [3.680000066757202]  [258.14410400390625]\n",
       "3  [3.7300000190734863]   [257.9705505371094]\n",
       "4   [3.559999942779541]    [-4363.0458984375]\n",
       "5  [3.7799999713897705]  [258.25469970703125]\n",
       "6   [4.110000133514404]   [3.910736083984375]\n",
       "7   [3.990000009536743]   [-7972.32958984375]\n",
       "8   [3.630000114440918]  [258.18670654296875]\n",
       "9   [3.880000114440918]     [258.22998046875]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = pd.DataFrame({'Actual': y_test.tolist(), 'Predicted': predictions.tolist()}).head(25)\n",
    "pred.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9926720710>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualise the above comparison result\n",
    "pred.plot(kind='bar', figsize=(13, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.2776167878627979\n",
      "MSE: 0.12774518548521124\n",
      "RMSE: 0.35741458488037564\n"
     ]
    }
   ],
   "source": [
    "# evaluate the performance of the algorithm\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, predictions))\n",
    "print('MSE:', metrics.mean_squared_error(y_test, predictions))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2856, 4413])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "\n",
    "    super().__init__()\n",
    "\n",
    "    self.loss_criterion = nn.MSELoss()\n",
    "    \n",
    "    self.fc_layers = nn.Sequential(\n",
    "          nn.Linear(4413,300),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(300,151),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(151,1)\n",
    "    )\n",
    "    \n",
    "\n",
    "  def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "    '''\n",
    "    Perform the forward pass with the net\n",
    "\n",
    "    '''\n",
    "    model_output = self.fc_layers(x)\n",
    "    return model_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training Loss :15.735135078430176 Test Loss:15.615273475646973\n",
      "Epoch 2 Training Loss :15.502562522888184 Test Loss:15.383432388305664\n",
      "Epoch 3 Training Loss :15.273833274841309 Test Loss:15.155447959899902\n",
      "Epoch 4 Training Loss :15.048802375793457 Test Loss:14.931161880493164\n",
      "Epoch 5 Training Loss :14.827361106872559 Test Loss:14.710461616516113\n",
      "Epoch 6 Training Loss :14.609353065490723 Test Loss:14.493183135986328\n",
      "Epoch 7 Training Loss :14.394558906555176 Test Loss:14.279114723205566\n",
      "Epoch 8 Training Loss :14.182868003845215 Test Loss:14.068144798278809\n",
      "Epoch 9 Training Loss :13.974233627319336 Test Loss:13.860233306884766\n",
      "Epoch 10 Training Loss :13.768583297729492 Test Loss:13.655309677124023\n",
      "Epoch 11 Training Loss :13.565756797790527 Test Loss:13.45322036743164\n",
      "Epoch 12 Training Loss :13.365621566772461 Test Loss:13.253800392150879\n",
      "Epoch 13 Training Loss :13.168015480041504 Test Loss:13.056900024414062\n",
      "Epoch 14 Training Loss :12.972800254821777 Test Loss:12.862394332885742\n",
      "Epoch 15 Training Loss :12.779918670654297 Test Loss:12.67023754119873\n",
      "Epoch 16 Training Loss :12.589319229125977 Test Loss:12.480406761169434\n",
      "Epoch 17 Training Loss :12.400917053222656 Test Loss:12.292794227600098\n",
      "Epoch 18 Training Loss :12.214628219604492 Test Loss:12.107303619384766\n",
      "Epoch 19 Training Loss :12.030364036560059 Test Loss:11.923835754394531\n",
      "Epoch 20 Training Loss :11.848045349121094 Test Loss:11.742293357849121\n",
      "Epoch 21 Training Loss :11.66759967803955 Test Loss:11.562578201293945\n",
      "Epoch 22 Training Loss :11.48893928527832 Test Loss:11.384631156921387\n",
      "Epoch 23 Training Loss :11.311956405639648 Test Loss:11.208348274230957\n",
      "Epoch 24 Training Loss :11.136526107788086 Test Loss:11.033609390258789\n",
      "Epoch 25 Training Loss :10.96253776550293 Test Loss:10.860306739807129\n",
      "Epoch 26 Training Loss :10.789900779724121 Test Loss:10.688352584838867\n",
      "Epoch 27 Training Loss :10.61854076385498 Test Loss:10.517678260803223\n",
      "Epoch 28 Training Loss :10.448390007019043 Test Loss:10.348215103149414\n",
      "Epoch 29 Training Loss :10.279400825500488 Test Loss:10.179915428161621\n",
      "Epoch 30 Training Loss :10.11153507232666 Test Loss:10.012738227844238\n",
      "Epoch 31 Training Loss :9.944756507873535 Test Loss:9.846643447875977\n",
      "Epoch 32 Training Loss :9.779036521911621 Test Loss:9.681599617004395\n",
      "Epoch 33 Training Loss :9.614341735839844 Test Loss:9.517578125\n",
      "Epoch 34 Training Loss :9.450642585754395 Test Loss:9.354558944702148\n",
      "Epoch 35 Training Loss :9.287914276123047 Test Loss:9.192513465881348\n",
      "Epoch 36 Training Loss :9.126131057739258 Test Loss:9.031418800354004\n",
      "Epoch 37 Training Loss :8.965269088745117 Test Loss:8.871246337890625\n",
      "Epoch 38 Training Loss :8.805309295654297 Test Loss:8.711977005004883\n",
      "Epoch 39 Training Loss :8.646235466003418 Test Loss:8.553596496582031\n",
      "Epoch 40 Training Loss :8.488037109375 Test Loss:8.396097183227539\n",
      "Epoch 41 Training Loss :8.330707550048828 Test Loss:8.239474296569824\n",
      "Epoch 42 Training Loss :8.174236297607422 Test Loss:8.083714485168457\n",
      "Epoch 43 Training Loss :8.01861572265625 Test Loss:7.92881441116333\n",
      "Epoch 44 Training Loss :7.863839626312256 Test Loss:7.774764537811279\n",
      "Epoch 45 Training Loss :7.709904670715332 Test Loss:7.621558666229248\n",
      "Epoch 46 Training Loss :7.556811809539795 Test Loss:7.469192981719971\n",
      "Epoch 47 Training Loss :7.4045610427856445 Test Loss:7.317670822143555\n",
      "Epoch 48 Training Loss :7.253155708312988 Test Loss:7.166995048522949\n",
      "Epoch 49 Training Loss :7.102599620819092 Test Loss:7.017172336578369\n",
      "Epoch 50 Training Loss :6.952899932861328 Test Loss:6.8682098388671875\n",
      "Epoch 51 Training Loss :6.804067611694336 Test Loss:6.720119476318359\n",
      "Epoch 52 Training Loss :6.656113624572754 Test Loss:6.572912693023682\n",
      "Epoch 53 Training Loss :6.5090532302856445 Test Loss:6.4266037940979\n",
      "Epoch 54 Training Loss :6.3629069328308105 Test Loss:6.281213283538818\n",
      "Epoch 55 Training Loss :6.21769380569458 Test Loss:6.136762619018555\n",
      "Epoch 56 Training Loss :6.07343864440918 Test Loss:5.993274688720703\n",
      "Epoch 57 Training Loss :5.930164813995361 Test Loss:5.850775718688965\n",
      "Epoch 58 Training Loss :5.787901878356934 Test Loss:5.709291934967041\n",
      "Epoch 59 Training Loss :5.646681308746338 Test Loss:5.568856716156006\n",
      "Epoch 60 Training Loss :5.506537437438965 Test Loss:5.429501056671143\n",
      "Epoch 61 Training Loss :5.367504596710205 Test Loss:5.291259288787842\n",
      "Epoch 62 Training Loss :5.229619026184082 Test Loss:5.154168605804443\n",
      "Epoch 63 Training Loss :5.092921257019043 Test Loss:5.018269062042236\n",
      "Epoch 64 Training Loss :4.957451820373535 Test Loss:4.883602142333984\n",
      "Epoch 65 Training Loss :4.823254108428955 Test Loss:4.750209331512451\n",
      "Epoch 66 Training Loss :4.690372467041016 Test Loss:4.618140697479248\n",
      "Epoch 67 Training Loss :4.558856010437012 Test Loss:4.487441539764404\n",
      "Epoch 68 Training Loss :4.428752899169922 Test Loss:4.358161926269531\n",
      "Epoch 69 Training Loss :4.300112724304199 Test Loss:4.230350971221924\n",
      "Epoch 70 Training Loss :4.172987461090088 Test Loss:4.104053974151611\n",
      "Epoch 71 Training Loss :4.047428607940674 Test Loss:3.979327917098999\n",
      "Epoch 72 Training Loss :3.9234864711761475 Test Loss:3.85622239112854\n",
      "Epoch 73 Training Loss :3.8012123107910156 Test Loss:3.734788179397583\n",
      "Epoch 74 Training Loss :3.6806607246398926 Test Loss:3.6150782108306885\n",
      "Epoch 75 Training Loss :3.5618834495544434 Test Loss:3.4971413612365723\n",
      "Epoch 76 Training Loss :3.444929599761963 Test Loss:3.381032943725586\n",
      "Epoch 77 Training Loss :3.3298494815826416 Test Loss:3.2667999267578125\n",
      "Epoch 78 Training Loss :3.216693162918091 Test Loss:3.1544904708862305\n",
      "Epoch 79 Training Loss :3.1055099964141846 Test Loss:3.0441532135009766\n",
      "Epoch 80 Training Loss :2.9963490962982178 Test Loss:2.9358386993408203\n",
      "Epoch 81 Training Loss :2.8892557621002197 Test Loss:2.8295910358428955\n",
      "Epoch 82 Training Loss :2.784273386001587 Test Loss:2.7254552841186523\n",
      "Epoch 83 Training Loss :2.681441307067871 Test Loss:2.6234703063964844\n",
      "Epoch 84 Training Loss :2.5808002948760986 Test Loss:2.523676633834839\n",
      "Epoch 85 Training Loss :2.482388734817505 Test Loss:2.426110029220581\n",
      "Epoch 86 Training Loss :2.3862407207489014 Test Loss:2.3308029174804688\n",
      "Epoch 87 Training Loss :2.292387008666992 Test Loss:2.2377867698669434\n",
      "Epoch 88 Training Loss :2.2008562088012695 Test Loss:2.147091865539551\n",
      "Epoch 89 Training Loss :2.1116743087768555 Test Loss:2.0587425231933594\n",
      "Epoch 90 Training Loss :2.0248630046844482 Test Loss:1.9727613925933838\n",
      "Epoch 91 Training Loss :1.9404397010803223 Test Loss:1.8891650438308716\n",
      "Epoch 92 Training Loss :1.8584188222885132 Test Loss:1.807964563369751\n",
      "Epoch 93 Training Loss :1.7788102626800537 Test Loss:1.7291711568832397\n",
      "Epoch 94 Training Loss :1.7016193866729736 Test Loss:1.6527894735336304\n",
      "Epoch 95 Training Loss :1.6268483400344849 Test Loss:1.5788211822509766\n",
      "Epoch 96 Training Loss :1.554494857788086 Test Loss:1.5072647333145142\n",
      "Epoch 97 Training Loss :1.4845538139343262 Test Loss:1.4381144046783447\n",
      "Epoch 98 Training Loss :1.4170150756835938 Test Loss:1.371358871459961\n",
      "Epoch 99 Training Loss :1.3518637418746948 Test Loss:1.3069837093353271\n",
      "Epoch 100 Training Loss :1.2890815734863281 Test Loss:1.244970679283142\n",
      "Epoch 101 Training Loss :1.2286463975906372 Test Loss:1.1852953433990479\n",
      "Epoch 102 Training Loss :1.1705318689346313 Test Loss:1.1279325485229492\n",
      "Epoch 103 Training Loss :1.114708662033081 Test Loss:1.0728520154953003\n",
      "Epoch 104 Training Loss :1.0611443519592285 Test Loss:1.0200213193893433\n",
      "Epoch 105 Training Loss :1.0098024606704712 Test Loss:0.9694034457206726\n",
      "Epoch 106 Training Loss :0.9606431126594543 Test Loss:0.9209586381912231\n",
      "Epoch 107 Training Loss :0.913624107837677 Test Loss:0.8746441602706909\n",
      "Epoch 108 Training Loss :0.8686999082565308 Test Loss:0.8304139971733093\n",
      "Epoch 109 Training Loss :0.8258227109909058 Test Loss:0.7882202863693237\n",
      "Epoch 110 Training Loss :0.7849432229995728 Test Loss:0.7480135560035706\n",
      "Epoch 111 Training Loss :0.746009349822998 Test Loss:0.7097413539886475\n",
      "Epoch 112 Training Loss :0.7089674472808838 Test Loss:0.6733498573303223\n",
      "Epoch 113 Training Loss :0.6737623810768127 Test Loss:0.6387838125228882\n",
      "Epoch 114 Training Loss :0.6403377056121826 Test Loss:0.6059862971305847\n",
      "Epoch 115 Training Loss :0.6086359024047852 Test Loss:0.5748994946479797\n",
      "Epoch 116 Training Loss :0.5785984396934509 Test Loss:0.5454643964767456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117 Training Loss :0.5501664876937866 Test Loss:0.5176218152046204\n",
      "Epoch 118 Training Loss :0.5232807397842407 Test Loss:0.49131301045417786\n",
      "Epoch 119 Training Loss :0.4978817403316498 Test Loss:0.46647846698760986\n",
      "Epoch 120 Training Loss :0.4739104211330414 Test Loss:0.4430589973926544\n",
      "Epoch 121 Training Loss :0.45130810141563416 Test Loss:0.4209958612918854\n",
      "Epoch 122 Training Loss :0.4300164580345154 Test Loss:0.40023040771484375\n",
      "Epoch 123 Training Loss :0.40997788310050964 Test Loss:0.3807052969932556\n",
      "Epoch 124 Training Loss :0.3911355435848236 Test Loss:0.36236366629600525\n",
      "Epoch 125 Training Loss :0.3734337091445923 Test Loss:0.34514981508255005\n",
      "Epoch 126 Training Loss :0.35681769251823425 Test Loss:0.32900896668434143\n",
      "Epoch 127 Training Loss :0.3412345051765442 Test Loss:0.31388816237449646\n",
      "Epoch 128 Training Loss :0.3266320526599884 Test Loss:0.29973551630973816\n",
      "Epoch 129 Training Loss :0.3129596412181854 Test Loss:0.286500483751297\n",
      "Epoch 130 Training Loss :0.3001681864261627 Test Loss:0.27413395047187805\n",
      "Epoch 131 Training Loss :0.2882102131843567 Test Loss:0.26258885860443115\n",
      "Epoch 132 Training Loss :0.2770400047302246 Test Loss:0.2518194913864136\n",
      "Epoch 133 Training Loss :0.266613245010376 Test Loss:0.2417818307876587\n",
      "Epoch 134 Training Loss :0.2568877041339874 Test Loss:0.2324337214231491\n",
      "Epoch 135 Training Loss :0.2478225827217102 Test Loss:0.22373446822166443\n",
      "Epoch 136 Training Loss :0.23937882483005524 Test Loss:0.21564526855945587\n",
      "Epoch 137 Training Loss :0.23151910305023193 Test Loss:0.2081288993358612\n",
      "Epoch 138 Training Loss :0.2242077887058258 Test Loss:0.20114994049072266\n",
      "Epoch 139 Training Loss :0.21741098165512085 Test Loss:0.19467464089393616\n",
      "Epoch 140 Training Loss :0.21109625697135925 Test Loss:0.1886707842350006\n",
      "Epoch 141 Training Loss :0.20523284375667572 Test Loss:0.18310794234275818\n",
      "Epoch 142 Training Loss :0.19979165494441986 Test Loss:0.17795725166797638\n",
      "Epoch 143 Training Loss :0.19474506378173828 Test Loss:0.17319127917289734\n",
      "Epoch 144 Training Loss :0.19006690382957458 Test Loss:0.16878411173820496\n",
      "Epoch 145 Training Loss :0.18573272228240967 Test Loss:0.16471152007579803\n",
      "Epoch 146 Training Loss :0.1817191243171692 Test Loss:0.1609504073858261\n",
      "Epoch 147 Training Loss :0.17800424993038177 Test Loss:0.15747907757759094\n",
      "Epoch 148 Training Loss :0.17456750571727753 Test Loss:0.1542772352695465\n",
      "Epoch 149 Training Loss :0.17138950526714325 Test Loss:0.15132565796375275\n",
      "Epoch 150 Training Loss :0.16845197975635529 Test Loss:0.1486063301563263\n",
      "Epoch 151 Training Loss :0.16573800146579742 Test Loss:0.14610256254673004\n",
      "Epoch 152 Training Loss :0.1632314920425415 Test Loss:0.14379851520061493\n",
      "Epoch 153 Training Loss :0.1609175056219101 Test Loss:0.14167951047420502\n",
      "Epoch 154 Training Loss :0.15878206491470337 Test Loss:0.139731764793396\n",
      "Epoch 155 Training Loss :0.1568121463060379 Test Loss:0.13794247806072235\n",
      "Epoch 156 Training Loss :0.15499556064605713 Test Loss:0.1362997442483902\n",
      "Epoch 157 Training Loss :0.15332092344760895 Test Loss:0.13479235768318176\n",
      "Epoch 158 Training Loss :0.15177758038043976 Test Loss:0.13340988755226135\n",
      "Epoch 159 Training Loss :0.1503557413816452 Test Loss:0.13214272260665894\n",
      "Epoch 160 Training Loss :0.14904619753360748 Test Loss:0.13098189234733582\n",
      "Epoch 161 Training Loss :0.14784039556980133 Test Loss:0.12991902232170105\n",
      "Epoch 162 Training Loss :0.14673040807247162 Test Loss:0.12894639372825623\n",
      "Epoch 163 Training Loss :0.14570888876914978 Test Loss:0.1280568242073059\n",
      "Epoch 164 Training Loss :0.14476898312568665 Test Loss:0.12724371254444122\n",
      "Epoch 165 Training Loss :0.1439044177532196 Test Loss:0.1265009194612503\n",
      "Epoch 166 Training Loss :0.1431092619895935 Test Loss:0.12582270801067352\n",
      "Epoch 167 Training Loss :0.14237815141677856 Test Loss:0.12520386278629303\n",
      "Epoch 168 Training Loss :0.14170600473880768 Test Loss:0.12463952600955963\n",
      "Epoch 169 Training Loss :0.14108820259571075 Test Loss:0.12412519007921219\n",
      "Epoch 170 Training Loss :0.14052045345306396 Test Loss:0.1236567497253418\n",
      "Epoch 171 Training Loss :0.13999876379966736 Test Loss:0.12323036789894104\n",
      "Epoch 172 Training Loss :0.13951945304870605 Test Loss:0.12284249067306519\n",
      "Epoch 173 Training Loss :0.13907916843891144 Test Loss:0.12248992919921875\n",
      "Epoch 174 Training Loss :0.1386747509241104 Test Loss:0.12216967344284058\n",
      "Epoch 175 Training Loss :0.13830330967903137 Test Loss:0.121878981590271\n",
      "Epoch 176 Training Loss :0.1379622220993042 Test Loss:0.12161531299352646\n",
      "Epoch 177 Training Loss :0.13764896988868713 Test Loss:0.12137634307146072\n",
      "Epoch 178 Training Loss :0.1373613327741623 Test Loss:0.12115995585918427\n",
      "Epoch 179 Training Loss :0.13709723949432373 Test Loss:0.1209641695022583\n",
      "Epoch 180 Training Loss :0.13685473799705505 Test Loss:0.12078716605901718\n",
      "Epoch 181 Training Loss :0.13663209974765778 Test Loss:0.12062731385231018\n",
      "Epoch 182 Training Loss :0.13642767071723938 Test Loss:0.12048309296369553\n",
      "Epoch 183 Training Loss :0.13623996078968048 Test Loss:0.12035311758518219\n",
      "Epoch 184 Training Loss :0.13606761395931244 Test Loss:0.1202361062169075\n",
      "Epoch 185 Training Loss :0.1359093338251114 Test Loss:0.12013085931539536\n",
      "Epoch 186 Training Loss :0.1357640027999878 Test Loss:0.12003636360168457\n",
      "Epoch 187 Training Loss :0.1356305181980133 Test Loss:0.11995160579681396\n",
      "Epoch 188 Training Loss :0.13550791144371033 Test Loss:0.1198756992816925\n",
      "Epoch 189 Training Loss :0.13539527356624603 Test Loss:0.11980781704187393\n",
      "Epoch 190 Training Loss :0.13529178500175476 Test Loss:0.11974723637104034\n",
      "Epoch 191 Training Loss :0.13519668579101562 Test Loss:0.11969323456287384\n",
      "Epoch 192 Training Loss :0.13510924577713013 Test Loss:0.11964520066976547\n",
      "Epoch 193 Training Loss :0.1350288689136505 Test Loss:0.11960257589817047\n",
      "Epoch 194 Training Loss :0.13495492935180664 Test Loss:0.11956483125686646\n",
      "Epoch 195 Training Loss :0.13488692045211792 Test Loss:0.11953148990869522\n",
      "Epoch 196 Training Loss :0.13482432067394257 Test Loss:0.11950213462114334\n",
      "Epoch 197 Training Loss :0.13476669788360596 Test Loss:0.1194763332605362\n",
      "Epoch 198 Training Loss :0.13471363484859467 Test Loss:0.11945376545190811\n",
      "Epoch 199 Training Loss :0.13466471433639526 Test Loss:0.11943408846855164\n",
      "Epoch 200 Training Loss :0.13461962342262268 Test Loss:0.11941701918840408\n",
      "Epoch 201 Training Loss :0.13457803428173065 Test Loss:0.11940226703882217\n",
      "Epoch 202 Training Loss :0.1345396339893341 Test Loss:0.1193896010518074\n",
      "Epoch 203 Training Loss :0.13450416922569275 Test Loss:0.11937880516052246\n",
      "Epoch 204 Training Loss :0.13447140157222748 Test Loss:0.11936967074871063\n",
      "Epoch 205 Training Loss :0.134441077709198 Test Loss:0.1193620041012764\n",
      "Epoch 206 Training Loss :0.1344130039215088 Test Loss:0.11935567110776901\n",
      "Epoch 207 Training Loss :0.13438700139522552 Test Loss:0.11935046315193176\n",
      "Epoch 208 Training Loss :0.13436287641525269 Test Loss:0.11934629827737808\n",
      "Epoch 209 Training Loss :0.13434049487113953 Test Loss:0.11934303492307663\n",
      "Epoch 210 Training Loss :0.13431966304779053 Test Loss:0.11934056878089905\n",
      "Epoch 211 Training Loss :0.13430029153823853 Test Loss:0.11933878064155579\n",
      "Epoch 212 Training Loss :0.13428226113319397 Test Loss:0.11933760344982147\n",
      "Epoch 213 Training Loss :0.13426542282104492 Test Loss:0.11933694034814835\n",
      "Epoch 214 Training Loss :0.1342497020959854 Test Loss:0.11933673918247223\n",
      "Epoch 215 Training Loss :0.13423499464988708 Test Loss:0.11933691054582596\n",
      "Epoch 216 Training Loss :0.13422122597694397 Test Loss:0.11933737993240356\n",
      "Epoch 217 Training Loss :0.1342083066701889 Test Loss:0.11933815479278564\n",
      "Epoch 218 Training Loss :0.13419616222381592 Test Loss:0.11933913826942444\n",
      "Epoch 219 Training Loss :0.13418471813201904 Test Loss:0.11934030801057816\n",
      "Epoch 220 Training Loss :0.1341739445924759 Test Loss:0.11934161186218262\n",
      "Epoch 221 Training Loss :0.1341637670993805 Test Loss:0.11934302747249603\n",
      "Epoch 222 Training Loss :0.13415412604808807 Test Loss:0.11934452503919601\n",
      "Epoch 223 Training Loss :0.13414499163627625 Test Loss:0.11934608221054077\n",
      "Epoch 224 Training Loss :0.13413630425930023 Test Loss:0.11934766173362732\n",
      "Epoch 225 Training Loss :0.13412804901599884 Test Loss:0.11934926360845566\n",
      "Epoch 226 Training Loss :0.1341201812028885 Test Loss:0.1193508505821228\n",
      "Epoch 227 Training Loss :0.1341126561164856 Test Loss:0.11935241520404816\n",
      "Epoch 228 Training Loss :0.13410545885562897 Test Loss:0.11935394257307053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229 Training Loss :0.13409854471683502 Test Loss:0.11935544013977051\n",
      "Epoch 230 Training Loss :0.13409189879894257 Test Loss:0.11935686320066452\n",
      "Epoch 231 Training Loss :0.1340855211019516 Test Loss:0.11935821920633316\n",
      "Epoch 232 Training Loss :0.13407935202121735 Test Loss:0.11935952305793762\n",
      "Epoch 233 Training Loss :0.1340733915567398 Test Loss:0.11936072260141373\n",
      "Epoch 234 Training Loss :0.1340676248073578 Test Loss:0.11936185508966446\n",
      "Epoch 235 Training Loss :0.1340620368719101 Test Loss:0.11936289072036743\n",
      "Epoch 236 Training Loss :0.13405659794807434 Test Loss:0.11936382949352264\n",
      "Epoch 237 Training Loss :0.13405130803585052 Test Loss:0.11936469376087189\n",
      "Epoch 238 Training Loss :0.13404616713523865 Test Loss:0.11936545372009277\n",
      "Epoch 239 Training Loss :0.13404113054275513 Test Loss:0.1193661093711853\n",
      "Epoch 240 Training Loss :0.13403621315956116 Test Loss:0.11936668306589127\n",
      "Epoch 241 Training Loss :0.13403140008449554 Test Loss:0.11936715245246887\n",
      "Epoch 242 Training Loss :0.1340266764163971 Test Loss:0.11936753243207932\n",
      "Epoch 243 Training Loss :0.134022057056427 Test Loss:0.11936779320240021\n",
      "Epoch 244 Training Loss :0.13401749730110168 Test Loss:0.11936797946691513\n",
      "Epoch 245 Training Loss :0.13401302695274353 Test Loss:0.1193680539727211\n",
      "Epoch 246 Training Loss :0.13400861620903015 Test Loss:0.1193680465221405\n",
      "Epoch 247 Training Loss :0.13400426506996155 Test Loss:0.11936795711517334\n",
      "Epoch 248 Training Loss :0.1339999884366989 Test Loss:0.11936776340007782\n",
      "Epoch 249 Training Loss :0.13399574160575867 Test Loss:0.11936748772859573\n",
      "Epoch 250 Training Loss :0.1339915692806244 Test Loss:0.11936712265014648\n",
      "Epoch 251 Training Loss :0.1339874267578125 Test Loss:0.11936667561531067\n",
      "Epoch 252 Training Loss :0.1339833289384842 Test Loss:0.11936615407466888\n",
      "Epoch 253 Training Loss :0.13397926092147827 Test Loss:0.11936553567647934\n",
      "Epoch 254 Training Loss :0.13397523760795593 Test Loss:0.11936486512422562\n",
      "Epoch 255 Training Loss :0.13397124409675598 Test Loss:0.11936410516500473\n",
      "Epoch 256 Training Loss :0.13396728038787842 Test Loss:0.11936326324939728\n",
      "Epoch 257 Training Loss :0.13396334648132324 Test Loss:0.11936236172914505\n",
      "Epoch 258 Training Loss :0.13395942747592926 Test Loss:0.11936140060424805\n",
      "Epoch 259 Training Loss :0.13395553827285767 Test Loss:0.11936035752296448\n",
      "Epoch 260 Training Loss :0.13395167887210846 Test Loss:0.11935926228761673\n",
      "Epoch 261 Training Loss :0.13394783437252045 Test Loss:0.11935808509588242\n",
      "Epoch 262 Training Loss :0.13394401967525482 Test Loss:0.11935687065124512\n",
      "Epoch 263 Training Loss :0.1339402049779892 Test Loss:0.11935558915138245\n",
      "Epoch 264 Training Loss :0.13393642008304596 Test Loss:0.1193542554974556\n",
      "Epoch 265 Training Loss :0.13393263518810272 Test Loss:0.11935286223888397\n",
      "Epoch 266 Training Loss :0.13392888009548187 Test Loss:0.11935143172740936\n",
      "Epoch 267 Training Loss :0.13392513990402222 Test Loss:0.11934994161128998\n",
      "Epoch 268 Training Loss :0.13392141461372375 Test Loss:0.11934839934110641\n",
      "Epoch 269 Training Loss :0.1339176893234253 Test Loss:0.11934682726860046\n",
      "Epoch 270 Training Loss :0.13391397893428802 Test Loss:0.11934518814086914\n",
      "Epoch 271 Training Loss :0.13391026854515076 Test Loss:0.11934353411197662\n",
      "Epoch 272 Training Loss :0.13390658795833588 Test Loss:0.11934182792901993\n",
      "Epoch 273 Training Loss :0.133902907371521 Test Loss:0.11934009939432144\n",
      "Epoch 274 Training Loss :0.1338992416858673 Test Loss:0.11933831125497818\n",
      "Epoch 275 Training Loss :0.13389557600021362 Test Loss:0.11933649331331253\n",
      "Epoch 276 Training Loss :0.13389192521572113 Test Loss:0.11933465301990509\n",
      "Epoch 277 Training Loss :0.13388828933238983 Test Loss:0.11933276802301407\n",
      "Epoch 278 Training Loss :0.13388463854789734 Test Loss:0.11933084577322006\n",
      "Epoch 279 Training Loss :0.13388101756572723 Test Loss:0.11932890862226486\n",
      "Epoch 280 Training Loss :0.13387739658355713 Test Loss:0.11932693421840668\n",
      "Epoch 281 Training Loss :0.13387377560138702 Test Loss:0.1193249449133873\n",
      "Epoch 282 Training Loss :0.1338701695203781 Test Loss:0.11932291835546494\n",
      "Epoch 283 Training Loss :0.1338665634393692 Test Loss:0.11932087689638138\n",
      "Epoch 284 Training Loss :0.13386297225952148 Test Loss:0.11931879073381424\n",
      "Epoch 285 Training Loss :0.13385938107967377 Test Loss:0.1193167045712471\n",
      "Epoch 286 Training Loss :0.13385578989982605 Test Loss:0.11931460350751877\n",
      "Epoch 287 Training Loss :0.13385221362113953 Test Loss:0.11931245774030685\n",
      "Epoch 288 Training Loss :0.133848637342453 Test Loss:0.11931030452251434\n",
      "Epoch 289 Training Loss :0.13384507596492767 Test Loss:0.11930814385414124\n",
      "Epoch 290 Training Loss :0.13384149968624115 Test Loss:0.11930596083402634\n",
      "Epoch 291 Training Loss :0.13383793830871582 Test Loss:0.11930375546216965\n",
      "Epoch 292 Training Loss :0.13383439183235168 Test Loss:0.11930152773857117\n",
      "Epoch 293 Training Loss :0.13383084535598755 Test Loss:0.11929928511381149\n",
      "Epoch 294 Training Loss :0.1338272988796234 Test Loss:0.11929704248905182\n",
      "Epoch 295 Training Loss :0.13382376730442047 Test Loss:0.11929478496313095\n",
      "Epoch 296 Training Loss :0.13382022082805634 Test Loss:0.11929251253604889\n",
      "Epoch 297 Training Loss :0.1338167041540146 Test Loss:0.11929022520780563\n",
      "Epoch 298 Training Loss :0.13381317257881165 Test Loss:0.11928793042898178\n",
      "Epoch 299 Training Loss :0.1338096559047699 Test Loss:0.11928561329841614\n",
      "Epoch 300 Training Loss :0.13380613923072815 Test Loss:0.11928331106901169\n",
      "Epoch 301 Training Loss :0.1338026374578476 Test Loss:0.11928096413612366\n",
      "Epoch 302 Training Loss :0.13379912078380585 Test Loss:0.11927864700555801\n",
      "Epoch 303 Training Loss :0.1337956190109253 Test Loss:0.11927629262208939\n",
      "Epoch 304 Training Loss :0.13379211723804474 Test Loss:0.11927393823862076\n",
      "Epoch 305 Training Loss :0.13378863036632538 Test Loss:0.11927158385515213\n",
      "Epoch 306 Training Loss :0.13378514349460602 Test Loss:0.11926921457052231\n",
      "Epoch 307 Training Loss :0.13378165662288666 Test Loss:0.11926683038473129\n",
      "Epoch 308 Training Loss :0.1337781697511673 Test Loss:0.11926446855068207\n",
      "Epoch 309 Training Loss :0.13377469778060913 Test Loss:0.11926208436489105\n",
      "Epoch 310 Training Loss :0.13377121090888977 Test Loss:0.11925969272851944\n",
      "Epoch 311 Training Loss :0.1337677538394928 Test Loss:0.11925730109214783\n",
      "Epoch 312 Training Loss :0.13376428186893463 Test Loss:0.11925490945577621\n",
      "Epoch 313 Training Loss :0.13376080989837646 Test Loss:0.119252510368824\n",
      "Epoch 314 Training Loss :0.13375736773014069 Test Loss:0.1192501038312912\n",
      "Epoch 315 Training Loss :0.1337539106607437 Test Loss:0.1192476898431778\n",
      "Epoch 316 Training Loss :0.13375045359134674 Test Loss:0.11924529075622559\n",
      "Epoch 317 Training Loss :0.13374701142311096 Test Loss:0.11924287676811218\n",
      "Epoch 318 Training Loss :0.13374356925487518 Test Loss:0.11924045532941818\n",
      "Epoch 319 Training Loss :0.1337401270866394 Test Loss:0.11923804134130478\n",
      "Epoch 320 Training Loss :0.13373668491840363 Test Loss:0.11923561990261078\n",
      "Epoch 321 Training Loss :0.13373325765132904 Test Loss:0.11923319101333618\n",
      "Epoch 322 Training Loss :0.13372983038425446 Test Loss:0.11923077702522278\n",
      "Epoch 323 Training Loss :0.13372640311717987 Test Loss:0.11922834813594818\n",
      "Epoch 324 Training Loss :0.13372299075126648 Test Loss:0.11922591179609299\n",
      "Epoch 325 Training Loss :0.1337195634841919 Test Loss:0.11922349035739899\n",
      "Epoch 326 Training Loss :0.1337161511182785 Test Loss:0.11922105401754379\n",
      "Epoch 327 Training Loss :0.1337127536535263 Test Loss:0.11921864002943039\n",
      "Epoch 328 Training Loss :0.1337093561887741 Test Loss:0.11921621114015579\n",
      "Epoch 329 Training Loss :0.13370594382286072 Test Loss:0.11921376734972\n",
      "Epoch 330 Training Loss :0.13370256125926971 Test Loss:0.119211345911026\n",
      "Epoch 331 Training Loss :0.13369916379451752 Test Loss:0.1192089170217514\n",
      "Epoch 332 Training Loss :0.13369576632976532 Test Loss:0.1192064881324768\n",
      "Epoch 333 Training Loss :0.13369238376617432 Test Loss:0.11920405179262161\n",
      "Epoch 334 Training Loss :0.1336890161037445 Test Loss:0.11920161545276642\n",
      "Epoch 335 Training Loss :0.1336856335401535 Test Loss:0.11919919401407242\n",
      "Epoch 336 Training Loss :0.1336822658777237 Test Loss:0.11919676512479782\n",
      "Epoch 337 Training Loss :0.13367889821529388 Test Loss:0.11919433623552322\n",
      "Epoch 338 Training Loss :0.13367553055286407 Test Loss:0.11919192224740982\n",
      "Epoch 339 Training Loss :0.13367216289043427 Test Loss:0.11918948590755463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 340 Training Loss :0.13366881012916565 Test Loss:0.11918707191944122\n",
      "Epoch 341 Training Loss :0.13366545736789703 Test Loss:0.11918465793132782\n",
      "Epoch 342 Training Loss :0.13366210460662842 Test Loss:0.11918222904205322\n",
      "Epoch 343 Training Loss :0.133658766746521 Test Loss:0.11917981505393982\n",
      "Epoch 344 Training Loss :0.13365541398525238 Test Loss:0.11917740106582642\n",
      "Epoch 345 Training Loss :0.13365207612514496 Test Loss:0.11917499452829361\n",
      "Epoch 346 Training Loss :0.13364875316619873 Test Loss:0.1191725805401802\n",
      "Epoch 347 Training Loss :0.1336454153060913 Test Loss:0.119170181453228\n",
      "Epoch 348 Training Loss :0.13364209234714508 Test Loss:0.1191677674651146\n",
      "Epoch 349 Training Loss :0.13363876938819885 Test Loss:0.11916536837816238\n",
      "Epoch 350 Training Loss :0.13363544642925262 Test Loss:0.11916296184062958\n",
      "Epoch 351 Training Loss :0.1336321234703064 Test Loss:0.11916056275367737\n",
      "Epoch 352 Training Loss :0.13362881541252136 Test Loss:0.11915817111730576\n",
      "Epoch 353 Training Loss :0.13362550735473633 Test Loss:0.11915577948093414\n",
      "Epoch 354 Training Loss :0.1336221992969513 Test Loss:0.11915338784456253\n",
      "Epoch 355 Training Loss :0.13361889123916626 Test Loss:0.11915099620819092\n",
      "Epoch 356 Training Loss :0.13361559808254242 Test Loss:0.1191486194729805\n",
      "Epoch 357 Training Loss :0.13361230492591858 Test Loss:0.11914622038602829\n",
      "Epoch 358 Training Loss :0.13360901176929474 Test Loss:0.11914384365081787\n",
      "Epoch 359 Training Loss :0.1336057186126709 Test Loss:0.11914146691560745\n",
      "Epoch 360 Training Loss :0.13360242545604706 Test Loss:0.11913909018039703\n",
      "Epoch 361 Training Loss :0.1335991472005844 Test Loss:0.11913671344518661\n",
      "Epoch 362 Training Loss :0.13359586894512177 Test Loss:0.1191343367099762\n",
      "Epoch 363 Training Loss :0.13359259068965912 Test Loss:0.11913196742534637\n",
      "Epoch 364 Training Loss :0.13358932733535767 Test Loss:0.11912960559129715\n",
      "Epoch 365 Training Loss :0.1335860639810562 Test Loss:0.11912723630666733\n",
      "Epoch 366 Training Loss :0.13358280062675476 Test Loss:0.1191248893737793\n",
      "Epoch 367 Training Loss :0.1335795372724533 Test Loss:0.11912252753973007\n",
      "Epoch 368 Training Loss :0.13357627391815186 Test Loss:0.11912017315626144\n",
      "Epoch 369 Training Loss :0.1335730254650116 Test Loss:0.11911782622337341\n",
      "Epoch 370 Training Loss :0.13356977701187134 Test Loss:0.11911547183990479\n",
      "Epoch 371 Training Loss :0.13356652855873108 Test Loss:0.11911311745643616\n",
      "Epoch 372 Training Loss :0.13356329500675201 Test Loss:0.11911077797412872\n",
      "Epoch 373 Training Loss :0.13356004655361176 Test Loss:0.11910844594240189\n",
      "Epoch 374 Training Loss :0.1335568130016327 Test Loss:0.11910610646009445\n",
      "Epoch 375 Training Loss :0.13355357944965363 Test Loss:0.11910377442836761\n",
      "Epoch 376 Training Loss :0.13355036079883575 Test Loss:0.11910143494606018\n",
      "Epoch 377 Training Loss :0.13354714214801788 Test Loss:0.11909910291433334\n",
      "Epoch 378 Training Loss :0.13354390859603882 Test Loss:0.1190967783331871\n",
      "Epoch 379 Training Loss :0.13354068994522095 Test Loss:0.11909445375204086\n",
      "Epoch 380 Training Loss :0.13353748619556427 Test Loss:0.11909213662147522\n",
      "Epoch 381 Training Loss :0.1335342824459076 Test Loss:0.11908981949090958\n",
      "Epoch 382 Training Loss :0.13353106379508972 Test Loss:0.11908750236034393\n",
      "Epoch 383 Training Loss :0.13352786004543304 Test Loss:0.11908519268035889\n",
      "Epoch 384 Training Loss :0.13352467119693756 Test Loss:0.11908288300037384\n",
      "Epoch 385 Training Loss :0.13352146744728088 Test Loss:0.1190805733203888\n",
      "Epoch 386 Training Loss :0.1335182785987854 Test Loss:0.11907827854156494\n",
      "Epoch 387 Training Loss :0.13351508975028992 Test Loss:0.11907597631216049\n",
      "Epoch 388 Training Loss :0.13351191580295563 Test Loss:0.11907367408275604\n",
      "Epoch 389 Training Loss :0.13350872695446014 Test Loss:0.11907138675451279\n",
      "Epoch 390 Training Loss :0.13350555300712585 Test Loss:0.11906910687685013\n",
      "Epoch 391 Training Loss :0.13350237905979156 Test Loss:0.11906681209802628\n",
      "Epoch 392 Training Loss :0.13349920511245728 Test Loss:0.11906453967094421\n",
      "Epoch 393 Training Loss :0.13349604606628418 Test Loss:0.11906225979328156\n",
      "Epoch 394 Training Loss :0.1334928721189499 Test Loss:0.1190599873661995\n",
      "Epoch 395 Training Loss :0.1334897130727768 Test Loss:0.11905770748853683\n",
      "Epoch 396 Training Loss :0.1334865689277649 Test Loss:0.11905544996261597\n",
      "Epoch 397 Training Loss :0.1334834098815918 Test Loss:0.11905317008495331\n",
      "Epoch 398 Training Loss :0.1334802508354187 Test Loss:0.11905091255903244\n",
      "Epoch 399 Training Loss :0.1334771066904068 Test Loss:0.11904865503311157\n",
      "Epoch 400 Training Loss :0.1334739625453949 Test Loss:0.1190463975071907\n",
      "Epoch 401 Training Loss :0.133470818400383 Test Loss:0.11904413998126984\n",
      "Epoch 402 Training Loss :0.1334676891565323 Test Loss:0.11904188245534897\n",
      "Epoch 403 Training Loss :0.13346455991268158 Test Loss:0.11903964728116989\n",
      "Epoch 404 Training Loss :0.13346143066883087 Test Loss:0.11903739720582962\n",
      "Epoch 405 Training Loss :0.13345830142498016 Test Loss:0.11903516203165054\n",
      "Epoch 406 Training Loss :0.13345518708229065 Test Loss:0.11903291195631027\n",
      "Epoch 407 Training Loss :0.13345207273960114 Test Loss:0.1190306693315506\n",
      "Epoch 408 Training Loss :0.13344894349575043 Test Loss:0.11902843415737152\n",
      "Epoch 409 Training Loss :0.1334458440542221 Test Loss:0.11902621388435364\n",
      "Epoch 410 Training Loss :0.1334427297115326 Test Loss:0.11902397871017456\n",
      "Epoch 411 Training Loss :0.13343961536884308 Test Loss:0.11902175843715668\n",
      "Epoch 412 Training Loss :0.13343651592731476 Test Loss:0.1190195307135582\n",
      "Epoch 413 Training Loss :0.13343341648578644 Test Loss:0.11901731789112091\n",
      "Epoch 414 Training Loss :0.1334303319454193 Test Loss:0.11901510506868362\n",
      "Epoch 415 Training Loss :0.133427232503891 Test Loss:0.11901289224624634\n",
      "Epoch 416 Training Loss :0.13342414796352386 Test Loss:0.11901068687438965\n",
      "Epoch 417 Training Loss :0.13342106342315674 Test Loss:0.11900848150253296\n",
      "Epoch 418 Training Loss :0.1334179788827896 Test Loss:0.11900628358125687\n",
      "Epoch 419 Training Loss :0.13341489434242249 Test Loss:0.11900408565998077\n",
      "Epoch 420 Training Loss :0.13341182470321655 Test Loss:0.11900189518928528\n",
      "Epoch 421 Training Loss :0.13340875506401062 Test Loss:0.11899970471858978\n",
      "Epoch 422 Training Loss :0.1334056854248047 Test Loss:0.11899751424789429\n",
      "Epoch 423 Training Loss :0.13340261578559875 Test Loss:0.11899533122777939\n",
      "Epoch 424 Training Loss :0.13339956104755402 Test Loss:0.11899314075708389\n",
      "Epoch 425 Training Loss :0.13339650630950928 Test Loss:0.11899098008871078\n",
      "Epoch 426 Training Loss :0.13339345157146454 Test Loss:0.11898879706859589\n",
      "Epoch 427 Training Loss :0.1333903968334198 Test Loss:0.11898662894964218\n",
      "Epoch 428 Training Loss :0.13338734209537506 Test Loss:0.11898446083068848\n",
      "Epoch 429 Training Loss :0.13338430225849152 Test Loss:0.11898230016231537\n",
      "Epoch 430 Training Loss :0.13338124752044678 Test Loss:0.11898013204336166\n",
      "Epoch 431 Training Loss :0.13337822258472443 Test Loss:0.11897796392440796\n",
      "Epoch 432 Training Loss :0.13337518274784088 Test Loss:0.11897581070661545\n",
      "Epoch 433 Training Loss :0.13337214291095734 Test Loss:0.11897365748882294\n",
      "Epoch 434 Training Loss :0.13336911797523499 Test Loss:0.11897151172161102\n",
      "Epoch 435 Training Loss :0.13336609303951263 Test Loss:0.1189693734049797\n",
      "Epoch 436 Training Loss :0.13336306810379028 Test Loss:0.11896722763776779\n",
      "Epoch 437 Training Loss :0.13336004316806793 Test Loss:0.11896508187055588\n",
      "Epoch 438 Training Loss :0.13335703313350677 Test Loss:0.11896295845508575\n",
      "Epoch 439 Training Loss :0.13335402309894562 Test Loss:0.11896081268787384\n",
      "Epoch 440 Training Loss :0.13335101306438446 Test Loss:0.11895868182182312\n",
      "Epoch 441 Training Loss :0.1333480030298233 Test Loss:0.1189565435051918\n",
      "Epoch 442 Training Loss :0.13334499299526215 Test Loss:0.11895443499088287\n",
      "Epoch 443 Training Loss :0.13334199786186218 Test Loss:0.11895229667425156\n",
      "Epoch 444 Training Loss :0.13333900272846222 Test Loss:0.11895018070936203\n",
      "Epoch 445 Training Loss :0.13333600759506226 Test Loss:0.1189480647444725\n",
      "Epoch 446 Training Loss :0.1333330124616623 Test Loss:0.11894594877958298\n",
      "Epoch 447 Training Loss :0.13333003222942352 Test Loss:0.11894383281469345\n",
      "Epoch 448 Training Loss :0.13332703709602356 Test Loss:0.11894173175096512\n",
      "Epoch 449 Training Loss :0.1333240568637848 Test Loss:0.11893962323665619\n",
      "Epoch 450 Training Loss :0.13332107663154602 Test Loss:0.11893752217292786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 451 Training Loss :0.13331811130046844 Test Loss:0.11893542110919952\n",
      "Epoch 452 Training Loss :0.13331514596939087 Test Loss:0.11893332749605179\n",
      "Epoch 453 Training Loss :0.1333121657371521 Test Loss:0.11893124878406525\n",
      "Epoch 454 Training Loss :0.13330921530723572 Test Loss:0.11892915517091751\n",
      "Epoch 455 Training Loss :0.13330624997615814 Test Loss:0.11892706900835037\n",
      "Epoch 456 Training Loss :0.13330328464508057 Test Loss:0.11892499029636383\n",
      "Epoch 457 Training Loss :0.13330033421516418 Test Loss:0.11892291158437729\n",
      "Epoch 458 Training Loss :0.1332973837852478 Test Loss:0.11892082542181015\n",
      "Epoch 459 Training Loss :0.13329443335533142 Test Loss:0.1189187616109848\n",
      "Epoch 460 Training Loss :0.13329148292541504 Test Loss:0.11891668289899826\n",
      "Epoch 461 Training Loss :0.13328854739665985 Test Loss:0.11891461908817291\n",
      "Epoch 462 Training Loss :0.13328561186790466 Test Loss:0.11891256272792816\n",
      "Epoch 463 Training Loss :0.13328267633914948 Test Loss:0.11891049891710281\n",
      "Epoch 464 Training Loss :0.1332797408103943 Test Loss:0.11890843510627747\n",
      "Epoch 465 Training Loss :0.1332768201828003 Test Loss:0.11890638619661331\n",
      "Epoch 466 Training Loss :0.1332738846540451 Test Loss:0.11890433728694916\n",
      "Epoch 467 Training Loss :0.1332709640264511 Test Loss:0.118902288377285\n",
      "Epoch 468 Training Loss :0.13326804339885712 Test Loss:0.11890024691820145\n",
      "Epoch 469 Training Loss :0.13326513767242432 Test Loss:0.11889819800853729\n",
      "Epoch 470 Training Loss :0.13326221704483032 Test Loss:0.11889615654945374\n",
      "Epoch 471 Training Loss :0.13325931131839752 Test Loss:0.11889412999153137\n",
      "Epoch 472 Training Loss :0.13325639069080353 Test Loss:0.11889209598302841\n",
      "Epoch 473 Training Loss :0.13325348496437073 Test Loss:0.11889005452394485\n",
      "Epoch 474 Training Loss :0.13325059413909912 Test Loss:0.11888802796602249\n",
      "Epoch 475 Training Loss :0.13324768841266632 Test Loss:0.11888602375984192\n",
      "Epoch 476 Training Loss :0.13324478268623352 Test Loss:0.11888398975133896\n",
      "Epoch 477 Training Loss :0.13324189186096191 Test Loss:0.11888197809457779\n",
      "Epoch 478 Training Loss :0.1332390010356903 Test Loss:0.11887995898723602\n",
      "Epoch 479 Training Loss :0.1332361102104187 Test Loss:0.11887796968221664\n",
      "Epoch 480 Training Loss :0.1332332342863083 Test Loss:0.11887594312429428\n",
      "Epoch 481 Training Loss :0.13323034346103668 Test Loss:0.1188739463686943\n",
      "Epoch 482 Training Loss :0.13322746753692627 Test Loss:0.11887193471193314\n",
      "Epoch 483 Training Loss :0.13322459161281586 Test Loss:0.11886994540691376\n",
      "Epoch 484 Training Loss :0.13322171568870544 Test Loss:0.11886794865131378\n",
      "Epoch 485 Training Loss :0.13321885466575623 Test Loss:0.1188659518957138\n",
      "Epoch 486 Training Loss :0.1332159787416458 Test Loss:0.11886396259069443\n",
      "Epoch 487 Training Loss :0.1332131177186966 Test Loss:0.11886197328567505\n",
      "Epoch 488 Training Loss :0.13321025669574738 Test Loss:0.11885997653007507\n",
      "Epoch 489 Training Loss :0.13320739567279816 Test Loss:0.1188579872250557\n",
      "Epoch 490 Training Loss :0.13320453464984894 Test Loss:0.1188560202717781\n",
      "Epoch 491 Training Loss :0.1332016885280609 Test Loss:0.11885403096675873\n",
      "Epoch 492 Training Loss :0.1331988424062729 Test Loss:0.11885204911231995\n",
      "Epoch 493 Training Loss :0.13319599628448486 Test Loss:0.11885007470846176\n",
      "Epoch 494 Training Loss :0.13319315016269684 Test Loss:0.11884810775518417\n",
      "Epoch 495 Training Loss :0.1331903040409088 Test Loss:0.11884614080190659\n",
      "Epoch 496 Training Loss :0.1331874579191208 Test Loss:0.118844173848629\n",
      "Epoch 497 Training Loss :0.13318464159965515 Test Loss:0.11884220689535141\n",
      "Epoch 498 Training Loss :0.13318179547786713 Test Loss:0.11884024739265442\n",
      "Epoch 499 Training Loss :0.1331789642572403 Test Loss:0.11883829534053802\n",
      "Epoch 500 Training Loss :0.13317614793777466 Test Loss:0.11883634328842163\n",
      "Epoch 501 Training Loss :0.13317331671714783 Test Loss:0.11883438378572464\n",
      "Epoch 502 Training Loss :0.1331705003976822 Test Loss:0.11883243918418884\n",
      "Epoch 503 Training Loss :0.13316768407821655 Test Loss:0.11883049458265305\n",
      "Epoch 504 Training Loss :0.13316486775875092 Test Loss:0.11882854998111725\n",
      "Epoch 505 Training Loss :0.13316205143928528 Test Loss:0.11882661283016205\n",
      "Epoch 506 Training Loss :0.13315925002098083 Test Loss:0.11882467567920685\n",
      "Epoch 507 Training Loss :0.1331564337015152 Test Loss:0.11882274597883224\n",
      "Epoch 508 Training Loss :0.13315363228321075 Test Loss:0.11882080882787704\n",
      "Epoch 509 Training Loss :0.1331508308649063 Test Loss:0.11881888657808304\n",
      "Epoch 510 Training Loss :0.13314802944660187 Test Loss:0.11881696432828903\n",
      "Epoch 511 Training Loss :0.13314524292945862 Test Loss:0.11881502717733383\n",
      "Epoch 512 Training Loss :0.13314244151115417 Test Loss:0.11881311982870102\n",
      "Epoch 513 Training Loss :0.13313965499401093 Test Loss:0.11881120502948761\n",
      "Epoch 514 Training Loss :0.13313686847686768 Test Loss:0.1188092902302742\n",
      "Epoch 515 Training Loss :0.13313409686088562 Test Loss:0.11880737543106079\n",
      "Epoch 516 Training Loss :0.13313131034374237 Test Loss:0.11880545318126678\n",
      "Epoch 517 Training Loss :0.13312852382659912 Test Loss:0.11880355328321457\n",
      "Epoch 518 Training Loss :0.13312575221061707 Test Loss:0.11880166828632355\n",
      "Epoch 519 Training Loss :0.133122980594635 Test Loss:0.11879976093769073\n",
      "Epoch 520 Training Loss :0.13312022387981415 Test Loss:0.11879786103963852\n",
      "Epoch 521 Training Loss :0.1331174522638321 Test Loss:0.1187959685921669\n",
      "Epoch 522 Training Loss :0.13311468064785004 Test Loss:0.11879408359527588\n",
      "Epoch 523 Training Loss :0.13311192393302917 Test Loss:0.11879219114780426\n",
      "Epoch 524 Training Loss :0.1331091672182083 Test Loss:0.11879030615091324\n",
      "Epoch 525 Training Loss :0.13310641050338745 Test Loss:0.11878842115402222\n",
      "Epoch 526 Training Loss :0.1331036537885666 Test Loss:0.11878654360771179\n",
      "Epoch 527 Training Loss :0.13310091197490692 Test Loss:0.11878467351198196\n",
      "Epoch 528 Training Loss :0.13309817016124725 Test Loss:0.11878280341625214\n",
      "Epoch 529 Training Loss :0.13309542834758759 Test Loss:0.11878091841936111\n",
      "Epoch 530 Training Loss :0.13309268653392792 Test Loss:0.11877905577421188\n",
      "Epoch 531 Training Loss :0.13308994472026825 Test Loss:0.11877718567848206\n",
      "Epoch 532 Training Loss :0.13308721780776978 Test Loss:0.11877531558275223\n",
      "Epoch 533 Training Loss :0.1330844759941101 Test Loss:0.1187734603881836\n",
      "Epoch 534 Training Loss :0.13308174908161163 Test Loss:0.11877161264419556\n",
      "Epoch 535 Training Loss :0.13307902216911316 Test Loss:0.11876974999904633\n",
      "Epoch 536 Training Loss :0.13307629525661469 Test Loss:0.1187678873538971\n",
      "Epoch 537 Training Loss :0.1330735832452774 Test Loss:0.11876605451107025\n",
      "Epoch 538 Training Loss :0.13307087123394012 Test Loss:0.11876420676708221\n",
      "Epoch 539 Training Loss :0.13306815922260284 Test Loss:0.11876234412193298\n",
      "Epoch 540 Training Loss :0.13306544721126556 Test Loss:0.11876051872968674\n",
      "Epoch 541 Training Loss :0.13306273519992828 Test Loss:0.1187586635351181\n",
      "Epoch 542 Training Loss :0.133060023188591 Test Loss:0.11875682324171066\n",
      "Epoch 543 Training Loss :0.13305732607841492 Test Loss:0.11875499784946442\n",
      "Epoch 544 Training Loss :0.13305461406707764 Test Loss:0.11875317245721817\n",
      "Epoch 545 Training Loss :0.13305191695690155 Test Loss:0.11875133216381073\n",
      "Epoch 546 Training Loss :0.13304921984672546 Test Loss:0.11874951422214508\n",
      "Epoch 547 Training Loss :0.13304652273654938 Test Loss:0.11874768137931824\n",
      "Epoch 548 Training Loss :0.13304384052753448 Test Loss:0.11874585598707199\n",
      "Epoch 549 Training Loss :0.1330411583185196 Test Loss:0.11874403059482574\n",
      "Epoch 550 Training Loss :0.1330384761095047 Test Loss:0.11874222010374069\n",
      "Epoch 551 Training Loss :0.1330357939004898 Test Loss:0.11874039471149445\n",
      "Epoch 552 Training Loss :0.13303311169147491 Test Loss:0.11873859167098999\n",
      "Epoch 553 Training Loss :0.13303042948246002 Test Loss:0.11873678117990494\n",
      "Epoch 554 Training Loss :0.13302776217460632 Test Loss:0.11873497068881989\n",
      "Epoch 555 Training Loss :0.13302509486675262 Test Loss:0.11873316764831543\n",
      "Epoch 556 Training Loss :0.13302242755889893 Test Loss:0.11873137950897217\n",
      "Epoch 557 Training Loss :0.13301976025104523 Test Loss:0.11872957646846771\n",
      "Epoch 558 Training Loss :0.13301709294319153 Test Loss:0.11872778087854385\n",
      "Epoch 559 Training Loss :0.13301444053649902 Test Loss:0.11872599273920059\n",
      "Epoch 560 Training Loss :0.13301178812980652 Test Loss:0.11872418969869614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 561 Training Loss :0.13300912082195282 Test Loss:0.11872240900993347\n",
      "Epoch 562 Training Loss :0.13300646841526031 Test Loss:0.11872061342000961\n",
      "Epoch 563 Training Loss :0.133003830909729 Test Loss:0.11871882528066635\n",
      "Epoch 564 Training Loss :0.1330011934041977 Test Loss:0.11871705204248428\n",
      "Epoch 565 Training Loss :0.1329985409975052 Test Loss:0.11871527135372162\n",
      "Epoch 566 Training Loss :0.13299590349197388 Test Loss:0.11871349811553955\n",
      "Epoch 567 Training Loss :0.13299326598644257 Test Loss:0.11871171742677689\n",
      "Epoch 568 Training Loss :0.13299062848091125 Test Loss:0.11870995908975601\n",
      "Epoch 569 Training Loss :0.13298799097537994 Test Loss:0.11870818585157394\n",
      "Epoch 570 Training Loss :0.13298535346984863 Test Loss:0.11870642751455307\n",
      "Epoch 571 Training Loss :0.13298273086547852 Test Loss:0.1187046617269516\n",
      "Epoch 572 Training Loss :0.1329801082611084 Test Loss:0.11870290338993073\n",
      "Epoch 573 Training Loss :0.13297748565673828 Test Loss:0.11870113015174866\n",
      "Epoch 574 Training Loss :0.13297486305236816 Test Loss:0.11869938671588898\n",
      "Epoch 575 Training Loss :0.13297225534915924 Test Loss:0.1186976358294487\n",
      "Epoch 576 Training Loss :0.13296963274478912 Test Loss:0.11869587749242783\n",
      "Epoch 577 Training Loss :0.1329670250415802 Test Loss:0.11869412660598755\n",
      "Epoch 578 Training Loss :0.13296441733837128 Test Loss:0.11869239062070847\n",
      "Epoch 579 Training Loss :0.13296182453632355 Test Loss:0.11869065463542938\n",
      "Epoch 580 Training Loss :0.13295921683311462 Test Loss:0.1186889111995697\n",
      "Epoch 581 Training Loss :0.1329566240310669 Test Loss:0.11868718266487122\n",
      "Epoch 582 Training Loss :0.13295401632785797 Test Loss:0.11868543177843094\n",
      "Epoch 583 Training Loss :0.13295142352581024 Test Loss:0.11868371069431305\n",
      "Epoch 584 Training Loss :0.1329488307237625 Test Loss:0.11868198961019516\n",
      "Epoch 585 Training Loss :0.13294623792171478 Test Loss:0.11868025362491608\n",
      "Epoch 586 Training Loss :0.13294364511966705 Test Loss:0.11867853999137878\n",
      "Epoch 587 Training Loss :0.13294106721878052 Test Loss:0.1186768114566803\n",
      "Epoch 588 Training Loss :0.1329384744167328 Test Loss:0.118675097823143\n",
      "Epoch 589 Training Loss :0.13293589651584625 Test Loss:0.11867337673902512\n",
      "Epoch 590 Training Loss :0.13293331861495972 Test Loss:0.11867165565490723\n",
      "Epoch 591 Training Loss :0.13293075561523438 Test Loss:0.11866993457078934\n",
      "Epoch 592 Training Loss :0.13292817771434784 Test Loss:0.11866822093725204\n",
      "Epoch 593 Training Loss :0.1329256147146225 Test Loss:0.11866652965545654\n",
      "Epoch 594 Training Loss :0.13292303681373596 Test Loss:0.11866482347249985\n",
      "Epoch 595 Training Loss :0.13292047381401062 Test Loss:0.11866311728954315\n",
      "Epoch 596 Training Loss :0.13291791081428528 Test Loss:0.11866141855716705\n",
      "Epoch 597 Training Loss :0.13291534781455994 Test Loss:0.11865970492362976\n",
      "Epoch 598 Training Loss :0.1329127997159958 Test Loss:0.11865802109241486\n",
      "Epoch 599 Training Loss :0.13291023671627045 Test Loss:0.11865631490945816\n",
      "Epoch 600 Training Loss :0.1329076886177063 Test Loss:0.11865461617708206\n",
      "Epoch 601 Training Loss :0.13290514051914215 Test Loss:0.11865292489528656\n",
      "Epoch 602 Training Loss :0.132902592420578 Test Loss:0.11865124106407166\n",
      "Epoch 603 Training Loss :0.13290005922317505 Test Loss:0.11864955723285675\n",
      "Epoch 604 Training Loss :0.1328975111246109 Test Loss:0.11864786595106125\n",
      "Epoch 605 Training Loss :0.13289497792720795 Test Loss:0.11864617466926575\n",
      "Epoch 606 Training Loss :0.1328924298286438 Test Loss:0.11864450573921204\n",
      "Epoch 607 Training Loss :0.13288989663124084 Test Loss:0.11864282190799713\n",
      "Epoch 608 Training Loss :0.1328873634338379 Test Loss:0.11864115297794342\n",
      "Epoch 609 Training Loss :0.13288484513759613 Test Loss:0.1186394914984703\n",
      "Epoch 610 Training Loss :0.13288231194019318 Test Loss:0.1186378225684166\n",
      "Epoch 611 Training Loss :0.13287979364395142 Test Loss:0.11863613873720169\n",
      "Epoch 612 Training Loss :0.13287727534770966 Test Loss:0.11863448470830917\n",
      "Epoch 613 Training Loss :0.1328747570514679 Test Loss:0.11863282322883606\n",
      "Epoch 614 Training Loss :0.13287223875522614 Test Loss:0.11863116174936295\n",
      "Epoch 615 Training Loss :0.13286972045898438 Test Loss:0.11862949281930923\n",
      "Epoch 616 Training Loss :0.1328672170639038 Test Loss:0.11862783879041672\n",
      "Epoch 617 Training Loss :0.13286471366882324 Test Loss:0.11862620711326599\n",
      "Epoch 618 Training Loss :0.13286219537258148 Test Loss:0.11862456053495407\n",
      "Epoch 619 Training Loss :0.13285969197750092 Test Loss:0.11862290650606155\n",
      "Epoch 620 Training Loss :0.13285720348358154 Test Loss:0.11862125247716904\n",
      "Epoch 621 Training Loss :0.13285470008850098 Test Loss:0.11861962080001831\n",
      "Epoch 622 Training Loss :0.1328522115945816 Test Loss:0.11861798912286758\n",
      "Epoch 623 Training Loss :0.13284970819950104 Test Loss:0.11861633509397507\n",
      "Epoch 624 Training Loss :0.13284721970558167 Test Loss:0.11861470341682434\n",
      "Epoch 625 Training Loss :0.1328447312116623 Test Loss:0.11861306428909302\n",
      "Epoch 626 Training Loss :0.1328422576189041 Test Loss:0.11861144006252289\n",
      "Epoch 627 Training Loss :0.13283976912498474 Test Loss:0.11860980838537216\n",
      "Epoch 628 Training Loss :0.13283729553222656 Test Loss:0.11860819160938263\n",
      "Epoch 629 Training Loss :0.1328348070383072 Test Loss:0.1186065673828125\n",
      "Epoch 630 Training Loss :0.132832333445549 Test Loss:0.11860493570566177\n",
      "Epoch 631 Training Loss :0.13282985985279083 Test Loss:0.11860332638025284\n",
      "Epoch 632 Training Loss :0.13282738626003265 Test Loss:0.1186017170548439\n",
      "Epoch 633 Training Loss :0.13282491266727448 Test Loss:0.11860010027885437\n",
      "Epoch 634 Training Loss :0.1328224539756775 Test Loss:0.11859848350286484\n",
      "Epoch 635 Training Loss :0.1328199952840805 Test Loss:0.1185968816280365\n",
      "Epoch 636 Training Loss :0.13281753659248352 Test Loss:0.11859527975320816\n",
      "Epoch 637 Training Loss :0.13281506299972534 Test Loss:0.11859367042779922\n",
      "Epoch 638 Training Loss :0.13281260430812836 Test Loss:0.11859206855297089\n",
      "Epoch 639 Training Loss :0.13281016051769257 Test Loss:0.11859047412872314\n",
      "Epoch 640 Training Loss :0.13280770182609558 Test Loss:0.1185888722538948\n",
      "Epoch 641 Training Loss :0.1328052580356598 Test Loss:0.11858727782964706\n",
      "Epoch 642 Training Loss :0.132802814245224 Test Loss:0.11858567595481873\n",
      "Epoch 643 Training Loss :0.1328003704547882 Test Loss:0.11858410388231277\n",
      "Epoch 644 Training Loss :0.13279792666435242 Test Loss:0.11858251690864563\n",
      "Epoch 645 Training Loss :0.13279548287391663 Test Loss:0.11858092248439789\n",
      "Epoch 646 Training Loss :0.13279303908348083 Test Loss:0.11857934296131134\n",
      "Epoch 647 Training Loss :0.13279061019420624 Test Loss:0.11857776343822479\n",
      "Epoch 648 Training Loss :0.13278818130493164 Test Loss:0.11857618391513824\n",
      "Epoch 649 Training Loss :0.13278575241565704 Test Loss:0.1185746043920517\n",
      "Epoch 650 Training Loss :0.13278332352638245 Test Loss:0.11857303231954575\n",
      "Epoch 651 Training Loss :0.13278089463710785 Test Loss:0.11857146769762039\n",
      "Epoch 652 Training Loss :0.13277848064899445 Test Loss:0.11856990307569504\n",
      "Epoch 653 Training Loss :0.13277605175971985 Test Loss:0.11856833100318909\n",
      "Epoch 654 Training Loss :0.13277363777160645 Test Loss:0.11856677383184433\n",
      "Epoch 655 Training Loss :0.13277122378349304 Test Loss:0.11856520175933838\n",
      "Epoch 656 Training Loss :0.13276880979537964 Test Loss:0.11856364458799362\n",
      "Epoch 657 Training Loss :0.13276639580726624 Test Loss:0.11856210231781006\n",
      "Epoch 658 Training Loss :0.13276399672031403 Test Loss:0.1185605525970459\n",
      "Epoch 659 Training Loss :0.13276158273220062 Test Loss:0.11855899542570114\n",
      "Epoch 660 Training Loss :0.1327591836452484 Test Loss:0.11855743825435638\n",
      "Epoch 661 Training Loss :0.1327567845582962 Test Loss:0.11855591088533401\n",
      "Epoch 662 Training Loss :0.132754385471344 Test Loss:0.11855434626340866\n",
      "Epoch 663 Training Loss :0.13275198638439178 Test Loss:0.1185527965426445\n",
      "Epoch 664 Training Loss :0.13274958729743958 Test Loss:0.11855126917362213\n",
      "Epoch 665 Training Loss :0.13274720311164856 Test Loss:0.11854971945285797\n",
      "Epoch 666 Training Loss :0.13274480402469635 Test Loss:0.1185481995344162\n",
      "Epoch 667 Training Loss :0.13274241983890533 Test Loss:0.11854666471481323\n",
      "Epoch 668 Training Loss :0.13274003565311432 Test Loss:0.11854512244462967\n",
      "Epoch 669 Training Loss :0.1327376514673233 Test Loss:0.1185435876250267\n",
      "Epoch 670 Training Loss :0.1327352672815323 Test Loss:0.11854206770658493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 671 Training Loss :0.13273289799690247 Test Loss:0.11854054778814316\n",
      "Epoch 672 Training Loss :0.13273051381111145 Test Loss:0.11853902041912079\n",
      "Epoch 673 Training Loss :0.13272814452648163 Test Loss:0.11853750795125961\n",
      "Epoch 674 Training Loss :0.1327257752418518 Test Loss:0.11853598058223724\n",
      "Epoch 675 Training Loss :0.13272340595722198 Test Loss:0.11853446811437607\n",
      "Epoch 676 Training Loss :0.13272103667259216 Test Loss:0.11853295564651489\n",
      "Epoch 677 Training Loss :0.13271866738796234 Test Loss:0.11853145062923431\n",
      "Epoch 678 Training Loss :0.1327163130044937 Test Loss:0.11852995306253433\n",
      "Epoch 679 Training Loss :0.13271395862102509 Test Loss:0.11852844059467316\n",
      "Epoch 680 Training Loss :0.13271158933639526 Test Loss:0.11852693557739258\n",
      "Epoch 681 Training Loss :0.13270923495292664 Test Loss:0.118525430560112\n",
      "Epoch 682 Training Loss :0.1327068954706192 Test Loss:0.11852394044399261\n",
      "Epoch 683 Training Loss :0.13270454108715057 Test Loss:0.11852243542671204\n",
      "Epoch 684 Training Loss :0.13270218670368195 Test Loss:0.11852093786001205\n",
      "Epoch 685 Training Loss :0.1326998472213745 Test Loss:0.11851944774389267\n",
      "Epoch 686 Training Loss :0.13269750773906708 Test Loss:0.11851795017719269\n",
      "Epoch 687 Training Loss :0.13269515335559845 Test Loss:0.1185164675116539\n",
      "Epoch 688 Training Loss :0.13269281387329102 Test Loss:0.11851497739553452\n",
      "Epoch 689 Training Loss :0.13269048929214478 Test Loss:0.11851349472999573\n",
      "Epoch 690 Training Loss :0.13268814980983734 Test Loss:0.11851201951503754\n",
      "Epoch 691 Training Loss :0.1326858252286911 Test Loss:0.11851054430007935\n",
      "Epoch 692 Training Loss :0.13268348574638367 Test Loss:0.11850906163454056\n",
      "Epoch 693 Training Loss :0.13268116116523743 Test Loss:0.11850758641958237\n",
      "Epoch 694 Training Loss :0.1326788365840912 Test Loss:0.11850611120462418\n",
      "Epoch 695 Training Loss :0.13267651200294495 Test Loss:0.11850464344024658\n",
      "Epoch 696 Training Loss :0.1326741874217987 Test Loss:0.11850316822528839\n",
      "Epoch 697 Training Loss :0.13267187774181366 Test Loss:0.1185017004609108\n",
      "Epoch 698 Training Loss :0.13266955316066742 Test Loss:0.1185002326965332\n",
      "Epoch 699 Training Loss :0.13266724348068237 Test Loss:0.1184987723827362\n",
      "Epoch 700 Training Loss :0.13266491889953613 Test Loss:0.1184973195195198\n",
      "Epoch 701 Training Loss :0.13266262412071228 Test Loss:0.11849585175514221\n",
      "Epoch 702 Training Loss :0.13266031444072723 Test Loss:0.118494413793087\n",
      "Epoch 703 Training Loss :0.1326580047607422 Test Loss:0.11849295347929001\n",
      "Epoch 704 Training Loss :0.13265570998191833 Test Loss:0.11849150061607361\n",
      "Epoch 705 Training Loss :0.1326534003019333 Test Loss:0.11849004775285721\n",
      "Epoch 706 Training Loss :0.13265112042427063 Test Loss:0.11848858743906021\n",
      "Epoch 707 Training Loss :0.13264881074428558 Test Loss:0.1184871643781662\n",
      "Epoch 708 Training Loss :0.13264651596546173 Test Loss:0.1184857115149498\n",
      "Epoch 709 Training Loss :0.13264423608779907 Test Loss:0.11848427355289459\n",
      "Epoch 710 Training Loss :0.13264194130897522 Test Loss:0.11848283559083939\n",
      "Epoch 711 Training Loss :0.13263966143131256 Test Loss:0.11848139762878418\n",
      "Epoch 712 Training Loss :0.1326373666524887 Test Loss:0.11847995966672897\n",
      "Epoch 713 Training Loss :0.13263508677482605 Test Loss:0.11847852915525436\n",
      "Epoch 714 Training Loss :0.1326328068971634 Test Loss:0.11847709119319916\n",
      "Epoch 715 Training Loss :0.13263052701950073 Test Loss:0.11847567558288574\n",
      "Epoch 716 Training Loss :0.13262826204299927 Test Loss:0.11847422271966934\n",
      "Epoch 717 Training Loss :0.1326259821653366 Test Loss:0.11847281455993652\n",
      "Epoch 718 Training Loss :0.13262371718883514 Test Loss:0.11847138404846191\n",
      "Epoch 719 Training Loss :0.13262145221233368 Test Loss:0.11846998333930969\n",
      "Epoch 720 Training Loss :0.13261918723583221 Test Loss:0.11846855282783508\n",
      "Epoch 721 Training Loss :0.13261692225933075 Test Loss:0.11846714466810226\n",
      "Epoch 722 Training Loss :0.1326146423816681 Test Loss:0.11846572905778885\n",
      "Epoch 723 Training Loss :0.13261239230632782 Test Loss:0.11846431344747543\n",
      "Epoch 724 Training Loss :0.13261014223098755 Test Loss:0.11846290528774261\n",
      "Epoch 725 Training Loss :0.13260789215564728 Test Loss:0.1184615045785904\n",
      "Epoch 726 Training Loss :0.132605642080307 Test Loss:0.11846008151769638\n",
      "Epoch 727 Training Loss :0.13260337710380554 Test Loss:0.11845868080854416\n",
      "Epoch 728 Training Loss :0.13260112702846527 Test Loss:0.11845728754997253\n",
      "Epoch 729 Training Loss :0.1325988918542862 Test Loss:0.11845587939023972\n",
      "Epoch 730 Training Loss :0.13259664177894592 Test Loss:0.11845448613166809\n",
      "Epoch 731 Training Loss :0.13259440660476685 Test Loss:0.11845308542251587\n",
      "Epoch 732 Training Loss :0.13259217143058777 Test Loss:0.11845168471336365\n",
      "Epoch 733 Training Loss :0.1325899213552475 Test Loss:0.11845029145479202\n",
      "Epoch 734 Training Loss :0.13258768618106842 Test Loss:0.1184488981962204\n",
      "Epoch 735 Training Loss :0.13258545100688934 Test Loss:0.11844751983880997\n",
      "Epoch 736 Training Loss :0.13258323073387146 Test Loss:0.11844611912965775\n",
      "Epoch 737 Training Loss :0.13258099555969238 Test Loss:0.11844474077224731\n",
      "Epoch 738 Training Loss :0.1325787752866745 Test Loss:0.11844334006309509\n",
      "Epoch 739 Training Loss :0.13257654011249542 Test Loss:0.11844196170568466\n",
      "Epoch 740 Training Loss :0.13257433474063873 Test Loss:0.11844058334827423\n",
      "Epoch 741 Training Loss :0.13257209956645966 Test Loss:0.1184392124414444\n",
      "Epoch 742 Training Loss :0.13256989419460297 Test Loss:0.11843783408403397\n",
      "Epoch 743 Training Loss :0.13256767392158508 Test Loss:0.11843646317720413\n",
      "Epoch 744 Training Loss :0.1325654536485672 Test Loss:0.1184350922703743\n",
      "Epoch 745 Training Loss :0.1325632482767105 Test Loss:0.11843372881412506\n",
      "Epoch 746 Training Loss :0.13256104290485382 Test Loss:0.11843235045671463\n",
      "Epoch 747 Training Loss :0.13255882263183594 Test Loss:0.11843099445104599\n",
      "Epoch 748 Training Loss :0.13255663216114044 Test Loss:0.11842961609363556\n",
      "Epoch 749 Training Loss :0.13255442678928375 Test Loss:0.11842827498912811\n",
      "Epoch 750 Training Loss :0.13255222141742706 Test Loss:0.11842690408229828\n",
      "Epoch 751 Training Loss :0.13255001604557037 Test Loss:0.11842555552721024\n",
      "Epoch 752 Training Loss :0.13254782557487488 Test Loss:0.1184241995215416\n",
      "Epoch 753 Training Loss :0.13254563510417938 Test Loss:0.11842284351587296\n",
      "Epoch 754 Training Loss :0.1325434446334839 Test Loss:0.11842148751020432\n",
      "Epoch 755 Training Loss :0.1325412541627884 Test Loss:0.11842013895511627\n",
      "Epoch 756 Training Loss :0.1325390636920929 Test Loss:0.11841879785060883\n",
      "Epoch 757 Training Loss :0.1325368732213974 Test Loss:0.11841745674610138\n",
      "Epoch 758 Training Loss :0.1325346976518631 Test Loss:0.11841610819101334\n",
      "Epoch 759 Training Loss :0.1325325071811676 Test Loss:0.11841475963592529\n",
      "Epoch 760 Training Loss :0.1325303316116333 Test Loss:0.11841343343257904\n",
      "Epoch 761 Training Loss :0.132528156042099 Test Loss:0.1184120923280716\n",
      "Epoch 762 Training Loss :0.1325259804725647 Test Loss:0.11841075867414474\n",
      "Epoch 763 Training Loss :0.1325238049030304 Test Loss:0.11840943247079849\n",
      "Epoch 764 Training Loss :0.1325216293334961 Test Loss:0.11840810626745224\n",
      "Epoch 765 Training Loss :0.1325194537639618 Test Loss:0.11840678751468658\n",
      "Epoch 766 Training Loss :0.13251729309558868 Test Loss:0.11840544641017914\n",
      "Epoch 767 Training Loss :0.13251513242721558 Test Loss:0.11840412020683289\n",
      "Epoch 768 Training Loss :0.13251297175884247 Test Loss:0.11840280145406723\n",
      "Epoch 769 Training Loss :0.13251081109046936 Test Loss:0.11840147525072098\n",
      "Epoch 770 Training Loss :0.13250865042209625 Test Loss:0.11840016394853592\n",
      "Epoch 771 Training Loss :0.13250648975372314 Test Loss:0.11839883774518967\n",
      "Epoch 772 Training Loss :0.13250432908535004 Test Loss:0.1183975338935852\n",
      "Epoch 773 Training Loss :0.13250218331813812 Test Loss:0.11839621514081955\n",
      "Epoch 774 Training Loss :0.13250002264976501 Test Loss:0.11839490383863449\n",
      "Epoch 775 Training Loss :0.1324978768825531 Test Loss:0.11839360743761063\n",
      "Epoch 776 Training Loss :0.1324957311153412 Test Loss:0.11839228868484497\n",
      "Epoch 777 Training Loss :0.13249358534812927 Test Loss:0.11839097738265991\n",
      "Epoch 778 Training Loss :0.13249143958091736 Test Loss:0.11838968098163605\n",
      "Epoch 779 Training Loss :0.13248930871486664 Test Loss:0.11838838458061218\n",
      "Epoch 780 Training Loss :0.13248716294765472 Test Loss:0.11838708072900772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 781 Training Loss :0.1324850171804428 Test Loss:0.11838578432798386\n",
      "Epoch 782 Training Loss :0.1324828863143921 Test Loss:0.1183844804763794\n",
      "Epoch 783 Training Loss :0.13248075544834137 Test Loss:0.11838319152593613\n",
      "Epoch 784 Training Loss :0.13247862458229065 Test Loss:0.11838189512491226\n",
      "Epoch 785 Training Loss :0.13247649371623993 Test Loss:0.11838061362504959\n",
      "Epoch 786 Training Loss :0.1324743628501892 Test Loss:0.11837931722402573\n",
      "Epoch 787 Training Loss :0.1324722319841385 Test Loss:0.11837803572416306\n",
      "Epoch 788 Training Loss :0.13247011601924896 Test Loss:0.11837674677371979\n",
      "Epoch 789 Training Loss :0.13246800005435944 Test Loss:0.11837547272443771\n",
      "Epoch 790 Training Loss :0.13246586918830872 Test Loss:0.11837418377399445\n",
      "Epoch 791 Training Loss :0.1324637532234192 Test Loss:0.11837290227413177\n",
      "Epoch 792 Training Loss :0.13246163725852966 Test Loss:0.1183716207742691\n",
      "Epoch 793 Training Loss :0.13245953619480133 Test Loss:0.11837035417556763\n",
      "Epoch 794 Training Loss :0.1324574202299118 Test Loss:0.11836908012628555\n",
      "Epoch 795 Training Loss :0.13245530426502228 Test Loss:0.11836780607700348\n",
      "Epoch 796 Training Loss :0.13245320320129395 Test Loss:0.1183665469288826\n",
      "Epoch 797 Training Loss :0.13245108723640442 Test Loss:0.11836527287960052\n",
      "Epoch 798 Training Loss :0.1324489861726761 Test Loss:0.11836401373147964\n",
      "Epoch 799 Training Loss :0.13244688510894775 Test Loss:0.11836274713277817\n",
      "Epoch 800 Training Loss :0.13244478404521942 Test Loss:0.11836148798465729\n",
      "Epoch 801 Training Loss :0.1324426829814911 Test Loss:0.11836022883653641\n",
      "Epoch 802 Training Loss :0.13244059681892395 Test Loss:0.11835896968841553\n",
      "Epoch 803 Training Loss :0.13243849575519562 Test Loss:0.11835770308971405\n",
      "Epoch 804 Training Loss :0.13243640959262848 Test Loss:0.11835645139217377\n",
      "Epoch 805 Training Loss :0.13243430852890015 Test Loss:0.11835520714521408\n",
      "Epoch 806 Training Loss :0.132432222366333 Test Loss:0.1183539479970932\n",
      "Epoch 807 Training Loss :0.13243013620376587 Test Loss:0.11835269629955292\n",
      "Epoch 808 Training Loss :0.13242805004119873 Test Loss:0.11835145950317383\n",
      "Epoch 809 Training Loss :0.13242597877979279 Test Loss:0.11835020035505295\n",
      "Epoch 810 Training Loss :0.13242389261722565 Test Loss:0.11834894865751266\n",
      "Epoch 811 Training Loss :0.1324218064546585 Test Loss:0.11834770441055298\n",
      "Epoch 812 Training Loss :0.13241973519325256 Test Loss:0.1183464527130127\n",
      "Epoch 813 Training Loss :0.13241766393184662 Test Loss:0.1183452159166336\n",
      "Epoch 814 Training Loss :0.13241557776927948 Test Loss:0.11834397912025452\n",
      "Epoch 815 Training Loss :0.13241350650787354 Test Loss:0.11834274977445602\n",
      "Epoch 816 Training Loss :0.1324114352464676 Test Loss:0.11834151297807693\n",
      "Epoch 817 Training Loss :0.13240937888622284 Test Loss:0.11834027618169785\n",
      "Epoch 818 Training Loss :0.1324073076248169 Test Loss:0.11833904683589935\n",
      "Epoch 819 Training Loss :0.13240523636341095 Test Loss:0.11833781749010086\n",
      "Epoch 820 Training Loss :0.1324031800031662 Test Loss:0.11833658814430237\n",
      "Epoch 821 Training Loss :0.13240112364292145 Test Loss:0.11833536624908447\n",
      "Epoch 822 Training Loss :0.1323990523815155 Test Loss:0.11833412200212479\n",
      "Epoch 823 Training Loss :0.13239699602127075 Test Loss:0.11833291500806808\n",
      "Epoch 824 Training Loss :0.132394939661026 Test Loss:0.11833169311285019\n",
      "Epoch 825 Training Loss :0.13239288330078125 Test Loss:0.11833047866821289\n",
      "Epoch 826 Training Loss :0.1323908418416977 Test Loss:0.11832926422357559\n",
      "Epoch 827 Training Loss :0.13238878548145294 Test Loss:0.1183280423283577\n",
      "Epoch 828 Training Loss :0.13238674402236938 Test Loss:0.118326835334301\n",
      "Epoch 829 Training Loss :0.13238468766212463 Test Loss:0.1183256283402443\n",
      "Epoch 830 Training Loss :0.13238264620304108 Test Loss:0.1183244064450264\n",
      "Epoch 831 Training Loss :0.13238060474395752 Test Loss:0.11832322180271149\n",
      "Epoch 832 Training Loss :0.13237856328487396 Test Loss:0.11832202225923538\n",
      "Epoch 833 Training Loss :0.1323765218257904 Test Loss:0.11832082271575928\n",
      "Epoch 834 Training Loss :0.13237448036670685 Test Loss:0.11831962317228317\n",
      "Epoch 835 Training Loss :0.1323724389076233 Test Loss:0.11831840872764587\n",
      "Epoch 836 Training Loss :0.13237041234970093 Test Loss:0.11831722408533096\n",
      "Epoch 837 Training Loss :0.13236838579177856 Test Loss:0.11831603199243546\n",
      "Epoch 838 Training Loss :0.1323663592338562 Test Loss:0.11831483244895935\n",
      "Epoch 839 Training Loss :0.13236431777477264 Test Loss:0.11831364035606384\n",
      "Epoch 840 Training Loss :0.13236229121685028 Test Loss:0.11831244826316833\n",
      "Epoch 841 Training Loss :0.13236026465892792 Test Loss:0.11831125617027283\n",
      "Epoch 842 Training Loss :0.13235823810100555 Test Loss:0.11831006407737732\n",
      "Epoch 843 Training Loss :0.13235622644424438 Test Loss:0.11830887943506241\n",
      "Epoch 844 Training Loss :0.13235419988632202 Test Loss:0.1183076873421669\n",
      "Epoch 845 Training Loss :0.13235218822956085 Test Loss:0.11830651760101318\n",
      "Epoch 846 Training Loss :0.13235017657279968 Test Loss:0.11830533295869827\n",
      "Epoch 847 Training Loss :0.13234815001487732 Test Loss:0.11830414831638336\n",
      "Epoch 848 Training Loss :0.13234613835811615 Test Loss:0.11830297857522964\n",
      "Epoch 849 Training Loss :0.13234414160251617 Test Loss:0.11830180138349533\n",
      "Epoch 850 Training Loss :0.132342129945755 Test Loss:0.11830063164234161\n",
      "Epoch 851 Training Loss :0.13234011828899384 Test Loss:0.1182994544506073\n",
      "Epoch 852 Training Loss :0.13233812153339386 Test Loss:0.11829827725887299\n",
      "Epoch 853 Training Loss :0.1323361098766327 Test Loss:0.11829711496829987\n",
      "Epoch 854 Training Loss :0.13233411312103271 Test Loss:0.11829593777656555\n",
      "Epoch 855 Training Loss :0.13233211636543274 Test Loss:0.11829478293657303\n",
      "Epoch 856 Training Loss :0.13233011960983276 Test Loss:0.11829362064599991\n",
      "Epoch 857 Training Loss :0.1323281228542328 Test Loss:0.11829245090484619\n",
      "Epoch 858 Training Loss :0.1323261260986328 Test Loss:0.11829130351543427\n",
      "Epoch 859 Training Loss :0.13232414424419403 Test Loss:0.11829013377428055\n",
      "Epoch 860 Training Loss :0.13232214748859406 Test Loss:0.11828897893428802\n",
      "Epoch 861 Training Loss :0.13232016563415527 Test Loss:0.1182878315448761\n",
      "Epoch 862 Training Loss :0.1323181837797165 Test Loss:0.11828668415546417\n",
      "Epoch 863 Training Loss :0.1323162019252777 Test Loss:0.11828552931547165\n",
      "Epoch 864 Training Loss :0.13231422007083893 Test Loss:0.11828438192605972\n",
      "Epoch 865 Training Loss :0.13231223821640015 Test Loss:0.1182832270860672\n",
      "Epoch 866 Training Loss :0.13231025636196136 Test Loss:0.11828208714723587\n",
      "Epoch 867 Training Loss :0.13230828940868378 Test Loss:0.11828093975782394\n",
      "Epoch 868 Training Loss :0.132306307554245 Test Loss:0.11827980726957321\n",
      "Epoch 869 Training Loss :0.1323043406009674 Test Loss:0.11827866733074188\n",
      "Epoch 870 Training Loss :0.13230237364768982 Test Loss:0.11827751994132996\n",
      "Epoch 871 Training Loss :0.13230040669441223 Test Loss:0.11827638745307922\n",
      "Epoch 872 Training Loss :0.13229843974113464 Test Loss:0.1182752475142479\n",
      "Epoch 873 Training Loss :0.13229647278785706 Test Loss:0.11827412247657776\n",
      "Epoch 874 Training Loss :0.13229452073574066 Test Loss:0.11827297508716583\n",
      "Epoch 875 Training Loss :0.13229255378246307 Test Loss:0.1182718500494957\n",
      "Epoch 876 Training Loss :0.13229060173034668 Test Loss:0.11827071011066437\n",
      "Epoch 877 Training Loss :0.1322886347770691 Test Loss:0.11826959252357483\n",
      "Epoch 878 Training Loss :0.1322866827249527 Test Loss:0.11826847493648529\n",
      "Epoch 879 Training Loss :0.1322847306728363 Test Loss:0.11826733499765396\n",
      "Epoch 880 Training Loss :0.1322827786207199 Test Loss:0.11826622486114502\n",
      "Epoch 881 Training Loss :0.13228082656860352 Test Loss:0.11826510727405548\n",
      "Epoch 882 Training Loss :0.13227887451648712 Test Loss:0.11826398968696594\n",
      "Epoch 883 Training Loss :0.13227692246437073 Test Loss:0.1182628720998764\n",
      "Epoch 884 Training Loss :0.13227498531341553 Test Loss:0.11826176196336746\n",
      "Epoch 885 Training Loss :0.13227303326129913 Test Loss:0.11826063692569733\n",
      "Epoch 886 Training Loss :0.13227109611034393 Test Loss:0.11825953423976898\n",
      "Epoch 887 Training Loss :0.13226915895938873 Test Loss:0.11825842410326004\n",
      "Epoch 888 Training Loss :0.13226722180843353 Test Loss:0.1182573139667511\n",
      "Epoch 889 Training Loss :0.13226528465747833 Test Loss:0.11825619637966156\n",
      "Epoch 890 Training Loss :0.13226334750652313 Test Loss:0.11825509369373322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 891 Training Loss :0.13226141035556793 Test Loss:0.11825399845838547\n",
      "Epoch 892 Training Loss :0.13225948810577393 Test Loss:0.11825290322303772\n",
      "Epoch 893 Training Loss :0.13225755095481873 Test Loss:0.11825178563594818\n",
      "Epoch 894 Training Loss :0.13225562870502472 Test Loss:0.11825069040060043\n",
      "Epoch 895 Training Loss :0.13225369155406952 Test Loss:0.11824959516525269\n",
      "Epoch 896 Training Loss :0.1322517693042755 Test Loss:0.11824849247932434\n",
      "Epoch 897 Training Loss :0.1322498470544815 Test Loss:0.118247389793396\n",
      "Epoch 898 Training Loss :0.1322479248046875 Test Loss:0.11824630945920944\n",
      "Epoch 899 Training Loss :0.1322460025548935 Test Loss:0.1182452142238617\n",
      "Epoch 900 Training Loss :0.1322440803050995 Test Loss:0.11824412643909454\n",
      "Epoch 901 Training Loss :0.13224217295646667 Test Loss:0.11824304610490799\n",
      "Epoch 902 Training Loss :0.13224025070667267 Test Loss:0.11824195832014084\n",
      "Epoch 903 Training Loss :0.13223834335803986 Test Loss:0.11824087053537369\n",
      "Epoch 904 Training Loss :0.13223643600940704 Test Loss:0.11823978275060654\n",
      "Epoch 905 Training Loss :0.13223452866077423 Test Loss:0.11823870241641998\n",
      "Epoch 906 Training Loss :0.13223262131214142 Test Loss:0.11823762208223343\n",
      "Epoch 907 Training Loss :0.1322307139635086 Test Loss:0.11823654919862747\n",
      "Epoch 908 Training Loss :0.1322288066148758 Test Loss:0.11823546141386032\n",
      "Epoch 909 Training Loss :0.13222689926624298 Test Loss:0.11823439598083496\n",
      "Epoch 910 Training Loss :0.13222500681877136 Test Loss:0.11823330819606781\n",
      "Epoch 911 Training Loss :0.13222311437129974 Test Loss:0.11823225021362305\n",
      "Epoch 912 Training Loss :0.13222120702266693 Test Loss:0.11823117733001709\n",
      "Epoch 913 Training Loss :0.1322193145751953 Test Loss:0.11823011189699173\n",
      "Epoch 914 Training Loss :0.1322174072265625 Test Loss:0.11822904646396637\n",
      "Epoch 915 Training Loss :0.13221552968025208 Test Loss:0.11822797358036041\n",
      "Epoch 916 Training Loss :0.13221363723278046 Test Loss:0.11822691559791565\n",
      "Epoch 917 Training Loss :0.13221174478530884 Test Loss:0.11822585761547089\n",
      "Epoch 918 Training Loss :0.13220985233783722 Test Loss:0.11822480708360672\n",
      "Epoch 919 Training Loss :0.1322079747915268 Test Loss:0.11822374165058136\n",
      "Epoch 920 Training Loss :0.13220608234405518 Test Loss:0.1182226836681366\n",
      "Epoch 921 Training Loss :0.13220420479774475 Test Loss:0.11822162568569183\n",
      "Epoch 922 Training Loss :0.13220232725143433 Test Loss:0.11822057515382767\n",
      "Epoch 923 Training Loss :0.1322004497051239 Test Loss:0.1182195246219635\n",
      "Epoch 924 Training Loss :0.13219857215881348 Test Loss:0.11821846663951874\n",
      "Epoch 925 Training Loss :0.13219669461250305 Test Loss:0.11821742355823517\n",
      "Epoch 926 Training Loss :0.13219481706619263 Test Loss:0.1182163879275322\n",
      "Epoch 927 Training Loss :0.1321929544210434 Test Loss:0.11821532994508743\n",
      "Epoch 928 Training Loss :0.13219107687473297 Test Loss:0.11821429431438446\n",
      "Epoch 929 Training Loss :0.13218921422958374 Test Loss:0.11821325123310089\n",
      "Epoch 930 Training Loss :0.13218733668327332 Test Loss:0.11821220070123672\n",
      "Epoch 931 Training Loss :0.13218548893928528 Test Loss:0.11821116507053375\n",
      "Epoch 932 Training Loss :0.13218362629413605 Test Loss:0.11821012943983078\n",
      "Epoch 933 Training Loss :0.13218176364898682 Test Loss:0.1182091012597084\n",
      "Epoch 934 Training Loss :0.13217990100383759 Test Loss:0.11820806562900543\n",
      "Epoch 935 Training Loss :0.13217803835868835 Test Loss:0.11820702999830246\n",
      "Epoch 936 Training Loss :0.13217617571353912 Test Loss:0.11820600926876068\n",
      "Epoch 937 Training Loss :0.1321743279695511 Test Loss:0.11820497363805771\n",
      "Epoch 938 Training Loss :0.13217248022556305 Test Loss:0.11820393800735474\n",
      "Epoch 939 Training Loss :0.13217061758041382 Test Loss:0.11820292472839355\n",
      "Epoch 940 Training Loss :0.13216876983642578 Test Loss:0.11820188909769058\n",
      "Epoch 941 Training Loss :0.13216692209243774 Test Loss:0.1182008609175682\n",
      "Epoch 942 Training Loss :0.1321650743484497 Test Loss:0.11819985508918762\n",
      "Epoch 943 Training Loss :0.13216322660446167 Test Loss:0.11819883435964584\n",
      "Epoch 944 Training Loss :0.13216137886047363 Test Loss:0.11819782108068466\n",
      "Epoch 945 Training Loss :0.1321595460176468 Test Loss:0.11819680780172348\n",
      "Epoch 946 Training Loss :0.13215769827365875 Test Loss:0.1181957870721817\n",
      "Epoch 947 Training Loss :0.13215585052967072 Test Loss:0.11819477379322052\n",
      "Epoch 948 Training Loss :0.13215401768684387 Test Loss:0.11819376051425934\n",
      "Epoch 949 Training Loss :0.13215218484401703 Test Loss:0.11819276213645935\n",
      "Epoch 950 Training Loss :0.13215035200119019 Test Loss:0.11819174885749817\n",
      "Epoch 951 Training Loss :0.13214850425720215 Test Loss:0.11819075047969818\n",
      "Epoch 952 Training Loss :0.1321466863155365 Test Loss:0.118189737200737\n",
      "Epoch 953 Training Loss :0.13214485347270966 Test Loss:0.11818873882293701\n",
      "Epoch 954 Training Loss :0.1321430206298828 Test Loss:0.11818776279687881\n",
      "Epoch 955 Training Loss :0.13214118778705597 Test Loss:0.11818674951791763\n",
      "Epoch 956 Training Loss :0.13213936984539032 Test Loss:0.11818575859069824\n",
      "Epoch 957 Training Loss :0.13213753700256348 Test Loss:0.11818476021289825\n",
      "Epoch 958 Training Loss :0.13213571906089783 Test Loss:0.11818376928567886\n",
      "Epoch 959 Training Loss :0.13213390111923218 Test Loss:0.11818277090787888\n",
      "Epoch 960 Training Loss :0.13213206827640533 Test Loss:0.11818177998065948\n",
      "Epoch 961 Training Loss :0.13213025033473969 Test Loss:0.11818079650402069\n",
      "Epoch 962 Training Loss :0.13212843239307404 Test Loss:0.1181798130273819\n",
      "Epoch 963 Training Loss :0.13212662935256958 Test Loss:0.1181788295507431\n",
      "Epoch 964 Training Loss :0.13212481141090393 Test Loss:0.11817783117294312\n",
      "Epoch 965 Training Loss :0.13212299346923828 Test Loss:0.11817685514688492\n",
      "Epoch 966 Training Loss :0.13212119042873383 Test Loss:0.11817587912082672\n",
      "Epoch 967 Training Loss :0.13211937248706818 Test Loss:0.11817490309476852\n",
      "Epoch 968 Training Loss :0.13211756944656372 Test Loss:0.11817391961812973\n",
      "Epoch 969 Training Loss :0.13211576640605927 Test Loss:0.11817293614149094\n",
      "Epoch 970 Training Loss :0.1321139633655548 Test Loss:0.11817196011543274\n",
      "Epoch 971 Training Loss :0.13211216032505035 Test Loss:0.11817098408937454\n",
      "Epoch 972 Training Loss :0.1321103721857071 Test Loss:0.11817000806331635\n",
      "Epoch 973 Training Loss :0.13210856914520264 Test Loss:0.11816903948783875\n",
      "Epoch 974 Training Loss :0.13210676610469818 Test Loss:0.11816807091236115\n",
      "Epoch 975 Training Loss :0.13210497796535492 Test Loss:0.11816710233688354\n",
      "Epoch 976 Training Loss :0.13210317492485046 Test Loss:0.11816613376140594\n",
      "Epoch 977 Training Loss :0.1321013867855072 Test Loss:0.11816515773534775\n",
      "Epoch 978 Training Loss :0.13209959864616394 Test Loss:0.11816421151161194\n",
      "Epoch 979 Training Loss :0.13209779560565948 Test Loss:0.11816324293613434\n",
      "Epoch 980 Training Loss :0.13209600746631622 Test Loss:0.11816228926181793\n",
      "Epoch 981 Training Loss :0.13209423422813416 Test Loss:0.11816131323575974\n",
      "Epoch 982 Training Loss :0.1320924460887909 Test Loss:0.11816035211086273\n",
      "Epoch 983 Training Loss :0.13209067285060883 Test Loss:0.11815939843654633\n",
      "Epoch 984 Training Loss :0.13208888471126556 Test Loss:0.11815843731164932\n",
      "Epoch 985 Training Loss :0.1320870965719223 Test Loss:0.11815747618675232\n",
      "Epoch 986 Training Loss :0.13208532333374023 Test Loss:0.11815652996301651\n",
      "Epoch 987 Training Loss :0.13208355009555817 Test Loss:0.1181555837392807\n",
      "Epoch 988 Training Loss :0.1320817768573761 Test Loss:0.1181546300649643\n",
      "Epoch 989 Training Loss :0.13208000361919403 Test Loss:0.11815366894006729\n",
      "Epoch 990 Training Loss :0.13207823038101196 Test Loss:0.11815273761749268\n",
      "Epoch 991 Training Loss :0.1320764571428299 Test Loss:0.11815179139375687\n",
      "Epoch 992 Training Loss :0.13207469880580902 Test Loss:0.11815083771944046\n",
      "Epoch 993 Training Loss :0.13207292556762695 Test Loss:0.11814990639686584\n",
      "Epoch 994 Training Loss :0.13207115232944489 Test Loss:0.11814894527196884\n",
      "Epoch 995 Training Loss :0.132069393992424 Test Loss:0.11814801394939423\n",
      "Epoch 996 Training Loss :0.13206763565540314 Test Loss:0.11814709007740021\n",
      "Epoch 997 Training Loss :0.13206587731838226 Test Loss:0.118146151304245\n",
      "Epoch 998 Training Loss :0.1320641189813614 Test Loss:0.11814519762992859\n",
      "Epoch 999 Training Loss :0.13206236064434052 Test Loss:0.11814428120851517\n",
      "Epoch 1000 Training Loss :0.13206060230731964 Test Loss:0.11814334988594055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1001 Training Loss :0.13205884397029877 Test Loss:0.11814241111278534\n",
      "Epoch 1002 Training Loss :0.1320571005344391 Test Loss:0.11814147979021072\n",
      "Epoch 1003 Training Loss :0.1320553421974182 Test Loss:0.1181405559182167\n",
      "Epoch 1004 Training Loss :0.13205359876155853 Test Loss:0.11813963204622269\n",
      "Epoch 1005 Training Loss :0.13205184042453766 Test Loss:0.11813870817422867\n",
      "Epoch 1006 Training Loss :0.13205009698867798 Test Loss:0.11813776940107346\n",
      "Epoch 1007 Training Loss :0.1320483535528183 Test Loss:0.11813686043024063\n",
      "Epoch 1008 Training Loss :0.13204661011695862 Test Loss:0.11813593655824661\n",
      "Epoch 1009 Training Loss :0.13204486668109894 Test Loss:0.1181350126862526\n",
      "Epoch 1010 Training Loss :0.13204312324523926 Test Loss:0.11813408881425858\n",
      "Epoch 1011 Training Loss :0.13204137980937958 Test Loss:0.11813317239284515\n",
      "Epoch 1012 Training Loss :0.1320396363735199 Test Loss:0.11813224852085114\n",
      "Epoch 1013 Training Loss :0.1320379078388214 Test Loss:0.11813133955001831\n",
      "Epoch 1014 Training Loss :0.13203616440296173 Test Loss:0.11813041567802429\n",
      "Epoch 1015 Training Loss :0.13203443586826324 Test Loss:0.11812950670719147\n",
      "Epoch 1016 Training Loss :0.13203269243240356 Test Loss:0.11812859773635864\n",
      "Epoch 1017 Training Loss :0.13203096389770508 Test Loss:0.11812768876552582\n",
      "Epoch 1018 Training Loss :0.1320292353630066 Test Loss:0.118126779794693\n",
      "Epoch 1019 Training Loss :0.1320275068283081 Test Loss:0.11812587082386017\n",
      "Epoch 1020 Training Loss :0.13202577829360962 Test Loss:0.11812496185302734\n",
      "Epoch 1021 Training Loss :0.13202404975891113 Test Loss:0.11812406033277512\n",
      "Epoch 1022 Training Loss :0.13202230632305145 Test Loss:0.11812316626310349\n",
      "Epoch 1023 Training Loss :0.13202059268951416 Test Loss:0.11812224984169006\n",
      "Epoch 1024 Training Loss :0.13201887905597687 Test Loss:0.11812136322259903\n",
      "Epoch 1025 Training Loss :0.13201715052127838 Test Loss:0.1181204617023468\n",
      "Epoch 1026 Training Loss :0.1320154219865799 Test Loss:0.11811956018209457\n",
      "Epoch 1027 Training Loss :0.1320137083530426 Test Loss:0.11811865866184235\n",
      "Epoch 1028 Training Loss :0.1320119947195053 Test Loss:0.11811777204275131\n",
      "Epoch 1029 Training Loss :0.13201028108596802 Test Loss:0.11811687797307968\n",
      "Epoch 1030 Training Loss :0.13200856745243073 Test Loss:0.11811599135398865\n",
      "Epoch 1031 Training Loss :0.13200685381889343 Test Loss:0.11811508983373642\n",
      "Epoch 1032 Training Loss :0.13200514018535614 Test Loss:0.11811421066522598\n",
      "Epoch 1033 Training Loss :0.13200342655181885 Test Loss:0.11811331659555435\n",
      "Epoch 1034 Training Loss :0.13200172781944275 Test Loss:0.11811242997646332\n",
      "Epoch 1035 Training Loss :0.13200001418590546 Test Loss:0.11811154335737228\n",
      "Epoch 1036 Training Loss :0.13199830055236816 Test Loss:0.11811065673828125\n",
      "Epoch 1037 Training Loss :0.13199660181999207 Test Loss:0.11810977011919022\n",
      "Epoch 1038 Training Loss :0.13199490308761597 Test Loss:0.11810889840126038\n",
      "Epoch 1039 Training Loss :0.13199318945407867 Test Loss:0.11810800433158875\n",
      "Epoch 1040 Training Loss :0.13199149072170258 Test Loss:0.1181071400642395\n",
      "Epoch 1041 Training Loss :0.13198979198932648 Test Loss:0.11810625344514847\n",
      "Epoch 1042 Training Loss :0.13198809325695038 Test Loss:0.11810536682605743\n",
      "Epoch 1043 Training Loss :0.13198639452457428 Test Loss:0.11810450255870819\n",
      "Epoch 1044 Training Loss :0.13198471069335938 Test Loss:0.11810361593961716\n",
      "Epoch 1045 Training Loss :0.13198301196098328 Test Loss:0.11810274422168732\n",
      "Epoch 1046 Training Loss :0.13198131322860718 Test Loss:0.11810187995433807\n",
      "Epoch 1047 Training Loss :0.13197962939739227 Test Loss:0.11810100823640823\n",
      "Epoch 1048 Training Loss :0.13197794556617737 Test Loss:0.1181001365184784\n",
      "Epoch 1049 Training Loss :0.13197624683380127 Test Loss:0.11809926480054855\n",
      "Epoch 1050 Training Loss :0.13197456300258636 Test Loss:0.11809840798377991\n",
      "Epoch 1051 Training Loss :0.13197287917137146 Test Loss:0.11809753626585007\n",
      "Epoch 1052 Training Loss :0.13197119534015656 Test Loss:0.11809666454792023\n",
      "Epoch 1053 Training Loss :0.13196951150894165 Test Loss:0.11809580773115158\n",
      "Epoch 1054 Training Loss :0.13196782767772675 Test Loss:0.11809494346380234\n",
      "Epoch 1055 Training Loss :0.13196615874767303 Test Loss:0.11809408664703369\n",
      "Epoch 1056 Training Loss :0.13196447491645813 Test Loss:0.11809323728084564\n",
      "Epoch 1057 Training Loss :0.13196279108524323 Test Loss:0.1180923730134964\n",
      "Epoch 1058 Training Loss :0.13196112215518951 Test Loss:0.11809152364730835\n",
      "Epoch 1059 Training Loss :0.1319594532251358 Test Loss:0.1180906668305397\n",
      "Epoch 1060 Training Loss :0.1319577693939209 Test Loss:0.11808981001377106\n",
      "Epoch 1061 Training Loss :0.1319561004638672 Test Loss:0.11808896064758301\n",
      "Epoch 1062 Training Loss :0.13195443153381348 Test Loss:0.11808811128139496\n",
      "Epoch 1063 Training Loss :0.13195276260375977 Test Loss:0.11808726191520691\n",
      "Epoch 1064 Training Loss :0.13195109367370605 Test Loss:0.11808641254901886\n",
      "Epoch 1065 Training Loss :0.13194943964481354 Test Loss:0.11808557063341141\n",
      "Epoch 1066 Training Loss :0.13194777071475983 Test Loss:0.11808472126722336\n",
      "Epoch 1067 Training Loss :0.13194610178470612 Test Loss:0.1180838793516159\n",
      "Epoch 1068 Training Loss :0.1319444477558136 Test Loss:0.11808303743600845\n",
      "Epoch 1069 Training Loss :0.13194279372692108 Test Loss:0.1180821880698204\n",
      "Epoch 1070 Training Loss :0.13194112479686737 Test Loss:0.11808135360479355\n",
      "Epoch 1071 Training Loss :0.13193947076797485 Test Loss:0.1180805191397667\n",
      "Epoch 1072 Training Loss :0.13193781673908234 Test Loss:0.11807967722415924\n",
      "Epoch 1073 Training Loss :0.13193616271018982 Test Loss:0.11807883530855179\n",
      "Epoch 1074 Training Loss :0.1319345086812973 Test Loss:0.11807800084352493\n",
      "Epoch 1075 Training Loss :0.13193286955356598 Test Loss:0.11807717382907867\n",
      "Epoch 1076 Training Loss :0.13193121552467346 Test Loss:0.11807633936405182\n",
      "Epoch 1077 Training Loss :0.13192956149578094 Test Loss:0.11807551980018616\n",
      "Epoch 1078 Training Loss :0.13192792236804962 Test Loss:0.1180746778845787\n",
      "Epoch 1079 Training Loss :0.1319262683391571 Test Loss:0.11807385087013245\n",
      "Epoch 1080 Training Loss :0.13192462921142578 Test Loss:0.11807302385568619\n",
      "Epoch 1081 Training Loss :0.13192297518253326 Test Loss:0.11807219684123993\n",
      "Epoch 1082 Training Loss :0.13192133605480194 Test Loss:0.11807136982679367\n",
      "Epoch 1083 Training Loss :0.13191969692707062 Test Loss:0.11807054281234741\n",
      "Epoch 1084 Training Loss :0.1319180577993393 Test Loss:0.11806971579790115\n",
      "Epoch 1085 Training Loss :0.13191643357276917 Test Loss:0.11806890368461609\n",
      "Epoch 1086 Training Loss :0.13191477954387665 Test Loss:0.11806808412075043\n",
      "Epoch 1087 Training Loss :0.13191315531730652 Test Loss:0.11806726455688477\n",
      "Epoch 1088 Training Loss :0.1319115161895752 Test Loss:0.1180664449930191\n",
      "Epoch 1089 Training Loss :0.13190989196300507 Test Loss:0.11806562542915344\n",
      "Epoch 1090 Training Loss :0.13190825283527374 Test Loss:0.11806480586528778\n",
      "Epoch 1091 Training Loss :0.1319066286087036 Test Loss:0.11806398630142212\n",
      "Epoch 1092 Training Loss :0.1319049894809723 Test Loss:0.11806318163871765\n",
      "Epoch 1093 Training Loss :0.13190336525440216 Test Loss:0.11806237697601318\n",
      "Epoch 1094 Training Loss :0.13190174102783203 Test Loss:0.11806156486272812\n",
      "Epoch 1095 Training Loss :0.1319001168012619 Test Loss:0.11806074529886246\n",
      "Epoch 1096 Training Loss :0.13189849257469177 Test Loss:0.11805994063615799\n",
      "Epoch 1097 Training Loss :0.13189686834812164 Test Loss:0.11805914342403412\n",
      "Epoch 1098 Training Loss :0.1318952441215515 Test Loss:0.11805833131074905\n",
      "Epoch 1099 Training Loss :0.13189361989498138 Test Loss:0.11805752664804459\n",
      "Epoch 1100 Training Loss :0.13189199566841125 Test Loss:0.11805672198534012\n",
      "Epoch 1101 Training Loss :0.13189038634300232 Test Loss:0.11805591732263565\n",
      "Epoch 1102 Training Loss :0.1318887621164322 Test Loss:0.11805512011051178\n",
      "Epoch 1103 Training Loss :0.13188713788986206 Test Loss:0.11805431544780731\n",
      "Epoch 1104 Training Loss :0.13188552856445312 Test Loss:0.11805352568626404\n",
      "Epoch 1105 Training Loss :0.1318839192390442 Test Loss:0.11805272847414017\n",
      "Epoch 1106 Training Loss :0.13188230991363525 Test Loss:0.1180519238114357\n",
      "Epoch 1107 Training Loss :0.13188070058822632 Test Loss:0.11805113404989243\n",
      "Epoch 1108 Training Loss :0.13187909126281738 Test Loss:0.11805033683776855\n",
      "Epoch 1109 Training Loss :0.13187748193740845 Test Loss:0.11804953962564468\n",
      "Epoch 1110 Training Loss :0.1318758726119995 Test Loss:0.11804874986410141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1111 Training Loss :0.13187426328659058 Test Loss:0.11804796755313873\n",
      "Epoch 1112 Training Loss :0.13187266886234283 Test Loss:0.11804717779159546\n",
      "Epoch 1113 Training Loss :0.1318710595369339 Test Loss:0.11804639548063278\n",
      "Epoch 1114 Training Loss :0.13186946511268616 Test Loss:0.11804560571908951\n",
      "Epoch 1115 Training Loss :0.13186785578727722 Test Loss:0.11804482340812683\n",
      "Epoch 1116 Training Loss :0.1318662464618683 Test Loss:0.11804403364658356\n",
      "Epoch 1117 Training Loss :0.13186465203762054 Test Loss:0.11804325878620148\n",
      "Epoch 1118 Training Loss :0.1318630576133728 Test Loss:0.1180424764752388\n",
      "Epoch 1119 Training Loss :0.13186146318912506 Test Loss:0.11804169416427612\n",
      "Epoch 1120 Training Loss :0.13185986876487732 Test Loss:0.11804091930389404\n",
      "Epoch 1121 Training Loss :0.13185827434062958 Test Loss:0.11804014444351196\n",
      "Epoch 1122 Training Loss :0.13185667991638184 Test Loss:0.11803936213254929\n",
      "Epoch 1123 Training Loss :0.1318550854921341 Test Loss:0.1180385872721672\n",
      "Epoch 1124 Training Loss :0.13185349106788635 Test Loss:0.11803781241178513\n",
      "Epoch 1125 Training Loss :0.1318519115447998 Test Loss:0.11803704500198364\n",
      "Epoch 1126 Training Loss :0.13185031712055206 Test Loss:0.11803627014160156\n",
      "Epoch 1127 Training Loss :0.13184873759746552 Test Loss:0.11803549528121948\n",
      "Epoch 1128 Training Loss :0.13184714317321777 Test Loss:0.118034727871418\n",
      "Epoch 1129 Training Loss :0.13184556365013123 Test Loss:0.11803396046161652\n",
      "Epoch 1130 Training Loss :0.13184398412704468 Test Loss:0.11803320050239563\n",
      "Epoch 1131 Training Loss :0.13184240460395813 Test Loss:0.11803242564201355\n",
      "Epoch 1132 Training Loss :0.13184082508087158 Test Loss:0.11803166568279266\n",
      "Epoch 1133 Training Loss :0.13183924555778503 Test Loss:0.11803090572357178\n",
      "Epoch 1134 Training Loss :0.1318376660346985 Test Loss:0.11803014576435089\n",
      "Epoch 1135 Training Loss :0.13183608651161194 Test Loss:0.11802937835454941\n",
      "Epoch 1136 Training Loss :0.1318345069885254 Test Loss:0.11802861839532852\n",
      "Epoch 1137 Training Loss :0.13183294236660004 Test Loss:0.11802787333726883\n",
      "Epoch 1138 Training Loss :0.1318313628435135 Test Loss:0.11802710592746735\n",
      "Epoch 1139 Training Loss :0.13182978332042694 Test Loss:0.11802634596824646\n",
      "Epoch 1140 Training Loss :0.1318282186985016 Test Loss:0.11802560091018677\n",
      "Epoch 1141 Training Loss :0.13182665407657623 Test Loss:0.11802484840154648\n",
      "Epoch 1142 Training Loss :0.13182507455348969 Test Loss:0.11802408844232559\n",
      "Epoch 1143 Training Loss :0.13182352483272552 Test Loss:0.1180233284831047\n",
      "Epoch 1144 Training Loss :0.13182194530963898 Test Loss:0.11802256852388382\n",
      "Epoch 1145 Training Loss :0.13182038068771362 Test Loss:0.11802181601524353\n",
      "Epoch 1146 Training Loss :0.13181881606578827 Test Loss:0.11802107095718384\n",
      "Epoch 1147 Training Loss :0.13181725144386292 Test Loss:0.11802031099796295\n",
      "Epoch 1148 Training Loss :0.13181568682193756 Test Loss:0.11801956593990326\n",
      "Epoch 1149 Training Loss :0.1318141371011734 Test Loss:0.11801879853010178\n",
      "Epoch 1150 Training Loss :0.13181257247924805 Test Loss:0.11801806837320328\n",
      "Epoch 1151 Training Loss :0.1318110227584839 Test Loss:0.11801732331514359\n",
      "Epoch 1152 Training Loss :0.13180945813655853 Test Loss:0.11801657825708389\n",
      "Epoch 1153 Training Loss :0.13180790841579437 Test Loss:0.1180158331990242\n",
      "Epoch 1154 Training Loss :0.13180634379386902 Test Loss:0.11801508814096451\n",
      "Epoch 1155 Training Loss :0.13180479407310486 Test Loss:0.11801434308290482\n",
      "Epoch 1156 Training Loss :0.1318032443523407 Test Loss:0.11801359802484512\n",
      "Epoch 1157 Training Loss :0.13180169463157654 Test Loss:0.11801286041736603\n",
      "Epoch 1158 Training Loss :0.13180014491081238 Test Loss:0.11801212280988693\n",
      "Epoch 1159 Training Loss :0.13179859519004822 Test Loss:0.11801138520240784\n",
      "Epoch 1160 Training Loss :0.13179704546928406 Test Loss:0.11801064759492874\n",
      "Epoch 1161 Training Loss :0.1317955106496811 Test Loss:0.11800990998744965\n",
      "Epoch 1162 Training Loss :0.13179396092891693 Test Loss:0.11800917237997055\n",
      "Epoch 1163 Training Loss :0.13179242610931396 Test Loss:0.11800842732191086\n",
      "Epoch 1164 Training Loss :0.131790891289711 Test Loss:0.11800769716501236\n",
      "Epoch 1165 Training Loss :0.13178935647010803 Test Loss:0.11800698190927505\n",
      "Epoch 1166 Training Loss :0.13178780674934387 Test Loss:0.11800625175237656\n",
      "Epoch 1167 Training Loss :0.1317862719297409 Test Loss:0.11800551414489746\n",
      "Epoch 1168 Training Loss :0.13178473711013794 Test Loss:0.11800478398799896\n",
      "Epoch 1169 Training Loss :0.13178320229053497 Test Loss:0.11800406128168106\n",
      "Epoch 1170 Training Loss :0.131781667470932 Test Loss:0.11800332367420197\n",
      "Epoch 1171 Training Loss :0.13178014755249023 Test Loss:0.11800261586904526\n",
      "Epoch 1172 Training Loss :0.13177861273288727 Test Loss:0.11800188571214676\n",
      "Epoch 1173 Training Loss :0.1317770779132843 Test Loss:0.11800115555524826\n",
      "Epoch 1174 Training Loss :0.13177555799484253 Test Loss:0.11800044029951096\n",
      "Epoch 1175 Training Loss :0.13177402317523956 Test Loss:0.11799973994493484\n",
      "Epoch 1176 Training Loss :0.1317725032567978 Test Loss:0.11799900233745575\n",
      "Epoch 1177 Training Loss :0.13177096843719482 Test Loss:0.11799826472997665\n",
      "Epoch 1178 Training Loss :0.13176944851875305 Test Loss:0.11799756437540054\n",
      "Epoch 1179 Training Loss :0.13176792860031128 Test Loss:0.11799684911966324\n",
      "Epoch 1180 Training Loss :0.1317664086818695 Test Loss:0.11799613386392593\n",
      "Epoch 1181 Training Loss :0.13176488876342773 Test Loss:0.11799541860818863\n",
      "Epoch 1182 Training Loss :0.13176336884498596 Test Loss:0.11799471080303192\n",
      "Epoch 1183 Training Loss :0.1317618489265442 Test Loss:0.11799398809671402\n",
      "Epoch 1184 Training Loss :0.13176032900810242 Test Loss:0.1179932951927185\n",
      "Epoch 1185 Training Loss :0.13175882399082184 Test Loss:0.1179925873875618\n",
      "Epoch 1186 Training Loss :0.13175728917121887 Test Loss:0.11799187958240509\n",
      "Epoch 1187 Training Loss :0.1317557841539383 Test Loss:0.11799116432666779\n",
      "Epoch 1188 Training Loss :0.13175427913665771 Test Loss:0.11799047887325287\n",
      "Epoch 1189 Training Loss :0.13175275921821594 Test Loss:0.11798977106809616\n",
      "Epoch 1190 Training Loss :0.13175125420093536 Test Loss:0.11798907071352005\n",
      "Epoch 1191 Training Loss :0.13174974918365479 Test Loss:0.11798837780952454\n",
      "Epoch 1192 Training Loss :0.131748229265213 Test Loss:0.11798767000436783\n",
      "Epoch 1193 Training Loss :0.13174672424793243 Test Loss:0.11798696964979172\n",
      "Epoch 1194 Training Loss :0.13174521923065186 Test Loss:0.1179862767457962\n",
      "Epoch 1195 Training Loss :0.13174371421337128 Test Loss:0.11798557639122009\n",
      "Epoch 1196 Training Loss :0.1317422240972519 Test Loss:0.11798489093780518\n",
      "Epoch 1197 Training Loss :0.1317407190799713 Test Loss:0.11798419803380966\n",
      "Epoch 1198 Training Loss :0.13173921406269073 Test Loss:0.11798349767923355\n",
      "Epoch 1199 Training Loss :0.13173772394657135 Test Loss:0.11798280477523804\n",
      "Epoch 1200 Training Loss :0.13173621892929077 Test Loss:0.11798210442066193\n",
      "Epoch 1201 Training Loss :0.1317347288131714 Test Loss:0.11798140406608582\n",
      "Epoch 1202 Training Loss :0.131733238697052 Test Loss:0.1179807260632515\n",
      "Epoch 1203 Training Loss :0.13173174858093262 Test Loss:0.11798002570867538\n",
      "Epoch 1204 Training Loss :0.13173024356365204 Test Loss:0.11797934770584106\n",
      "Epoch 1205 Training Loss :0.13172875344753265 Test Loss:0.11797865480184555\n",
      "Epoch 1206 Training Loss :0.13172727823257446 Test Loss:0.11797796934843063\n",
      "Epoch 1207 Training Loss :0.13172578811645508 Test Loss:0.11797727644443512\n",
      "Epoch 1208 Training Loss :0.1317242980003357 Test Loss:0.1179765909910202\n",
      "Epoch 1209 Training Loss :0.1317228227853775 Test Loss:0.11797590553760529\n",
      "Epoch 1210 Training Loss :0.13172133266925812 Test Loss:0.11797522008419037\n",
      "Epoch 1211 Training Loss :0.13171985745429993 Test Loss:0.11797454953193665\n",
      "Epoch 1212 Training Loss :0.13171836733818054 Test Loss:0.11797385662794113\n",
      "Epoch 1213 Training Loss :0.13171689212322235 Test Loss:0.11797317862510681\n",
      "Epoch 1214 Training Loss :0.13171540200710297 Test Loss:0.11797250062227249\n",
      "Epoch 1215 Training Loss :0.13171392679214478 Test Loss:0.11797180771827698\n",
      "Epoch 1216 Training Loss :0.13171245157718658 Test Loss:0.11797113716602325\n",
      "Epoch 1217 Training Loss :0.1317109763622284 Test Loss:0.11797045171260834\n",
      "Epoch 1218 Training Loss :0.1317095011472702 Test Loss:0.11796977370977402\n",
      "Epoch 1219 Training Loss :0.131708025932312 Test Loss:0.11796911805868149\n",
      "Epoch 1220 Training Loss :0.13170655071735382 Test Loss:0.11796844005584717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1221 Training Loss :0.13170507550239563 Test Loss:0.11796776205301285\n",
      "Epoch 1222 Training Loss :0.13170360028743744 Test Loss:0.11796709150075912\n",
      "Epoch 1223 Training Loss :0.13170213997364044 Test Loss:0.117966428399086\n",
      "Epoch 1224 Training Loss :0.13170066475868225 Test Loss:0.11796575039625168\n",
      "Epoch 1225 Training Loss :0.13169918954372406 Test Loss:0.11796509474515915\n",
      "Epoch 1226 Training Loss :0.13169772922992706 Test Loss:0.11796442419290543\n",
      "Epoch 1227 Training Loss :0.13169625401496887 Test Loss:0.1179637536406517\n",
      "Epoch 1228 Training Loss :0.13169479370117188 Test Loss:0.11796309053897858\n",
      "Epoch 1229 Training Loss :0.13169333338737488 Test Loss:0.11796242743730545\n",
      "Epoch 1230 Training Loss :0.13169187307357788 Test Loss:0.11796177178621292\n",
      "Epoch 1231 Training Loss :0.1316903978586197 Test Loss:0.1179611086845398\n",
      "Epoch 1232 Training Loss :0.1316889375448227 Test Loss:0.11796045303344727\n",
      "Epoch 1233 Training Loss :0.1316874772310257 Test Loss:0.11795980483293533\n",
      "Epoch 1234 Training Loss :0.1316860169172287 Test Loss:0.11795913428068161\n",
      "Epoch 1235 Training Loss :0.1316845566034317 Test Loss:0.11795848608016968\n",
      "Epoch 1236 Training Loss :0.1316830962896347 Test Loss:0.11795783042907715\n",
      "Epoch 1237 Training Loss :0.1316816508769989 Test Loss:0.11795717477798462\n",
      "Epoch 1238 Training Loss :0.1316801905632019 Test Loss:0.11795651167631149\n",
      "Epoch 1239 Training Loss :0.1316787451505661 Test Loss:0.11795585602521896\n",
      "Epoch 1240 Training Loss :0.1316772848367691 Test Loss:0.11795521527528763\n",
      "Epoch 1241 Training Loss :0.1316758394241333 Test Loss:0.1179545596241951\n",
      "Epoch 1242 Training Loss :0.1316743940114975 Test Loss:0.11795390397310257\n",
      "Epoch 1243 Training Loss :0.1316729485988617 Test Loss:0.11795327067375183\n",
      "Epoch 1244 Training Loss :0.1316714882850647 Test Loss:0.1179526075720787\n",
      "Epoch 1245 Training Loss :0.1316700428724289 Test Loss:0.11795195192098618\n",
      "Epoch 1246 Training Loss :0.1316685974597931 Test Loss:0.11795131862163544\n",
      "Epoch 1247 Training Loss :0.1316671520471573 Test Loss:0.1179506704211235\n",
      "Epoch 1248 Training Loss :0.13166570663452148 Test Loss:0.11795002967119217\n",
      "Epoch 1249 Training Loss :0.13166427612304688 Test Loss:0.11794938147068024\n",
      "Epoch 1250 Training Loss :0.13166283071041107 Test Loss:0.1179487481713295\n",
      "Epoch 1251 Training Loss :0.13166138529777527 Test Loss:0.11794810742139816\n",
      "Epoch 1252 Training Loss :0.13165995478630066 Test Loss:0.11794746667146683\n",
      "Epoch 1253 Training Loss :0.13165850937366486 Test Loss:0.11794682592153549\n",
      "Epoch 1254 Training Loss :0.13165707886219025 Test Loss:0.11794620007276535\n",
      "Epoch 1255 Training Loss :0.13165564835071564 Test Loss:0.11794555932283401\n",
      "Epoch 1256 Training Loss :0.13165420293807983 Test Loss:0.11794492602348328\n",
      "Epoch 1257 Training Loss :0.13165277242660522 Test Loss:0.11794428527355194\n",
      "Epoch 1258 Training Loss :0.13165134191513062 Test Loss:0.1179436594247818\n",
      "Epoch 1259 Training Loss :0.131649911403656 Test Loss:0.11794304102659225\n",
      "Epoch 1260 Training Loss :0.1316484808921814 Test Loss:0.11794241517782211\n",
      "Epoch 1261 Training Loss :0.1316470503807068 Test Loss:0.11794178187847137\n",
      "Epoch 1262 Training Loss :0.13164561986923218 Test Loss:0.11794114857912064\n",
      "Epoch 1263 Training Loss :0.13164418935775757 Test Loss:0.11794053763151169\n",
      "Epoch 1264 Training Loss :0.13164275884628296 Test Loss:0.11793989688158035\n",
      "Epoch 1265 Training Loss :0.13164132833480835 Test Loss:0.11793927103281021\n",
      "Epoch 1266 Training Loss :0.13163991272449493 Test Loss:0.11793865263462067\n",
      "Epoch 1267 Training Loss :0.13163848221302032 Test Loss:0.11793803423643112\n",
      "Epoch 1268 Training Loss :0.1316370666027069 Test Loss:0.11793740838766098\n",
      "Epoch 1269 Training Loss :0.1316356509923935 Test Loss:0.11793679744005203\n",
      "Epoch 1270 Training Loss :0.13163422048091888 Test Loss:0.11793618649244308\n",
      "Epoch 1271 Training Loss :0.13163280487060547 Test Loss:0.11793556809425354\n",
      "Epoch 1272 Training Loss :0.13163138926029205 Test Loss:0.117934949696064\n",
      "Epoch 1273 Training Loss :0.13162997364997864 Test Loss:0.11793433874845505\n",
      "Epoch 1274 Training Loss :0.13162854313850403 Test Loss:0.11793370544910431\n",
      "Epoch 1275 Training Loss :0.1316271424293518 Test Loss:0.11793310195207596\n",
      "Epoch 1276 Training Loss :0.1316257268190384 Test Loss:0.11793248355388641\n",
      "Epoch 1277 Training Loss :0.13162431120872498 Test Loss:0.11793186515569687\n",
      "Epoch 1278 Training Loss :0.13162289559841156 Test Loss:0.11793126165866852\n",
      "Epoch 1279 Training Loss :0.13162147998809814 Test Loss:0.11793065816164017\n",
      "Epoch 1280 Training Loss :0.13162007927894592 Test Loss:0.11793003976345062\n",
      "Epoch 1281 Training Loss :0.1316186636686325 Test Loss:0.11792943626642227\n",
      "Epoch 1282 Training Loss :0.13161726295948029 Test Loss:0.11792883276939392\n",
      "Epoch 1283 Training Loss :0.13161586225032806 Test Loss:0.11792822927236557\n",
      "Epoch 1284 Training Loss :0.13161444664001465 Test Loss:0.11792760342359543\n",
      "Epoch 1285 Training Loss :0.13161304593086243 Test Loss:0.11792701482772827\n",
      "Epoch 1286 Training Loss :0.1316116452217102 Test Loss:0.11792641133069992\n",
      "Epoch 1287 Training Loss :0.13161024451255798 Test Loss:0.11792580783367157\n",
      "Epoch 1288 Training Loss :0.13160884380340576 Test Loss:0.11792520433664322\n",
      "Epoch 1289 Training Loss :0.13160744309425354 Test Loss:0.11792460829019547\n",
      "Epoch 1290 Training Loss :0.13160604238510132 Test Loss:0.11792401224374771\n",
      "Epoch 1291 Training Loss :0.1316046416759491 Test Loss:0.11792340874671936\n",
      "Epoch 1292 Training Loss :0.13160324096679688 Test Loss:0.1179228201508522\n",
      "Epoch 1293 Training Loss :0.13160185515880585 Test Loss:0.11792220175266266\n",
      "Epoch 1294 Training Loss :0.13160045444965363 Test Loss:0.1179216206073761\n",
      "Epoch 1295 Training Loss :0.1315990686416626 Test Loss:0.11792102456092834\n",
      "Epoch 1296 Training Loss :0.13159766793251038 Test Loss:0.11792043596506119\n",
      "Epoch 1297 Training Loss :0.13159628212451935 Test Loss:0.11791982501745224\n",
      "Epoch 1298 Training Loss :0.13159489631652832 Test Loss:0.11791924387216568\n",
      "Epoch 1299 Training Loss :0.1315934956073761 Test Loss:0.11791865527629852\n",
      "Epoch 1300 Training Loss :0.13159210979938507 Test Loss:0.11791806668043137\n",
      "Epoch 1301 Training Loss :0.13159072399139404 Test Loss:0.11791747063398361\n",
      "Epoch 1302 Training Loss :0.13158933818340302 Test Loss:0.11791688203811646\n",
      "Epoch 1303 Training Loss :0.131587952375412 Test Loss:0.1179163008928299\n",
      "Epoch 1304 Training Loss :0.13158656656742096 Test Loss:0.11791571229696274\n",
      "Epoch 1305 Training Loss :0.13158516585826874 Test Loss:0.11791513860225677\n",
      "Epoch 1306 Training Loss :0.1315837949514389 Test Loss:0.11791455000638962\n",
      "Epoch 1307 Training Loss :0.13158240914344788 Test Loss:0.11791397631168365\n",
      "Epoch 1308 Training Loss :0.13158102333545685 Test Loss:0.1179133802652359\n",
      "Epoch 1309 Training Loss :0.13157965242862701 Test Loss:0.11791279911994934\n",
      "Epoch 1310 Training Loss :0.131578266620636 Test Loss:0.11791221052408218\n",
      "Epoch 1311 Training Loss :0.13157689571380615 Test Loss:0.11791164427995682\n",
      "Epoch 1312 Training Loss :0.13157550990581512 Test Loss:0.11791106313467026\n",
      "Epoch 1313 Training Loss :0.1315741389989853 Test Loss:0.1179104670882225\n",
      "Epoch 1314 Training Loss :0.13157276809215546 Test Loss:0.11790990829467773\n",
      "Epoch 1315 Training Loss :0.13157138228416443 Test Loss:0.11790933459997177\n",
      "Epoch 1316 Training Loss :0.1315700113773346 Test Loss:0.11790876090526581\n",
      "Epoch 1317 Training Loss :0.13156864047050476 Test Loss:0.11790818721055984\n",
      "Epoch 1318 Training Loss :0.13156726956367493 Test Loss:0.11790761351585388\n",
      "Epoch 1319 Training Loss :0.1315658986568451 Test Loss:0.11790705472230911\n",
      "Epoch 1320 Training Loss :0.13156452775001526 Test Loss:0.11790647357702255\n",
      "Epoch 1321 Training Loss :0.13156317174434662 Test Loss:0.11790589243173599\n",
      "Epoch 1322 Training Loss :0.13156180083751678 Test Loss:0.11790532618761063\n",
      "Epoch 1323 Training Loss :0.13156042993068695 Test Loss:0.11790476739406586\n",
      "Epoch 1324 Training Loss :0.13155905902385712 Test Loss:0.1179041862487793\n",
      "Epoch 1325 Training Loss :0.13155770301818848 Test Loss:0.11790362745523453\n",
      "Epoch 1326 Training Loss :0.13155633211135864 Test Loss:0.11790305376052856\n",
      "Epoch 1327 Training Loss :0.13155497610569 Test Loss:0.11790250986814499\n",
      "Epoch 1328 Training Loss :0.13155360519886017 Test Loss:0.11790193617343903\n",
      "Epoch 1329 Training Loss :0.13155224919319153 Test Loss:0.11790136992931366\n",
      "Epoch 1330 Training Loss :0.1315508931875229 Test Loss:0.1179008036851883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1331 Training Loss :0.13154952228069305 Test Loss:0.11790024489164352\n",
      "Epoch 1332 Training Loss :0.13154816627502441 Test Loss:0.11789968609809875\n",
      "Epoch 1333 Training Loss :0.13154681026935577 Test Loss:0.11789912730455399\n",
      "Epoch 1334 Training Loss :0.13154545426368713 Test Loss:0.11789856851100922\n",
      "Epoch 1335 Training Loss :0.1315440982580185 Test Loss:0.11789800971746445\n",
      "Epoch 1336 Training Loss :0.13154274225234985 Test Loss:0.11789745092391968\n",
      "Epoch 1337 Training Loss :0.1315413862466812 Test Loss:0.11789688467979431\n",
      "Epoch 1338 Training Loss :0.13154003024101257 Test Loss:0.11789634078741074\n",
      "Epoch 1339 Training Loss :0.13153867423534393 Test Loss:0.11789578944444656\n",
      "Epoch 1340 Training Loss :0.1315373331308365 Test Loss:0.11789524555206299\n",
      "Epoch 1341 Training Loss :0.13153597712516785 Test Loss:0.11789469420909882\n",
      "Epoch 1342 Training Loss :0.1315346211194992 Test Loss:0.11789413541555405\n",
      "Epoch 1343 Training Loss :0.13153328001499176 Test Loss:0.11789358407258987\n",
      "Epoch 1344 Training Loss :0.13153192400932312 Test Loss:0.1178930401802063\n",
      "Epoch 1345 Training Loss :0.13153058290481567 Test Loss:0.11789248138666153\n",
      "Epoch 1346 Training Loss :0.13152924180030823 Test Loss:0.11789194494485855\n",
      "Epoch 1347 Training Loss :0.13152790069580078 Test Loss:0.11789138615131378\n",
      "Epoch 1348 Training Loss :0.13152654469013214 Test Loss:0.1178908422589302\n",
      "Epoch 1349 Training Loss :0.1315252035856247 Test Loss:0.11789031326770782\n",
      "Epoch 1350 Training Loss :0.13152386248111725 Test Loss:0.11788976192474365\n",
      "Epoch 1351 Training Loss :0.1315225213766098 Test Loss:0.11788921803236008\n",
      "Epoch 1352 Training Loss :0.13152118027210236 Test Loss:0.1178886741399765\n",
      "Epoch 1353 Training Loss :0.1315198391675949 Test Loss:0.11788811534643173\n",
      "Epoch 1354 Training Loss :0.13151851296424866 Test Loss:0.11788757890462875\n",
      "Epoch 1355 Training Loss :0.1315171718597412 Test Loss:0.11788703501224518\n",
      "Epoch 1356 Training Loss :0.13151584565639496 Test Loss:0.1178864985704422\n",
      "Epoch 1357 Training Loss :0.1315145045518875 Test Loss:0.11788596212863922\n",
      "Epoch 1358 Training Loss :0.13151316344738007 Test Loss:0.11788542568683624\n",
      "Epoch 1359 Training Loss :0.1315118372440338 Test Loss:0.11788488179445267\n",
      "Epoch 1360 Training Loss :0.13151051104068756 Test Loss:0.11788433790206909\n",
      "Epoch 1361 Training Loss :0.1315091848373413 Test Loss:0.11788381636142731\n",
      "Epoch 1362 Training Loss :0.13150784373283386 Test Loss:0.11788327246904373\n",
      "Epoch 1363 Training Loss :0.1315065175294876 Test Loss:0.11788275837898254\n",
      "Epoch 1364 Training Loss :0.13150519132614136 Test Loss:0.11788221448659897\n",
      "Epoch 1365 Training Loss :0.1315038502216339 Test Loss:0.11788168549537659\n",
      "Epoch 1366 Training Loss :0.13150253891944885 Test Loss:0.11788114160299301\n",
      "Epoch 1367 Training Loss :0.1315012127161026 Test Loss:0.11788060516119003\n",
      "Epoch 1368 Training Loss :0.13149988651275635 Test Loss:0.11788006871938705\n",
      "Epoch 1369 Training Loss :0.1314985603094101 Test Loss:0.11787953972816467\n",
      "Epoch 1370 Training Loss :0.13149723410606384 Test Loss:0.11787901818752289\n",
      "Epoch 1371 Training Loss :0.13149592280387878 Test Loss:0.1178785040974617\n",
      "Epoch 1372 Training Loss :0.13149459660053253 Test Loss:0.11787797510623932\n",
      "Epoch 1373 Training Loss :0.13149328529834747 Test Loss:0.11787742376327515\n",
      "Epoch 1374 Training Loss :0.13149195909500122 Test Loss:0.11787690967321396\n",
      "Epoch 1375 Training Loss :0.13149064779281616 Test Loss:0.11787639558315277\n",
      "Epoch 1376 Training Loss :0.1314893215894699 Test Loss:0.11787586659193039\n",
      "Epoch 1377 Training Loss :0.13148801028728485 Test Loss:0.11787533760070801\n",
      "Epoch 1378 Training Loss :0.1314866989850998 Test Loss:0.11787482351064682\n",
      "Epoch 1379 Training Loss :0.13148537278175354 Test Loss:0.11787430942058563\n",
      "Epoch 1380 Training Loss :0.13148406147956848 Test Loss:0.11787378042936325\n",
      "Epoch 1381 Training Loss :0.13148276507854462 Test Loss:0.11787326633930206\n",
      "Epoch 1382 Training Loss :0.13148143887519836 Test Loss:0.11787275224924088\n",
      "Epoch 1383 Training Loss :0.1314801424741745 Test Loss:0.1178722232580185\n",
      "Epoch 1384 Training Loss :0.13147883117198944 Test Loss:0.1178717091679573\n",
      "Epoch 1385 Training Loss :0.13147751986980438 Test Loss:0.11787118762731552\n",
      "Epoch 1386 Training Loss :0.13147620856761932 Test Loss:0.11787067353725433\n",
      "Epoch 1387 Training Loss :0.13147489726543427 Test Loss:0.11787015944719315\n",
      "Epoch 1388 Training Loss :0.1314736008644104 Test Loss:0.11786964535713196\n",
      "Epoch 1389 Training Loss :0.13147228956222534 Test Loss:0.11786913871765137\n",
      "Epoch 1390 Training Loss :0.13147097826004028 Test Loss:0.11786860972642899\n",
      "Epoch 1391 Training Loss :0.13146968185901642 Test Loss:0.1178681030869484\n",
      "Epoch 1392 Training Loss :0.13146838545799255 Test Loss:0.11786758899688721\n",
      "Epoch 1393 Training Loss :0.1314670890569687 Test Loss:0.11786707490682602\n",
      "Epoch 1394 Training Loss :0.13146577775478363 Test Loss:0.11786657571792603\n",
      "Epoch 1395 Training Loss :0.13146448135375977 Test Loss:0.11786606907844543\n",
      "Epoch 1396 Training Loss :0.1314631849527359 Test Loss:0.11786554753780365\n",
      "Epoch 1397 Training Loss :0.13146188855171204 Test Loss:0.11786503344774246\n",
      "Epoch 1398 Training Loss :0.13146059215068817 Test Loss:0.11786454170942307\n",
      "Epoch 1399 Training Loss :0.1314592957496643 Test Loss:0.11786402016878128\n",
      "Epoch 1400 Training Loss :0.13145799934864044 Test Loss:0.11786351352930069\n",
      "Epoch 1401 Training Loss :0.13145670294761658 Test Loss:0.1178630143404007\n",
      "Epoch 1402 Training Loss :0.1314554065465927 Test Loss:0.11786249279975891\n",
      "Epoch 1403 Training Loss :0.13145411014556885 Test Loss:0.11786198616027832\n",
      "Epoch 1404 Training Loss :0.13145282864570618 Test Loss:0.11786150932312012\n",
      "Epoch 1405 Training Loss :0.1314515322446823 Test Loss:0.11786100268363953\n",
      "Epoch 1406 Training Loss :0.13145025074481964 Test Loss:0.11786050349473953\n",
      "Epoch 1407 Training Loss :0.13144895434379578 Test Loss:0.11785999685525894\n",
      "Epoch 1408 Training Loss :0.1314476728439331 Test Loss:0.11785949766635895\n",
      "Epoch 1409 Training Loss :0.13144637644290924 Test Loss:0.11785901337862015\n",
      "Epoch 1410 Training Loss :0.13144509494304657 Test Loss:0.11785850673913956\n",
      "Epoch 1411 Training Loss :0.1314437985420227 Test Loss:0.11785801500082016\n",
      "Epoch 1412 Training Loss :0.13144251704216003 Test Loss:0.11785751581192017\n",
      "Epoch 1413 Training Loss :0.13144123554229736 Test Loss:0.11785702407360077\n",
      "Epoch 1414 Training Loss :0.1314399540424347 Test Loss:0.11785653233528137\n",
      "Epoch 1415 Training Loss :0.13143867254257202 Test Loss:0.11785604059696198\n",
      "Epoch 1416 Training Loss :0.13143739104270935 Test Loss:0.11785554885864258\n",
      "Epoch 1417 Training Loss :0.13143609464168549 Test Loss:0.11785505712032318\n",
      "Epoch 1418 Training Loss :0.13143481314182281 Test Loss:0.11785456538200378\n",
      "Epoch 1419 Training Loss :0.13143354654312134 Test Loss:0.11785408109426498\n",
      "Epoch 1420 Training Loss :0.13143226504325867 Test Loss:0.11785358935594559\n",
      "Epoch 1421 Training Loss :0.131430983543396 Test Loss:0.11785310506820679\n",
      "Epoch 1422 Training Loss :0.13142970204353333 Test Loss:0.11785261332988739\n",
      "Epoch 1423 Training Loss :0.13142842054367065 Test Loss:0.11785213649272919\n",
      "Epoch 1424 Training Loss :0.13142715394496918 Test Loss:0.1178516373038292\n",
      "Epoch 1425 Training Loss :0.1314258724451065 Test Loss:0.11785116046667099\n",
      "Epoch 1426 Training Loss :0.13142459094524384 Test Loss:0.11785066872835159\n",
      "Epoch 1427 Training Loss :0.13142332434654236 Test Loss:0.11785018444061279\n",
      "Epoch 1428 Training Loss :0.13142205774784088 Test Loss:0.11784970760345459\n",
      "Epoch 1429 Training Loss :0.1314207911491394 Test Loss:0.11784923076629639\n",
      "Epoch 1430 Training Loss :0.13141950964927673 Test Loss:0.11784873902797699\n",
      "Epoch 1431 Training Loss :0.13141824305057526 Test Loss:0.11784826964139938\n",
      "Epoch 1432 Training Loss :0.13141697645187378 Test Loss:0.11784777045249939\n",
      "Epoch 1433 Training Loss :0.1314157098531723 Test Loss:0.11784730851650238\n",
      "Epoch 1434 Training Loss :0.13141444325447083 Test Loss:0.11784682422876358\n",
      "Epoch 1435 Training Loss :0.13141317665576935 Test Loss:0.11784633994102478\n",
      "Epoch 1436 Training Loss :0.13141191005706787 Test Loss:0.11784587055444717\n",
      "Epoch 1437 Training Loss :0.1314106434583664 Test Loss:0.11784539371728897\n",
      "Epoch 1438 Training Loss :0.1314093917608261 Test Loss:0.11784493178129196\n",
      "Epoch 1439 Training Loss :0.13140812516212463 Test Loss:0.11784445494413376\n",
      "Epoch 1440 Training Loss :0.13140685856342316 Test Loss:0.11784397810697556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1441 Training Loss :0.13140560686588287 Test Loss:0.11784350126981735\n",
      "Epoch 1442 Training Loss :0.1314043402671814 Test Loss:0.11784303188323975\n",
      "Epoch 1443 Training Loss :0.1314030885696411 Test Loss:0.11784255504608154\n",
      "Epoch 1444 Training Loss :0.13140182197093964 Test Loss:0.11784208565950394\n",
      "Epoch 1445 Training Loss :0.13140057027339935 Test Loss:0.11784160882234573\n",
      "Epoch 1446 Training Loss :0.13139931857585907 Test Loss:0.11784114688634872\n",
      "Epoch 1447 Training Loss :0.1313980519771576 Test Loss:0.11784066259860992\n",
      "Epoch 1448 Training Loss :0.1313968002796173 Test Loss:0.11784020066261292\n",
      "Epoch 1449 Training Loss :0.13139554858207703 Test Loss:0.11783973127603531\n",
      "Epoch 1450 Training Loss :0.13139429688453674 Test Loss:0.1178392618894577\n",
      "Epoch 1451 Training Loss :0.13139304518699646 Test Loss:0.1178387999534607\n",
      "Epoch 1452 Training Loss :0.13139179348945618 Test Loss:0.11783832311630249\n",
      "Epoch 1453 Training Loss :0.1313905417919159 Test Loss:0.11783786863088608\n",
      "Epoch 1454 Training Loss :0.1313892900943756 Test Loss:0.11783740669488907\n",
      "Epoch 1455 Training Loss :0.13138803839683533 Test Loss:0.11783693730831146\n",
      "Epoch 1456 Training Loss :0.13138680160045624 Test Loss:0.11783646047115326\n",
      "Epoch 1457 Training Loss :0.13138554990291595 Test Loss:0.11783599108457565\n",
      "Epoch 1458 Training Loss :0.13138431310653687 Test Loss:0.11783554404973984\n",
      "Epoch 1459 Training Loss :0.13138306140899658 Test Loss:0.11783508211374283\n",
      "Epoch 1460 Training Loss :0.1313818246126175 Test Loss:0.11783460527658463\n",
      "Epoch 1461 Training Loss :0.1313805729150772 Test Loss:0.11783414334058762\n",
      "Epoch 1462 Training Loss :0.13137933611869812 Test Loss:0.1178337037563324\n",
      "Epoch 1463 Training Loss :0.13137809932231903 Test Loss:0.1178332269191742\n",
      "Epoch 1464 Training Loss :0.13137686252593994 Test Loss:0.11783276498317719\n",
      "Epoch 1465 Training Loss :0.13137561082839966 Test Loss:0.11783230304718018\n",
      "Epoch 1466 Training Loss :0.13137437403202057 Test Loss:0.11783184856176376\n",
      "Epoch 1467 Training Loss :0.13137313723564148 Test Loss:0.11783139407634735\n",
      "Epoch 1468 Training Loss :0.1313719004392624 Test Loss:0.11783093214035034\n",
      "Epoch 1469 Training Loss :0.1313706785440445 Test Loss:0.11783047020435333\n",
      "Epoch 1470 Training Loss :0.1313694417476654 Test Loss:0.11783001571893692\n",
      "Epoch 1471 Training Loss :0.13136820495128632 Test Loss:0.1178295686841011\n",
      "Epoch 1472 Training Loss :0.13136696815490723 Test Loss:0.11782911419868469\n",
      "Epoch 1473 Training Loss :0.13136574625968933 Test Loss:0.11782865971326828\n",
      "Epoch 1474 Training Loss :0.13136452436447144 Test Loss:0.11782821267843246\n",
      "Epoch 1475 Training Loss :0.13136328756809235 Test Loss:0.11782775819301605\n",
      "Epoch 1476 Training Loss :0.13136205077171326 Test Loss:0.11782731115818024\n",
      "Epoch 1477 Training Loss :0.13136082887649536 Test Loss:0.11782685667276382\n",
      "Epoch 1478 Training Loss :0.13135959208011627 Test Loss:0.1178264170885086\n",
      "Epoch 1479 Training Loss :0.13135838508605957 Test Loss:0.1178259626030922\n",
      "Epoch 1480 Training Loss :0.13135714828968048 Test Loss:0.11782550811767578\n",
      "Epoch 1481 Training Loss :0.13135592639446259 Test Loss:0.11782507598400116\n",
      "Epoch 1482 Training Loss :0.1313547044992447 Test Loss:0.11782461404800415\n",
      "Epoch 1483 Training Loss :0.1313534826040268 Test Loss:0.11782417446374893\n",
      "Epoch 1484 Training Loss :0.1313522607088089 Test Loss:0.11782373487949371\n",
      "Epoch 1485 Training Loss :0.131351038813591 Test Loss:0.1178232952952385\n",
      "Epoch 1486 Training Loss :0.1313498318195343 Test Loss:0.11782284826040268\n",
      "Epoch 1487 Training Loss :0.1313485950231552 Test Loss:0.11782240867614746\n",
      "Epoch 1488 Training Loss :0.1313473880290985 Test Loss:0.11782196909189224\n",
      "Epoch 1489 Training Loss :0.13134616613388062 Test Loss:0.11782153695821762\n",
      "Epoch 1490 Training Loss :0.1313449591398239 Test Loss:0.11782106757164001\n",
      "Epoch 1491 Training Loss :0.13134373724460602 Test Loss:0.11782064288854599\n",
      "Epoch 1492 Training Loss :0.13134251534938812 Test Loss:0.11782020330429077\n",
      "Epoch 1493 Training Loss :0.13134130835533142 Test Loss:0.11781977117061615\n",
      "Epoch 1494 Training Loss :0.13134008646011353 Test Loss:0.11781933903694153\n",
      "Epoch 1495 Training Loss :0.13133887946605682 Test Loss:0.11781889200210571\n",
      "Epoch 1496 Training Loss :0.13133767247200012 Test Loss:0.11781847476959229\n",
      "Epoch 1497 Training Loss :0.13133645057678223 Test Loss:0.11781803518533707\n",
      "Epoch 1498 Training Loss :0.13133524358272552 Test Loss:0.11781760305166245\n",
      "Epoch 1499 Training Loss :0.13133403658866882 Test Loss:0.11781717836856842\n",
      "Epoch 1500 Training Loss :0.13133282959461212 Test Loss:0.1178167536854744\n",
      "Epoch 1501 Training Loss :0.13133160769939423 Test Loss:0.11781632155179977\n",
      "Epoch 1502 Training Loss :0.13133040070533752 Test Loss:0.11781589686870575\n",
      "Epoch 1503 Training Loss :0.13132919371128082 Test Loss:0.11781547218561172\n",
      "Epoch 1504 Training Loss :0.13132798671722412 Test Loss:0.1178150400519371\n",
      "Epoch 1505 Training Loss :0.13132677972316742 Test Loss:0.11781462281942368\n",
      "Epoch 1506 Training Loss :0.13132557272911072 Test Loss:0.11781419068574905\n",
      "Epoch 1507 Training Loss :0.13132436573505402 Test Loss:0.11781376600265503\n",
      "Epoch 1508 Training Loss :0.13132315874099731 Test Loss:0.117813341319561\n",
      "Epoch 1509 Training Loss :0.1313219517469406 Test Loss:0.11781293153762817\n",
      "Epoch 1510 Training Loss :0.1313207596540451 Test Loss:0.11781251430511475\n",
      "Epoch 1511 Training Loss :0.1313195526599884 Test Loss:0.11781208962202072\n",
      "Epoch 1512 Training Loss :0.1313183456659317 Test Loss:0.1178116723895073\n",
      "Epoch 1513 Training Loss :0.1313171535730362 Test Loss:0.11781125515699387\n",
      "Epoch 1514 Training Loss :0.1313159465789795 Test Loss:0.11781083047389984\n",
      "Epoch 1515 Training Loss :0.1313147395849228 Test Loss:0.11781041324138641\n",
      "Epoch 1516 Training Loss :0.13131354749202728 Test Loss:0.11781000345945358\n",
      "Epoch 1517 Training Loss :0.13131235539913177 Test Loss:0.11780958622694016\n",
      "Epoch 1518 Training Loss :0.13131114840507507 Test Loss:0.11780917644500732\n",
      "Epoch 1519 Training Loss :0.13130995631217957 Test Loss:0.1178087517619133\n",
      "Epoch 1520 Training Loss :0.13130876421928406 Test Loss:0.11780834943056107\n",
      "Epoch 1521 Training Loss :0.13130757212638855 Test Loss:0.11780793964862823\n",
      "Epoch 1522 Training Loss :0.13130638003349304 Test Loss:0.11780752241611481\n",
      "Epoch 1523 Training Loss :0.13130517303943634 Test Loss:0.11780709773302078\n",
      "Epoch 1524 Training Loss :0.13130398094654083 Test Loss:0.11780670285224915\n",
      "Epoch 1525 Training Loss :0.13130278885364532 Test Loss:0.11780629307031631\n",
      "Epoch 1526 Training Loss :0.131301611661911 Test Loss:0.11780588328838348\n",
      "Epoch 1527 Training Loss :0.1313004046678543 Test Loss:0.11780548095703125\n",
      "Epoch 1528 Training Loss :0.13129922747612 Test Loss:0.11780506372451782\n",
      "Epoch 1529 Training Loss :0.1312980353832245 Test Loss:0.1178046464920044\n",
      "Epoch 1530 Training Loss :0.13129684329032898 Test Loss:0.11780425906181335\n",
      "Epoch 1531 Training Loss :0.13129565119743347 Test Loss:0.11780384927988052\n",
      "Epoch 1532 Training Loss :0.13129445910453796 Test Loss:0.11780344694852829\n",
      "Epoch 1533 Training Loss :0.13129326701164246 Test Loss:0.11780303716659546\n",
      "Epoch 1534 Training Loss :0.13129208981990814 Test Loss:0.11780262738466263\n",
      "Epoch 1535 Training Loss :0.13129089772701263 Test Loss:0.1178022250533104\n",
      "Epoch 1536 Training Loss :0.13128972053527832 Test Loss:0.11780183017253876\n",
      "Epoch 1537 Training Loss :0.1312885284423828 Test Loss:0.11780142039060593\n",
      "Epoch 1538 Training Loss :0.1312873512506485 Test Loss:0.1178010106086731\n",
      "Epoch 1539 Training Loss :0.13128617405891418 Test Loss:0.11780060082674026\n",
      "Epoch 1540 Training Loss :0.13128498196601868 Test Loss:0.11780020594596863\n",
      "Epoch 1541 Training Loss :0.13128380477428436 Test Loss:0.11779981106519699\n",
      "Epoch 1542 Training Loss :0.13128262758255005 Test Loss:0.11779941618442535\n",
      "Epoch 1543 Training Loss :0.13128145039081573 Test Loss:0.11779902875423431\n",
      "Epoch 1544 Training Loss :0.13128027319908142 Test Loss:0.11779863387346268\n",
      "Epoch 1545 Training Loss :0.1312790960073471 Test Loss:0.11779823154211044\n",
      "Epoch 1546 Training Loss :0.1312779188156128 Test Loss:0.1177978366613388\n",
      "Epoch 1547 Training Loss :0.13127672672271729 Test Loss:0.11779744178056717\n",
      "Epoch 1548 Training Loss :0.13127556443214417 Test Loss:0.11779705435037613\n",
      "Epoch 1549 Training Loss :0.13127438724040985 Test Loss:0.11779666692018509\n",
      "Epoch 1550 Training Loss :0.13127321004867554 Test Loss:0.11779626458883286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1551 Training Loss :0.13127204775810242 Test Loss:0.11779587715864182\n",
      "Epoch 1552 Training Loss :0.1312708705663681 Test Loss:0.11779548972845078\n",
      "Epoch 1553 Training Loss :0.1312696933746338 Test Loss:0.11779510974884033\n",
      "Epoch 1554 Training Loss :0.13126853108406067 Test Loss:0.1177947148680687\n",
      "Epoch 1555 Training Loss :0.13126735389232635 Test Loss:0.11779431998729706\n",
      "Epoch 1556 Training Loss :0.13126619160175323 Test Loss:0.11779392510652542\n",
      "Epoch 1557 Training Loss :0.13126502931118011 Test Loss:0.11779355257749557\n",
      "Epoch 1558 Training Loss :0.1312638521194458 Test Loss:0.11779315024614334\n",
      "Epoch 1559 Training Loss :0.13126268982887268 Test Loss:0.1177927777171135\n",
      "Epoch 1560 Training Loss :0.13126152753829956 Test Loss:0.11779239028692245\n",
      "Epoch 1561 Training Loss :0.13126036524772644 Test Loss:0.11779200285673141\n",
      "Epoch 1562 Training Loss :0.13125920295715332 Test Loss:0.11779161542654037\n",
      "Epoch 1563 Training Loss :0.1312580406665802 Test Loss:0.11779125034809113\n",
      "Epoch 1564 Training Loss :0.1312568634748459 Test Loss:0.11779085546731949\n",
      "Epoch 1565 Training Loss :0.13125570118427277 Test Loss:0.11779046803712845\n",
      "Epoch 1566 Training Loss :0.13125453889369965 Test Loss:0.1177901029586792\n",
      "Epoch 1567 Training Loss :0.13125337660312653 Test Loss:0.11778972297906876\n",
      "Epoch 1568 Training Loss :0.1312522143125534 Test Loss:0.11778932809829712\n",
      "Epoch 1569 Training Loss :0.13125105202198029 Test Loss:0.11778894811868668\n",
      "Epoch 1570 Training Loss :0.13124990463256836 Test Loss:0.11778856813907623\n",
      "Epoch 1571 Training Loss :0.13124874234199524 Test Loss:0.11778819561004639\n",
      "Epoch 1572 Training Loss :0.13124758005142212 Test Loss:0.11778781563043594\n",
      "Epoch 1573 Training Loss :0.131246417760849 Test Loss:0.1177874356508255\n",
      "Epoch 1574 Training Loss :0.13124527037143707 Test Loss:0.11778706312179565\n",
      "Epoch 1575 Training Loss :0.13124410808086395 Test Loss:0.11778667569160461\n",
      "Epoch 1576 Training Loss :0.13124296069145203 Test Loss:0.11778628826141357\n",
      "Epoch 1577 Training Loss :0.1312417984008789 Test Loss:0.11778592318296432\n",
      "Epoch 1578 Training Loss :0.13124065101146698 Test Loss:0.11778554320335388\n",
      "Epoch 1579 Training Loss :0.13123950362205505 Test Loss:0.11778517812490463\n",
      "Epoch 1580 Training Loss :0.13123834133148193 Test Loss:0.11778479814529419\n",
      "Epoch 1581 Training Loss :0.13123719394207 Test Loss:0.11778442561626434\n",
      "Epoch 1582 Training Loss :0.13123604655265808 Test Loss:0.1177840456366539\n",
      "Epoch 1583 Training Loss :0.13123488426208496 Test Loss:0.11778368055820465\n",
      "Epoch 1584 Training Loss :0.13123373687267303 Test Loss:0.11778330057859421\n",
      "Epoch 1585 Training Loss :0.1312325894832611 Test Loss:0.11778292804956436\n",
      "Epoch 1586 Training Loss :0.13123145699501038 Test Loss:0.11778255552053452\n",
      "Epoch 1587 Training Loss :0.13123029470443726 Test Loss:0.11778217554092407\n",
      "Epoch 1588 Training Loss :0.13122914731502533 Test Loss:0.11778181791305542\n",
      "Epoch 1589 Training Loss :0.1312279999256134 Test Loss:0.11778144538402557\n",
      "Epoch 1590 Training Loss :0.13122686743736267 Test Loss:0.11778106540441513\n",
      "Epoch 1591 Training Loss :0.13122570514678955 Test Loss:0.11778070777654648\n",
      "Epoch 1592 Training Loss :0.13122455775737762 Test Loss:0.11778034269809723\n",
      "Epoch 1593 Training Loss :0.1312234252691269 Test Loss:0.11777998507022858\n",
      "Epoch 1594 Training Loss :0.13122227787971497 Test Loss:0.11777960509061813\n",
      "Epoch 1595 Training Loss :0.13122113049030304 Test Loss:0.11777923256158829\n",
      "Epoch 1596 Training Loss :0.1312199980020523 Test Loss:0.11777886748313904\n",
      "Epoch 1597 Training Loss :0.13121885061264038 Test Loss:0.11777850985527039\n",
      "Epoch 1598 Training Loss :0.13121770322322845 Test Loss:0.11777815222740173\n",
      "Epoch 1599 Training Loss :0.13121657073497772 Test Loss:0.11777777969837189\n",
      "Epoch 1600 Training Loss :0.131215438246727 Test Loss:0.11777741461992264\n",
      "Epoch 1601 Training Loss :0.13121429085731506 Test Loss:0.11777704954147339\n",
      "Epoch 1602 Training Loss :0.13121315836906433 Test Loss:0.11777669191360474\n",
      "Epoch 1603 Training Loss :0.1312120109796524 Test Loss:0.11777633428573608\n",
      "Epoch 1604 Training Loss :0.13121087849140167 Test Loss:0.11777597665786743\n",
      "Epoch 1605 Training Loss :0.13120974600315094 Test Loss:0.11777560412883759\n",
      "Epoch 1606 Training Loss :0.1312086135149002 Test Loss:0.11777525395154953\n",
      "Epoch 1607 Training Loss :0.13120746612548828 Test Loss:0.11777488887310028\n",
      "Epoch 1608 Training Loss :0.13120633363723755 Test Loss:0.11777452379465103\n",
      "Epoch 1609 Training Loss :0.13120520114898682 Test Loss:0.11777415871620178\n",
      "Epoch 1610 Training Loss :0.13120408356189728 Test Loss:0.11777380108833313\n",
      "Epoch 1611 Training Loss :0.13120293617248535 Test Loss:0.11777345091104507\n",
      "Epoch 1612 Training Loss :0.13120180368423462 Test Loss:0.11777309328317642\n",
      "Epoch 1613 Training Loss :0.1312006711959839 Test Loss:0.11777272820472717\n",
      "Epoch 1614 Training Loss :0.13119953870773315 Test Loss:0.11777239292860031\n",
      "Epoch 1615 Training Loss :0.13119840621948242 Test Loss:0.11777202785015106\n",
      "Epoch 1616 Training Loss :0.1311972737312317 Test Loss:0.11777166277170181\n",
      "Epoch 1617 Training Loss :0.13119615614414215 Test Loss:0.11777131259441376\n",
      "Epoch 1618 Training Loss :0.13119502365589142 Test Loss:0.1177709549665451\n",
      "Epoch 1619 Training Loss :0.13119389116764069 Test Loss:0.11777059733867645\n",
      "Epoch 1620 Training Loss :0.13119277358055115 Test Loss:0.1177702471613884\n",
      "Epoch 1621 Training Loss :0.13119164109230042 Test Loss:0.11776990443468094\n",
      "Epoch 1622 Training Loss :0.13119050860404968 Test Loss:0.11776953935623169\n",
      "Epoch 1623 Training Loss :0.13118939101696014 Test Loss:0.11776919662952423\n",
      "Epoch 1624 Training Loss :0.1311882734298706 Test Loss:0.11776882410049438\n",
      "Epoch 1625 Training Loss :0.13118714094161987 Test Loss:0.11776847392320633\n",
      "Epoch 1626 Training Loss :0.13118602335453033 Test Loss:0.11776812374591827\n",
      "Epoch 1627 Training Loss :0.1311849057674408 Test Loss:0.11776778101921082\n",
      "Epoch 1628 Training Loss :0.13118378818035126 Test Loss:0.11776742339134216\n",
      "Epoch 1629 Training Loss :0.13118267059326172 Test Loss:0.11776707321405411\n",
      "Epoch 1630 Training Loss :0.131181538105011 Test Loss:0.11776670813560486\n",
      "Epoch 1631 Training Loss :0.13118043541908264 Test Loss:0.1177663654088974\n",
      "Epoch 1632 Training Loss :0.1311793178319931 Test Loss:0.11776600778102875\n",
      "Epoch 1633 Training Loss :0.13117820024490356 Test Loss:0.11776567250490189\n",
      "Epoch 1634 Training Loss :0.13117706775665283 Test Loss:0.11776532232761383\n",
      "Epoch 1635 Training Loss :0.1311759650707245 Test Loss:0.11776496469974518\n",
      "Epoch 1636 Training Loss :0.13117484748363495 Test Loss:0.11776461452245712\n",
      "Epoch 1637 Training Loss :0.1311737298965454 Test Loss:0.11776427924633026\n",
      "Epoch 1638 Training Loss :0.13117262721061707 Test Loss:0.1177639290690422\n",
      "Epoch 1639 Training Loss :0.13117150962352753 Test Loss:0.11776358634233475\n",
      "Epoch 1640 Training Loss :0.13117040693759918 Test Loss:0.11776324361562729\n",
      "Epoch 1641 Training Loss :0.13116928935050964 Test Loss:0.11776289343833923\n",
      "Epoch 1642 Training Loss :0.1311681717634201 Test Loss:0.11776256561279297\n",
      "Epoch 1643 Training Loss :0.13116706907749176 Test Loss:0.11776221543550491\n",
      "Epoch 1644 Training Loss :0.13116596639156342 Test Loss:0.11776187270879745\n",
      "Epoch 1645 Training Loss :0.13116484880447388 Test Loss:0.11776154488325119\n",
      "Epoch 1646 Training Loss :0.13116374611854553 Test Loss:0.11776118725538254\n",
      "Epoch 1647 Training Loss :0.131162628531456 Test Loss:0.11776085197925568\n",
      "Epoch 1648 Training Loss :0.13116152584552765 Test Loss:0.11776050925254822\n",
      "Epoch 1649 Training Loss :0.1311604231595993 Test Loss:0.11776017397642136\n",
      "Epoch 1650 Training Loss :0.13115932047367096 Test Loss:0.1177598387002945\n",
      "Epoch 1651 Training Loss :0.13115820288658142 Test Loss:0.11775949597358704\n",
      "Epoch 1652 Training Loss :0.13115711510181427 Test Loss:0.11775916069746017\n",
      "Epoch 1653 Training Loss :0.13115599751472473 Test Loss:0.11775882542133331\n",
      "Epoch 1654 Training Loss :0.1311548948287964 Test Loss:0.11775849014520645\n",
      "Epoch 1655 Training Loss :0.13115379214286804 Test Loss:0.11775815486907959\n",
      "Epoch 1656 Training Loss :0.1311526894569397 Test Loss:0.11775781959295273\n",
      "Epoch 1657 Training Loss :0.13115158677101135 Test Loss:0.11775749176740646\n",
      "Epoch 1658 Training Loss :0.131150484085083 Test Loss:0.1177571564912796\n",
      "Epoch 1659 Training Loss :0.13114938139915466 Test Loss:0.11775682121515274\n",
      "Epoch 1660 Training Loss :0.1311482936143875 Test Loss:0.11775650084018707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1661 Training Loss :0.13114717602729797 Test Loss:0.11775616556406021\n",
      "Epoch 1662 Training Loss :0.13114608824253082 Test Loss:0.11775582283735275\n",
      "Epoch 1663 Training Loss :0.13114498555660248 Test Loss:0.11775549501180649\n",
      "Epoch 1664 Training Loss :0.13114389777183533 Test Loss:0.11775517463684082\n",
      "Epoch 1665 Training Loss :0.13114279508590698 Test Loss:0.11775483936071396\n",
      "Epoch 1666 Training Loss :0.13114169239997864 Test Loss:0.1177545040845871\n",
      "Epoch 1667 Training Loss :0.1311406046152115 Test Loss:0.11775416880846024\n",
      "Epoch 1668 Training Loss :0.13113950192928314 Test Loss:0.11775383353233337\n",
      "Epoch 1669 Training Loss :0.131138414144516 Test Loss:0.1177535131573677\n",
      "Epoch 1670 Training Loss :0.13113731145858765 Test Loss:0.11775318533182144\n",
      "Epoch 1671 Training Loss :0.1311362236738205 Test Loss:0.11775286495685577\n",
      "Epoch 1672 Training Loss :0.13113512098789215 Test Loss:0.11775253713130951\n",
      "Epoch 1673 Training Loss :0.131134033203125 Test Loss:0.11775221675634384\n",
      "Epoch 1674 Training Loss :0.13113293051719666 Test Loss:0.11775188148021698\n",
      "Epoch 1675 Training Loss :0.1311318427324295 Test Loss:0.11775156110525131\n",
      "Epoch 1676 Training Loss :0.13113075494766235 Test Loss:0.11775123327970505\n",
      "Epoch 1677 Training Loss :0.131129652261734 Test Loss:0.11775091290473938\n",
      "Epoch 1678 Training Loss :0.13112856447696686 Test Loss:0.11775059252977371\n",
      "Epoch 1679 Training Loss :0.1311274766921997 Test Loss:0.11775026470422745\n",
      "Epoch 1680 Training Loss :0.13112638890743256 Test Loss:0.11774992942810059\n",
      "Epoch 1681 Training Loss :0.1311253011226654 Test Loss:0.11774961650371552\n",
      "Epoch 1682 Training Loss :0.13112421333789825 Test Loss:0.11774930357933044\n",
      "Epoch 1683 Training Loss :0.1311231255531311 Test Loss:0.11774898320436478\n",
      "Epoch 1684 Training Loss :0.13112203776836395 Test Loss:0.11774865537881851\n",
      "Epoch 1685 Training Loss :0.1311209499835968 Test Loss:0.11774832755327225\n",
      "Epoch 1686 Training Loss :0.13111986219882965 Test Loss:0.11774802953004837\n",
      "Epoch 1687 Training Loss :0.1311187893152237 Test Loss:0.1177477091550827\n",
      "Epoch 1688 Training Loss :0.13111770153045654 Test Loss:0.11774738878011703\n",
      "Epoch 1689 Training Loss :0.1311166137456894 Test Loss:0.11774706840515137\n",
      "Epoch 1690 Training Loss :0.13111552596092224 Test Loss:0.1177467554807663\n",
      "Epoch 1691 Training Loss :0.1311144381761551 Test Loss:0.11774645000696182\n",
      "Epoch 1692 Training Loss :0.13111336529254913 Test Loss:0.11774612963199615\n",
      "Epoch 1693 Training Loss :0.13111227750778198 Test Loss:0.11774580925703049\n",
      "Epoch 1694 Training Loss :0.13111120462417603 Test Loss:0.11774550378322601\n",
      "Epoch 1695 Training Loss :0.13111011683940887 Test Loss:0.11774519085884094\n",
      "Epoch 1696 Training Loss :0.13110904395580292 Test Loss:0.11774488538503647\n",
      "Epoch 1697 Training Loss :0.13110795617103577 Test Loss:0.1177445650100708\n",
      "Epoch 1698 Training Loss :0.13110686838626862 Test Loss:0.11774426698684692\n",
      "Epoch 1699 Training Loss :0.13110579550266266 Test Loss:0.11774393916130066\n",
      "Epoch 1700 Training Loss :0.1311047226190567 Test Loss:0.11774364113807678\n",
      "Epoch 1701 Training Loss :0.13110363483428955 Test Loss:0.11774332076311111\n",
      "Epoch 1702 Training Loss :0.1311025619506836 Test Loss:0.11774301528930664\n",
      "Epoch 1703 Training Loss :0.13110148906707764 Test Loss:0.11774271726608276\n",
      "Epoch 1704 Training Loss :0.13110041618347168 Test Loss:0.11774241179227829\n",
      "Epoch 1705 Training Loss :0.13109932839870453 Test Loss:0.11774209141731262\n",
      "Epoch 1706 Training Loss :0.13109825551509857 Test Loss:0.11774178594350815\n",
      "Epoch 1707 Training Loss :0.13109718263149261 Test Loss:0.11774148792028427\n",
      "Epoch 1708 Training Loss :0.13109609484672546 Test Loss:0.1177411675453186\n",
      "Epoch 1709 Training Loss :0.1310950219631195 Test Loss:0.11774086207151413\n",
      "Epoch 1710 Training Loss :0.13109394907951355 Test Loss:0.11774054914712906\n",
      "Epoch 1711 Training Loss :0.1310928761959076 Test Loss:0.11774025857448578\n",
      "Epoch 1712 Training Loss :0.13109180331230164 Test Loss:0.11773994565010071\n",
      "Epoch 1713 Training Loss :0.13109073042869568 Test Loss:0.11773964762687683\n",
      "Epoch 1714 Training Loss :0.13108965754508972 Test Loss:0.11773932725191116\n",
      "Epoch 1715 Training Loss :0.13108859956264496 Test Loss:0.11773903667926788\n",
      "Epoch 1716 Training Loss :0.131087526679039 Test Loss:0.117738738656044\n",
      "Epoch 1717 Training Loss :0.13108645379543304 Test Loss:0.11773841828107834\n",
      "Epoch 1718 Training Loss :0.1310853809118271 Test Loss:0.11773811280727386\n",
      "Epoch 1719 Training Loss :0.13108432292938232 Test Loss:0.11773780733346939\n",
      "Epoch 1720 Training Loss :0.13108325004577637 Test Loss:0.11773750931024551\n",
      "Epoch 1721 Training Loss :0.1310821771621704 Test Loss:0.11773720383644104\n",
      "Epoch 1722 Training Loss :0.13108111917972565 Test Loss:0.11773691326379776\n",
      "Epoch 1723 Training Loss :0.1310800462961197 Test Loss:0.11773660778999329\n",
      "Epoch 1724 Training Loss :0.13107898831367493 Test Loss:0.11773629486560822\n",
      "Epoch 1725 Training Loss :0.13107791543006897 Test Loss:0.11773599684238434\n",
      "Epoch 1726 Training Loss :0.131076842546463 Test Loss:0.11773569136857986\n",
      "Epoch 1727 Training Loss :0.13107578456401825 Test Loss:0.11773540824651718\n",
      "Epoch 1728 Training Loss :0.1310747265815735 Test Loss:0.1177351102232933\n",
      "Epoch 1729 Training Loss :0.13107365369796753 Test Loss:0.11773480474948883\n",
      "Epoch 1730 Training Loss :0.13107259571552277 Test Loss:0.11773449927568436\n",
      "Epoch 1731 Training Loss :0.131071537733078 Test Loss:0.11773420870304108\n",
      "Epoch 1732 Training Loss :0.13107047975063324 Test Loss:0.11773388832807541\n",
      "Epoch 1733 Training Loss :0.13106940686702728 Test Loss:0.11773361265659332\n",
      "Epoch 1734 Training Loss :0.13106834888458252 Test Loss:0.11773329973220825\n",
      "Epoch 1735 Training Loss :0.13106729090213776 Test Loss:0.11773301661014557\n",
      "Epoch 1736 Training Loss :0.131066232919693 Test Loss:0.11773273348808289\n",
      "Epoch 1737 Training Loss :0.13106517493724823 Test Loss:0.11773242056369781\n",
      "Epoch 1738 Training Loss :0.13106411695480347 Test Loss:0.11773212999105453\n",
      "Epoch 1739 Training Loss :0.1310630589723587 Test Loss:0.11773183941841125\n",
      "Epoch 1740 Training Loss :0.13106200098991394 Test Loss:0.11773154139518738\n",
      "Epoch 1741 Training Loss :0.13106094300746918 Test Loss:0.1177312433719635\n",
      "Epoch 1742 Training Loss :0.1310598999261856 Test Loss:0.11773093789815903\n",
      "Epoch 1743 Training Loss :0.13105884194374084 Test Loss:0.11773066967725754\n",
      "Epoch 1744 Training Loss :0.13105778396129608 Test Loss:0.11773035675287247\n",
      "Epoch 1745 Training Loss :0.13105672597885132 Test Loss:0.11773008108139038\n",
      "Epoch 1746 Training Loss :0.13105568289756775 Test Loss:0.11772977560758591\n",
      "Epoch 1747 Training Loss :0.13105462491512299 Test Loss:0.11772949248552322\n",
      "Epoch 1748 Training Loss :0.13105358183383942 Test Loss:0.11772919446229935\n",
      "Epoch 1749 Training Loss :0.13105252385139465 Test Loss:0.11772890388965607\n",
      "Epoch 1750 Training Loss :0.13105148077011108 Test Loss:0.11772862821817398\n",
      "Epoch 1751 Training Loss :0.13105042278766632 Test Loss:0.1177283450961113\n",
      "Epoch 1752 Training Loss :0.13104937970638275 Test Loss:0.11772803962230682\n",
      "Epoch 1753 Training Loss :0.131048321723938 Test Loss:0.11772776395082474\n",
      "Epoch 1754 Training Loss :0.13104727864265442 Test Loss:0.11772747337818146\n",
      "Epoch 1755 Training Loss :0.13104623556137085 Test Loss:0.11772719025611877\n",
      "Epoch 1756 Training Loss :0.13104519248008728 Test Loss:0.1177268996834755\n",
      "Epoch 1757 Training Loss :0.13104413449764252 Test Loss:0.11772661656141281\n",
      "Epoch 1758 Training Loss :0.13104309141635895 Test Loss:0.11772632598876953\n",
      "Epoch 1759 Training Loss :0.13104204833507538 Test Loss:0.11772605776786804\n",
      "Epoch 1760 Training Loss :0.1310410052537918 Test Loss:0.11772577464580536\n",
      "Epoch 1761 Training Loss :0.13103996217250824 Test Loss:0.11772549152374268\n",
      "Epoch 1762 Training Loss :0.13103891909122467 Test Loss:0.11772520840167999\n",
      "Epoch 1763 Training Loss :0.1310378760099411 Test Loss:0.1177249401807785\n",
      "Epoch 1764 Training Loss :0.13103683292865753 Test Loss:0.11772466450929642\n",
      "Epoch 1765 Training Loss :0.13103578984737396 Test Loss:0.11772438883781433\n",
      "Epoch 1766 Training Loss :0.1310347467660904 Test Loss:0.11772410571575165\n",
      "Epoch 1767 Training Loss :0.13103370368480682 Test Loss:0.11772383749485016\n",
      "Epoch 1768 Training Loss :0.13103266060352325 Test Loss:0.11772355437278748\n",
      "Epoch 1769 Training Loss :0.13103161752223969 Test Loss:0.11772327870130539\n",
      "Epoch 1770 Training Loss :0.13103057444095612 Test Loss:0.1177230104804039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1771 Training Loss :0.13102954626083374 Test Loss:0.11772273480892181\n",
      "Epoch 1772 Training Loss :0.13102850317955017 Test Loss:0.11772245913743973\n",
      "Epoch 1773 Training Loss :0.1310274749994278 Test Loss:0.11772219091653824\n",
      "Epoch 1774 Training Loss :0.13102643191814423 Test Loss:0.11772190034389496\n",
      "Epoch 1775 Training Loss :0.13102540373802185 Test Loss:0.11772164702415466\n",
      "Epoch 1776 Training Loss :0.13102436065673828 Test Loss:0.11772137135267258\n",
      "Epoch 1777 Training Loss :0.1310233175754547 Test Loss:0.11772110313177109\n",
      "Epoch 1778 Training Loss :0.13102228939533234 Test Loss:0.117720827460289\n",
      "Epoch 1779 Training Loss :0.13102124631404877 Test Loss:0.11772054433822632\n",
      "Epoch 1780 Training Loss :0.1310202181339264 Test Loss:0.11772028356790543\n",
      "Epoch 1781 Training Loss :0.13101918995380402 Test Loss:0.11772001534700394\n",
      "Epoch 1782 Training Loss :0.13101814687252045 Test Loss:0.11771974712610245\n",
      "Epoch 1783 Training Loss :0.13101711869239807 Test Loss:0.11771948635578156\n",
      "Epoch 1784 Training Loss :0.1310160905122757 Test Loss:0.11771920323371887\n",
      "Epoch 1785 Training Loss :0.13101506233215332 Test Loss:0.11771894991397858\n",
      "Epoch 1786 Training Loss :0.13101403415203094 Test Loss:0.11771868169307709\n",
      "Epoch 1787 Training Loss :0.13101300597190857 Test Loss:0.1177184134721756\n",
      "Epoch 1788 Training Loss :0.1310119777917862 Test Loss:0.1177181527018547\n",
      "Epoch 1789 Training Loss :0.13101094961166382 Test Loss:0.11771788448095322\n",
      "Epoch 1790 Training Loss :0.13100992143154144 Test Loss:0.11771762371063232\n",
      "Epoch 1791 Training Loss :0.13100889325141907 Test Loss:0.11771736294031143\n",
      "Epoch 1792 Training Loss :0.13100787997245789 Test Loss:0.11771708726882935\n",
      "Epoch 1793 Training Loss :0.1310068517923355 Test Loss:0.11771682649850845\n",
      "Epoch 1794 Training Loss :0.13100582361221313 Test Loss:0.11771657317876816\n",
      "Epoch 1795 Training Loss :0.13100479543209076 Test Loss:0.11771631240844727\n",
      "Epoch 1796 Training Loss :0.13100378215312958 Test Loss:0.11771605163812637\n",
      "Epoch 1797 Training Loss :0.1310027539730072 Test Loss:0.11771578341722488\n",
      "Epoch 1798 Training Loss :0.13100172579288483 Test Loss:0.11771552264690399\n",
      "Epoch 1799 Training Loss :0.13100071251392365 Test Loss:0.1177152693271637\n",
      "Epoch 1800 Training Loss :0.13099968433380127 Test Loss:0.11771500110626221\n",
      "Epoch 1801 Training Loss :0.1309986710548401 Test Loss:0.11771474033594131\n",
      "Epoch 1802 Training Loss :0.1309976428747177 Test Loss:0.11771448701620102\n",
      "Epoch 1803 Training Loss :0.13099661469459534 Test Loss:0.11771423369646072\n",
      "Epoch 1804 Training Loss :0.13099560141563416 Test Loss:0.11771395802497864\n",
      "Epoch 1805 Training Loss :0.13099458813667297 Test Loss:0.11771370470523834\n",
      "Epoch 1806 Training Loss :0.1309935599565506 Test Loss:0.11771344393491745\n",
      "Epoch 1807 Training Loss :0.13099254667758942 Test Loss:0.11771319061517715\n",
      "Epoch 1808 Training Loss :0.13099153339862823 Test Loss:0.11771292984485626\n",
      "Epoch 1809 Training Loss :0.13099052011966705 Test Loss:0.11771268397569656\n",
      "Epoch 1810 Training Loss :0.13098950684070587 Test Loss:0.11771241575479507\n",
      "Epoch 1811 Training Loss :0.1309884786605835 Test Loss:0.11771216243505478\n",
      "Epoch 1812 Training Loss :0.1309874802827835 Test Loss:0.11771190911531448\n",
      "Epoch 1813 Training Loss :0.13098645210266113 Test Loss:0.11771166324615479\n",
      "Epoch 1814 Training Loss :0.13098543882369995 Test Loss:0.11771140247583389\n",
      "Epoch 1815 Training Loss :0.13098442554473877 Test Loss:0.1177111491560936\n",
      "Epoch 1816 Training Loss :0.1309834122657776 Test Loss:0.1177108958363533\n",
      "Epoch 1817 Training Loss :0.1309824138879776 Test Loss:0.117710642516613\n",
      "Epoch 1818 Training Loss :0.13098140060901642 Test Loss:0.11771038174629211\n",
      "Epoch 1819 Training Loss :0.13098038733005524 Test Loss:0.11771013587713242\n",
      "Epoch 1820 Training Loss :0.13097937405109406 Test Loss:0.11770988255739212\n",
      "Epoch 1821 Training Loss :0.13097837567329407 Test Loss:0.11770962923765182\n",
      "Epoch 1822 Training Loss :0.13097736239433289 Test Loss:0.11770938336849213\n",
      "Epoch 1823 Training Loss :0.1309763491153717 Test Loss:0.11770914494991302\n",
      "Epoch 1824 Training Loss :0.13097535073757172 Test Loss:0.11770889163017273\n",
      "Epoch 1825 Training Loss :0.13097433745861053 Test Loss:0.11770864576101303\n",
      "Epoch 1826 Training Loss :0.13097332417964935 Test Loss:0.11770839989185333\n",
      "Epoch 1827 Training Loss :0.13097232580184937 Test Loss:0.11770814657211304\n",
      "Epoch 1828 Training Loss :0.13097132742404938 Test Loss:0.11770790815353394\n",
      "Epoch 1829 Training Loss :0.1309703141450882 Test Loss:0.11770766973495483\n",
      "Epoch 1830 Training Loss :0.1309693157672882 Test Loss:0.11770742386579514\n",
      "Epoch 1831 Training Loss :0.13096830248832703 Test Loss:0.11770717799663544\n",
      "Epoch 1832 Training Loss :0.13096730411052704 Test Loss:0.11770693957805634\n",
      "Epoch 1833 Training Loss :0.13096629083156586 Test Loss:0.11770669370889664\n",
      "Epoch 1834 Training Loss :0.13096529245376587 Test Loss:0.11770645529031754\n",
      "Epoch 1835 Training Loss :0.13096429407596588 Test Loss:0.11770621687173843\n",
      "Epoch 1836 Training Loss :0.1309632807970047 Test Loss:0.11770597100257874\n",
      "Epoch 1837 Training Loss :0.1309622824192047 Test Loss:0.11770573258399963\n",
      "Epoch 1838 Training Loss :0.13096128404140472 Test Loss:0.11770549416542053\n",
      "Epoch 1839 Training Loss :0.13096028566360474 Test Loss:0.11770524084568024\n",
      "Epoch 1840 Training Loss :0.13095927238464355 Test Loss:0.11770500242710114\n",
      "Epoch 1841 Training Loss :0.13095827400684357 Test Loss:0.11770477890968323\n",
      "Epoch 1842 Training Loss :0.13095727562904358 Test Loss:0.11770451813936234\n",
      "Epoch 1843 Training Loss :0.1309562623500824 Test Loss:0.11770427227020264\n",
      "Epoch 1844 Training Loss :0.1309552639722824 Test Loss:0.11770404875278473\n",
      "Epoch 1845 Training Loss :0.13095426559448242 Test Loss:0.11770380288362503\n",
      "Epoch 1846 Training Loss :0.13095328211784363 Test Loss:0.11770356446504593\n",
      "Epoch 1847 Training Loss :0.13095228374004364 Test Loss:0.11770333349704742\n",
      "Epoch 1848 Training Loss :0.13095128536224365 Test Loss:0.11770309507846832\n",
      "Epoch 1849 Training Loss :0.13095028698444366 Test Loss:0.11770286411046982\n",
      "Epoch 1850 Training Loss :0.13094928860664368 Test Loss:0.11770263314247131\n",
      "Epoch 1851 Training Loss :0.1309482902288437 Test Loss:0.11770239472389221\n",
      "Epoch 1852 Training Loss :0.1309472918510437 Test Loss:0.11770215630531311\n",
      "Epoch 1853 Training Loss :0.1309462934732437 Test Loss:0.1177019253373146\n",
      "Epoch 1854 Training Loss :0.13094530999660492 Test Loss:0.1177017092704773\n",
      "Epoch 1855 Training Loss :0.13094431161880493 Test Loss:0.1177014634013176\n",
      "Epoch 1856 Training Loss :0.13094331324100494 Test Loss:0.11770123988389969\n",
      "Epoch 1857 Training Loss :0.13094232976436615 Test Loss:0.11770099401473999\n",
      "Epoch 1858 Training Loss :0.13094133138656616 Test Loss:0.11770077794790268\n",
      "Epoch 1859 Training Loss :0.13094034790992737 Test Loss:0.11770054697990417\n",
      "Epoch 1860 Training Loss :0.13093934953212738 Test Loss:0.11770030856132507\n",
      "Epoch 1861 Training Loss :0.1309383660554886 Test Loss:0.11770007759332657\n",
      "Epoch 1862 Training Loss :0.1309373676776886 Test Loss:0.11769985407590866\n",
      "Epoch 1863 Training Loss :0.1309363842010498 Test Loss:0.11769963055849075\n",
      "Epoch 1864 Training Loss :0.130935400724411 Test Loss:0.11769939959049225\n",
      "Epoch 1865 Training Loss :0.13093440234661102 Test Loss:0.11769917607307434\n",
      "Epoch 1866 Training Loss :0.13093341886997223 Test Loss:0.11769896000623703\n",
      "Epoch 1867 Training Loss :0.13093243539333344 Test Loss:0.11769872158765793\n",
      "Epoch 1868 Training Loss :0.13093145191669464 Test Loss:0.11769849061965942\n",
      "Epoch 1869 Training Loss :0.13093045353889465 Test Loss:0.11769826710224152\n",
      "Epoch 1870 Training Loss :0.13092947006225586 Test Loss:0.11769803613424301\n",
      "Epoch 1871 Training Loss :0.13092848658561707 Test Loss:0.1176978275179863\n",
      "Epoch 1872 Training Loss :0.13092750310897827 Test Loss:0.11769759654998779\n",
      "Epoch 1873 Training Loss :0.13092650473117828 Test Loss:0.11769738048315048\n",
      "Epoch 1874 Training Loss :0.1309255212545395 Test Loss:0.11769715696573257\n",
      "Epoch 1875 Training Loss :0.1309245377779007 Test Loss:0.11769691854715347\n",
      "Epoch 1876 Training Loss :0.1309235543012619 Test Loss:0.11769670993089676\n",
      "Epoch 1877 Training Loss :0.1309225708246231 Test Loss:0.11769649386405945\n",
      "Epoch 1878 Training Loss :0.13092158734798431 Test Loss:0.11769627034664154\n",
      "Epoch 1879 Training Loss :0.13092060387134552 Test Loss:0.11769603937864304\n",
      "Epoch 1880 Training Loss :0.13091962039470673 Test Loss:0.11769583821296692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1881 Training Loss :0.13091865181922913 Test Loss:0.11769558489322662\n",
      "Epoch 1882 Training Loss :0.13091766834259033 Test Loss:0.11769536882638931\n",
      "Epoch 1883 Training Loss :0.13091668486595154 Test Loss:0.117695152759552\n",
      "Epoch 1884 Training Loss :0.13091570138931274 Test Loss:0.1176949292421341\n",
      "Epoch 1885 Training Loss :0.13091473281383514 Test Loss:0.11769470572471619\n",
      "Epoch 1886 Training Loss :0.13091374933719635 Test Loss:0.11769449710845947\n",
      "Epoch 1887 Training Loss :0.13091276586055756 Test Loss:0.11769427359104156\n",
      "Epoch 1888 Training Loss :0.13091179728507996 Test Loss:0.11769405007362366\n",
      "Epoch 1889 Training Loss :0.13091082870960236 Test Loss:0.11769383400678635\n",
      "Epoch 1890 Training Loss :0.13090984523296356 Test Loss:0.11769361048936844\n",
      "Epoch 1891 Training Loss :0.13090887665748596 Test Loss:0.11769338697195053\n",
      "Epoch 1892 Training Loss :0.13090789318084717 Test Loss:0.11769317090511322\n",
      "Epoch 1893 Training Loss :0.13090692460536957 Test Loss:0.1176929622888565\n",
      "Epoch 1894 Training Loss :0.13090595602989197 Test Loss:0.1176927462220192\n",
      "Epoch 1895 Training Loss :0.13090497255325317 Test Loss:0.11769253760576248\n",
      "Epoch 1896 Training Loss :0.13090400397777557 Test Loss:0.11769231408834457\n",
      "Epoch 1897 Training Loss :0.13090303540229797 Test Loss:0.11769209057092667\n",
      "Epoch 1898 Training Loss :0.13090206682682037 Test Loss:0.11769186705350876\n",
      "Epoch 1899 Training Loss :0.13090108335018158 Test Loss:0.11769165843725204\n",
      "Epoch 1900 Training Loss :0.13090011477470398 Test Loss:0.11769145727157593\n",
      "Epoch 1901 Training Loss :0.13089914619922638 Test Loss:0.11769123375415802\n",
      "Epoch 1902 Training Loss :0.13089817762374878 Test Loss:0.11769101768732071\n",
      "Epoch 1903 Training Loss :0.13089720904827118 Test Loss:0.1176908016204834\n",
      "Epoch 1904 Training Loss :0.13089624047279358 Test Loss:0.11769059300422668\n",
      "Epoch 1905 Training Loss :0.13089527189731598 Test Loss:0.11769038438796997\n",
      "Epoch 1906 Training Loss :0.13089430332183838 Test Loss:0.11769016832113266\n",
      "Epoch 1907 Training Loss :0.13089333474636078 Test Loss:0.11768994480371475\n",
      "Epoch 1908 Training Loss :0.13089236617088318 Test Loss:0.11768973618745804\n",
      "Epoch 1909 Training Loss :0.13089139759540558 Test Loss:0.11768952757120132\n",
      "Epoch 1910 Training Loss :0.13089042901992798 Test Loss:0.11768930405378342\n",
      "Epoch 1911 Training Loss :0.13088947534561157 Test Loss:0.1176891177892685\n",
      "Epoch 1912 Training Loss :0.13088850677013397 Test Loss:0.11768890172243118\n",
      "Epoch 1913 Training Loss :0.13088753819465637 Test Loss:0.11768868565559387\n",
      "Epoch 1914 Training Loss :0.13088656961917877 Test Loss:0.11768848448991776\n",
      "Epoch 1915 Training Loss :0.13088561594486237 Test Loss:0.11768826842308044\n",
      "Epoch 1916 Training Loss :0.13088464736938477 Test Loss:0.11768805980682373\n",
      "Epoch 1917 Training Loss :0.13088369369506836 Test Loss:0.11768784373998642\n",
      "Epoch 1918 Training Loss :0.13088272511959076 Test Loss:0.1176876425743103\n",
      "Epoch 1919 Training Loss :0.13088175654411316 Test Loss:0.1176874190568924\n",
      "Epoch 1920 Training Loss :0.13088080286979675 Test Loss:0.11768722534179688\n",
      "Epoch 1921 Training Loss :0.13087984919548035 Test Loss:0.11768701672554016\n",
      "Epoch 1922 Training Loss :0.13087889552116394 Test Loss:0.11768680810928345\n",
      "Epoch 1923 Training Loss :0.13087792694568634 Test Loss:0.11768660694360733\n",
      "Epoch 1924 Training Loss :0.13087695837020874 Test Loss:0.11768639087677002\n",
      "Epoch 1925 Training Loss :0.13087600469589233 Test Loss:0.1176861897110939\n",
      "Epoch 1926 Training Loss :0.13087505102157593 Test Loss:0.11768598854541779\n",
      "Epoch 1927 Training Loss :0.13087409734725952 Test Loss:0.11768577247858047\n",
      "Epoch 1928 Training Loss :0.13087312877178192 Test Loss:0.11768557876348495\n",
      "Epoch 1929 Training Loss :0.13087217509746552 Test Loss:0.11768537014722824\n",
      "Epoch 1930 Training Loss :0.13087120652198792 Test Loss:0.11768516898155212\n",
      "Epoch 1931 Training Loss :0.1308702677488327 Test Loss:0.11768496781587601\n",
      "Epoch 1932 Training Loss :0.1308692991733551 Test Loss:0.1176847591996193\n",
      "Epoch 1933 Training Loss :0.1308683454990387 Test Loss:0.11768455058336258\n",
      "Epoch 1934 Training Loss :0.1308673918247223 Test Loss:0.11768434196710587\n",
      "Epoch 1935 Training Loss :0.13086643815040588 Test Loss:0.11768414825201035\n",
      "Epoch 1936 Training Loss :0.13086548447608948 Test Loss:0.11768393963575363\n",
      "Epoch 1937 Training Loss :0.13086453080177307 Test Loss:0.11768373101949692\n",
      "Epoch 1938 Training Loss :0.13086357712745667 Test Loss:0.1176835298538208\n",
      "Epoch 1939 Training Loss :0.13086263835430145 Test Loss:0.11768332868814468\n",
      "Epoch 1940 Training Loss :0.13086166977882385 Test Loss:0.11768313497304916\n",
      "Epoch 1941 Training Loss :0.13086073100566864 Test Loss:0.11768292635679245\n",
      "Epoch 1942 Training Loss :0.13085977733135223 Test Loss:0.11768273264169693\n",
      "Epoch 1943 Training Loss :0.13085882365703583 Test Loss:0.11768252402544022\n",
      "Epoch 1944 Training Loss :0.13085788488388062 Test Loss:0.1176823303103447\n",
      "Epoch 1945 Training Loss :0.1308569312095642 Test Loss:0.11768213659524918\n",
      "Epoch 1946 Training Loss :0.1308559775352478 Test Loss:0.11768192797899246\n",
      "Epoch 1947 Training Loss :0.1308550387620926 Test Loss:0.11768173426389694\n",
      "Epoch 1948 Training Loss :0.13085408508777618 Test Loss:0.11768152564764023\n",
      "Epoch 1949 Training Loss :0.13085313141345978 Test Loss:0.11768132448196411\n",
      "Epoch 1950 Training Loss :0.13085219264030457 Test Loss:0.11768113076686859\n",
      "Epoch 1951 Training Loss :0.13085123896598816 Test Loss:0.11768091470003128\n",
      "Epoch 1952 Training Loss :0.13085028529167175 Test Loss:0.11768072843551636\n",
      "Epoch 1953 Training Loss :0.13084934651851654 Test Loss:0.11768052726984024\n",
      "Epoch 1954 Training Loss :0.13084839284420013 Test Loss:0.11768032610416412\n",
      "Epoch 1955 Training Loss :0.13084745407104492 Test Loss:0.1176801323890686\n",
      "Epoch 1956 Training Loss :0.13084650039672852 Test Loss:0.11767993122339249\n",
      "Epoch 1957 Training Loss :0.1308455616235733 Test Loss:0.11767973750829697\n",
      "Epoch 1958 Training Loss :0.1308446079492569 Test Loss:0.11767955124378204\n",
      "Epoch 1959 Training Loss :0.13084366917610168 Test Loss:0.11767934262752533\n",
      "Epoch 1960 Training Loss :0.13084273040294647 Test Loss:0.11767914891242981\n",
      "Epoch 1961 Training Loss :0.13084177672863007 Test Loss:0.11767894774675369\n",
      "Epoch 1962 Training Loss :0.13084082305431366 Test Loss:0.11767875403165817\n",
      "Epoch 1963 Training Loss :0.13083988428115845 Test Loss:0.11767856031656265\n",
      "Epoch 1964 Training Loss :0.13083894550800323 Test Loss:0.11767836660146713\n",
      "Epoch 1965 Training Loss :0.13083799183368683 Test Loss:0.11767817288637161\n",
      "Epoch 1966 Training Loss :0.13083705306053162 Test Loss:0.11767797917127609\n",
      "Epoch 1967 Training Loss :0.1308361142873764 Test Loss:0.11767777800559998\n",
      "Epoch 1968 Training Loss :0.1308351755142212 Test Loss:0.11767757683992386\n",
      "Epoch 1969 Training Loss :0.13083423674106598 Test Loss:0.11767739057540894\n",
      "Epoch 1970 Training Loss :0.13083328306674957 Test Loss:0.11767719686031342\n",
      "Epoch 1971 Training Loss :0.13083234429359436 Test Loss:0.11767701059579849\n",
      "Epoch 1972 Training Loss :0.13083140552043915 Test Loss:0.11767680943012238\n",
      "Epoch 1973 Training Loss :0.13083046674728394 Test Loss:0.11767661571502686\n",
      "Epoch 1974 Training Loss :0.13082952797412872 Test Loss:0.11767642199993134\n",
      "Epoch 1975 Training Loss :0.1308285892009735 Test Loss:0.11767622083425522\n",
      "Epoch 1976 Training Loss :0.1308276504278183 Test Loss:0.1176760271191597\n",
      "Epoch 1977 Training Loss :0.13082671165466309 Test Loss:0.11767584830522537\n",
      "Epoch 1978 Training Loss :0.13082577288150787 Test Loss:0.11767565459012985\n",
      "Epoch 1979 Training Loss :0.13082483410835266 Test Loss:0.11767545342445374\n",
      "Epoch 1980 Training Loss :0.13082389533519745 Test Loss:0.11767527461051941\n",
      "Epoch 1981 Training Loss :0.13082297146320343 Test Loss:0.11767508089542389\n",
      "Epoch 1982 Training Loss :0.13082203269004822 Test Loss:0.11767488718032837\n",
      "Epoch 1983 Training Loss :0.130821093916893 Test Loss:0.11767470091581345\n",
      "Epoch 1984 Training Loss :0.1308201551437378 Test Loss:0.11767450720071793\n",
      "Epoch 1985 Training Loss :0.13081921637058258 Test Loss:0.117674320936203\n",
      "Epoch 1986 Training Loss :0.13081829249858856 Test Loss:0.11767414212226868\n",
      "Epoch 1987 Training Loss :0.13081735372543335 Test Loss:0.11767394095659256\n",
      "Epoch 1988 Training Loss :0.13081642985343933 Test Loss:0.11767374724149704\n",
      "Epoch 1989 Training Loss :0.13081549108028412 Test Loss:0.11767356842756271\n",
      "Epoch 1990 Training Loss :0.1308145672082901 Test Loss:0.11767338216304779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1991 Training Loss :0.1308136284351349 Test Loss:0.11767319589853287\n",
      "Epoch 1992 Training Loss :0.13081270456314087 Test Loss:0.11767300963401794\n",
      "Epoch 1993 Training Loss :0.13081176578998566 Test Loss:0.11767283082008362\n",
      "Epoch 1994 Training Loss :0.13081082701683044 Test Loss:0.1176726296544075\n",
      "Epoch 1995 Training Loss :0.13080990314483643 Test Loss:0.11767244338989258\n",
      "Epoch 1996 Training Loss :0.1308089792728424 Test Loss:0.11767224967479706\n",
      "Epoch 1997 Training Loss :0.1308080404996872 Test Loss:0.11767207086086273\n",
      "Epoch 1998 Training Loss :0.13080711662769318 Test Loss:0.1176718920469284\n",
      "Epoch 1999 Training Loss :0.13080619275569916 Test Loss:0.11767170578241348\n",
      "Epoch 2000 Training Loss :0.13080526888370514 Test Loss:0.11767151206731796\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "EPOCHS = 2000\n",
    "\n",
    "simpleNNmodel = SimpleNet()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    optimizer = optim.SGD(simpleNNmodel.parameters(), lr=0.003)\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    simpleNNmodel.train()\n",
    "    \n",
    "    y_train_output = simpleNNmodel(X_train)\n",
    "    y_test_output = simpleNNmodel(X_test)\n",
    "    loss = simpleNNmodel.loss_criterion(y_train_output,y_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    validation_loss = simpleNNmodel.loss_criterion(y_test_output,y_test)\n",
    "#     print('MAE:', metrics.mean_absolute_error(y_test_output.detach().numpy(), y_test.detach().numpy()))\n",
    "#     print('RMSE:', np.sqrt(validation_loss.detach().numpy()))\n",
    "#     print('R2:', metrics.r2_score(y_test.detach().numpy(), y_test_output.detach().numpy()))\n",
    "    \n",
    "    print('Epoch {} Training Loss :{} Test Loss:{}'.format((epoch+1),loss,validation_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.8886],\n",
      "        [3.9020],\n",
      "        [3.8839],\n",
      "        [3.8785],\n",
      "        [3.8864],\n",
      "        [3.9308],\n",
      "        [3.8793],\n",
      "        [3.8904],\n",
      "        [3.8690],\n",
      "        [3.9171],\n",
      "        [3.8881],\n",
      "        [3.8807],\n",
      "        [3.9163],\n",
      "        [3.8548],\n",
      "        [3.8579],\n",
      "        [3.9277],\n",
      "        [3.8693],\n",
      "        [3.8679],\n",
      "        [3.9061],\n",
      "        [3.8824],\n",
      "        [3.8555],\n",
      "        [3.8780],\n",
      "        [3.8592],\n",
      "        [3.8639],\n",
      "        [3.8823],\n",
      "        [3.8842],\n",
      "        [3.8976],\n",
      "        [3.8843],\n",
      "        [3.8669],\n",
      "        [3.8760],\n",
      "        [3.9087],\n",
      "        [3.9475],\n",
      "        [3.9078],\n",
      "        [3.8602],\n",
      "        [3.9114],\n",
      "        [3.8811],\n",
      "        [3.8731],\n",
      "        [3.8778],\n",
      "        [3.8919],\n",
      "        [3.9504],\n",
      "        [3.8646],\n",
      "        [3.9009],\n",
      "        [3.9045],\n",
      "        [3.8933],\n",
      "        [3.8802],\n",
      "        [3.8740],\n",
      "        [3.8899],\n",
      "        [3.9283],\n",
      "        [3.8792],\n",
      "        [3.9088],\n",
      "        [3.8946],\n",
      "        [3.8833],\n",
      "        [3.8886],\n",
      "        [3.9641],\n",
      "        [3.8726],\n",
      "        [3.8636],\n",
      "        [3.8935],\n",
      "        [3.8584],\n",
      "        [3.8640],\n",
      "        [3.8670],\n",
      "        [3.8923],\n",
      "        [3.8520],\n",
      "        [3.8752],\n",
      "        [3.8767],\n",
      "        [3.8858],\n",
      "        [3.9287],\n",
      "        [3.8596],\n",
      "        [3.8671],\n",
      "        [3.8965],\n",
      "        [3.8736],\n",
      "        [3.9360],\n",
      "        [3.8851],\n",
      "        [3.8837],\n",
      "        [3.9148],\n",
      "        [3.9192],\n",
      "        [3.8705],\n",
      "        [3.9211],\n",
      "        [3.8930],\n",
      "        [3.9222],\n",
      "        [3.8904],\n",
      "        [3.8839],\n",
      "        [3.8830],\n",
      "        [3.8790],\n",
      "        [3.8796],\n",
      "        [3.8661],\n",
      "        [3.8955],\n",
      "        [3.9394],\n",
      "        [3.9261],\n",
      "        [3.9690],\n",
      "        [3.8975],\n",
      "        [3.8897],\n",
      "        [3.9011],\n",
      "        [3.8801],\n",
      "        [3.8639],\n",
      "        [3.9420],\n",
      "        [3.8909],\n",
      "        [3.8763],\n",
      "        [3.8496],\n",
      "        [3.8906],\n",
      "        [3.8950],\n",
      "        [3.8853],\n",
      "        [3.8752],\n",
      "        [3.8698],\n",
      "        [3.9012],\n",
      "        [3.9269],\n",
      "        [3.9151],\n",
      "        [3.8870],\n",
      "        [3.8760],\n",
      "        [3.8920],\n",
      "        [3.8363],\n",
      "        [3.8522],\n",
      "        [3.9011],\n",
      "        [3.8968],\n",
      "        [3.8468],\n",
      "        [3.9059],\n",
      "        [3.8945],\n",
      "        [3.8805],\n",
      "        [3.9228],\n",
      "        [3.8807],\n",
      "        [3.8673],\n",
      "        [3.9276],\n",
      "        [3.9080],\n",
      "        [3.8903],\n",
      "        [3.9096],\n",
      "        [3.8847],\n",
      "        [3.8993],\n",
      "        [3.8744],\n",
      "        [3.9157],\n",
      "        [3.8926],\n",
      "        [3.8736],\n",
      "        [3.8656],\n",
      "        [3.8885],\n",
      "        [3.9111],\n",
      "        [3.9566],\n",
      "        [3.8878],\n",
      "        [3.8903],\n",
      "        [3.9125],\n",
      "        [3.8822],\n",
      "        [3.8910],\n",
      "        [3.9161],\n",
      "        [3.8797],\n",
      "        [3.8893],\n",
      "        [3.9059],\n",
      "        [3.8955],\n",
      "        [3.8838],\n",
      "        [3.8852],\n",
      "        [3.8895],\n",
      "        [3.9098],\n",
      "        [3.9268],\n",
      "        [3.8833],\n",
      "        [3.8867],\n",
      "        [3.8875],\n",
      "        [3.8848],\n",
      "        [3.8880],\n",
      "        [3.8910],\n",
      "        [3.9295],\n",
      "        [3.8764],\n",
      "        [3.9065],\n",
      "        [3.8845],\n",
      "        [3.8632],\n",
      "        [3.8727],\n",
      "        [3.9090],\n",
      "        [3.9295],\n",
      "        [3.9141],\n",
      "        [3.8756],\n",
      "        [3.8879],\n",
      "        [3.8934],\n",
      "        [3.8933],\n",
      "        [3.8893],\n",
      "        [3.9044],\n",
      "        [3.8918],\n",
      "        [3.8644],\n",
      "        [3.8564],\n",
      "        [3.8696],\n",
      "        [3.8486],\n",
      "        [3.9086],\n",
      "        [3.8790],\n",
      "        [3.8701],\n",
      "        [3.8825],\n",
      "        [3.8878],\n",
      "        [3.9302],\n",
      "        [3.9157],\n",
      "        [3.9052],\n",
      "        [3.9276],\n",
      "        [3.8859],\n",
      "        [3.9388],\n",
      "        [3.8951],\n",
      "        [3.8405],\n",
      "        [3.8651],\n",
      "        [3.8897],\n",
      "        [3.8557],\n",
      "        [3.9329],\n",
      "        [3.9451],\n",
      "        [3.9297],\n",
      "        [3.8840],\n",
      "        [3.8951],\n",
      "        [3.8885],\n",
      "        [3.9412],\n",
      "        [3.8922],\n",
      "        [3.8678],\n",
      "        [3.8730],\n",
      "        [3.9184],\n",
      "        [3.9164],\n",
      "        [3.8873],\n",
      "        [3.8737],\n",
      "        [3.8794],\n",
      "        [3.9013],\n",
      "        [3.9226],\n",
      "        [3.9255],\n",
      "        [3.9440],\n",
      "        [3.8968],\n",
      "        [3.8831],\n",
      "        [3.8771],\n",
      "        [3.8755],\n",
      "        [3.8610],\n",
      "        [3.8773],\n",
      "        [3.9086],\n",
      "        [3.8359],\n",
      "        [3.8722],\n",
      "        [3.8617],\n",
      "        [3.9574],\n",
      "        [3.8782],\n",
      "        [3.9289],\n",
      "        [3.8558],\n",
      "        [3.8935],\n",
      "        [3.8679],\n",
      "        [3.8785],\n",
      "        [3.8930],\n",
      "        [3.8899],\n",
      "        [3.8736],\n",
      "        [3.8817],\n",
      "        [3.9239],\n",
      "        [3.8788],\n",
      "        [3.9299],\n",
      "        [3.9004],\n",
      "        [3.8719],\n",
      "        [3.9162],\n",
      "        [3.8883],\n",
      "        [3.8732],\n",
      "        [3.8864],\n",
      "        [3.8494],\n",
      "        [3.9085],\n",
      "        [3.8468],\n",
      "        [3.9071],\n",
      "        [3.8880],\n",
      "        [3.8475],\n",
      "        [3.8763],\n",
      "        [3.9319],\n",
      "        [3.8739],\n",
      "        [3.9192],\n",
      "        [3.8942],\n",
      "        [3.8694],\n",
      "        [3.8840],\n",
      "        [3.8778],\n",
      "        [3.8862],\n",
      "        [3.8824],\n",
      "        [3.8965],\n",
      "        [3.9037],\n",
      "        [3.8967],\n",
      "        [3.8848],\n",
      "        [3.8786],\n",
      "        [3.9033],\n",
      "        [3.8985],\n",
      "        [3.8420],\n",
      "        [3.8945],\n",
      "        [3.8765],\n",
      "        [3.8870],\n",
      "        [3.9085],\n",
      "        [3.8573],\n",
      "        [3.8936],\n",
      "        [3.8982],\n",
      "        [3.8909],\n",
      "        [3.8824],\n",
      "        [3.8575],\n",
      "        [3.9038],\n",
      "        [3.9077],\n",
      "        [3.8975],\n",
      "        [3.9049],\n",
      "        [3.8522],\n",
      "        [3.8908],\n",
      "        [3.8685],\n",
      "        [3.8706],\n",
      "        [3.8661],\n",
      "        [3.9272],\n",
      "        [3.8706],\n",
      "        [3.8657],\n",
      "        [3.8838],\n",
      "        [3.9147],\n",
      "        [3.8733],\n",
      "        [3.8602],\n",
      "        [3.8814],\n",
      "        [3.8919],\n",
      "        [3.8650],\n",
      "        [3.8620],\n",
      "        [3.8737],\n",
      "        [3.8464],\n",
      "        [3.9307],\n",
      "        [3.8967],\n",
      "        [3.9079],\n",
      "        [3.8924],\n",
      "        [3.8962],\n",
      "        [3.8746],\n",
      "        [3.8623],\n",
      "        [3.8868],\n",
      "        [3.8564],\n",
      "        [3.8614],\n",
      "        [3.8532],\n",
      "        [3.8874],\n",
      "        [3.8712],\n",
      "        [3.8755],\n",
      "        [3.8763],\n",
      "        [3.8676],\n",
      "        [3.8900],\n",
      "        [3.8801],\n",
      "        [3.9126],\n",
      "        [3.8553],\n",
      "        [3.9669],\n",
      "        [3.8798],\n",
      "        [3.9129],\n",
      "        [3.8685],\n",
      "        [3.9010],\n",
      "        [3.9097],\n",
      "        [3.9145],\n",
      "        [3.8944],\n",
      "        [3.9307],\n",
      "        [3.8662],\n",
      "        [3.8874],\n",
      "        [3.9060],\n",
      "        [3.8690],\n",
      "        [3.8617],\n",
      "        [3.8454],\n",
      "        [3.9326],\n",
      "        [3.9091],\n",
      "        [3.8659],\n",
      "        [3.8573],\n",
      "        [3.8479],\n",
      "        [3.8727],\n",
      "        [3.9017],\n",
      "        [3.9012],\n",
      "        [3.8742],\n",
      "        [3.8719],\n",
      "        [3.9325],\n",
      "        [3.8782],\n",
      "        [3.9405],\n",
      "        [3.8902],\n",
      "        [3.8863],\n",
      "        [3.8795],\n",
      "        [3.9218],\n",
      "        [3.8690],\n",
      "        [3.9202],\n",
      "        [3.9303],\n",
      "        [3.8838],\n",
      "        [3.8718],\n",
      "        [3.8441],\n",
      "        [4.0036],\n",
      "        [3.9177],\n",
      "        [3.8851],\n",
      "        [3.8814],\n",
      "        [3.8940],\n",
      "        [3.9262],\n",
      "        [3.9032],\n",
      "        [3.9233],\n",
      "        [3.8935],\n",
      "        [3.8719]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_test_output = simpleNNmodel(X_test)\n",
    "# print(y_test_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
